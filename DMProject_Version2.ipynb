{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMProject-Version2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "t4zEC-3gLROH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "zPDFxiwWyTPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245f99a4-ffc2-4b90-fbee-b7f54425a5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rijm4ze6LPl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cba845-a195-4d63-b9a0-d8b1b6e7ffcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "import time\n",
        "import os\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from sklearn import metrics\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "G3YKFnJBLUdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sparse_mat(a2b, a2idx, b2idx):\n",
        "    n = len(a2idx)\n",
        "    m = len(b2idx)\n",
        "    assoc = np.zeros((n, m))\n",
        "    for a, b_assoc in a2b.iteritems():\n",
        "        if a not in a2idx:\n",
        "            continue\n",
        "        for b in b_assoc:\n",
        "            if b not in b2idx:\n",
        "                continue\n",
        "            assoc[a2idx[a], b2idx[b]] = 1.\n",
        "    assoc = sp.coo_matrix(assoc)\n",
        "    return assoc\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "Zxu8LLY8LxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "    This function computes the mean average precision at k between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])"
      ],
      "metadata": {
        "id": "_lxl2O1cL1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def zeros(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def ones(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "metadata": {
        "id": "Af-xGZdwLaQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "class MultiLayer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    # Properties    \n",
        "        name: String, defines the variable scope of the layer.\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.support = tf.placeholder(tf.float32)\n",
        "        self.issparse = True\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                print(input_dim)\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]])\n",
        "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        self.support = tf.placeholder(tf.float32)\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DEDICOMDecoder(MultiLayer):\n",
        "    \"\"\"DEDICOM Tensor Factorization Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            self.vars['global_interaction'] = weight_variable_glorot(\n",
        "                input_dim, input_dim, name='global_interaction')\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='local_variation_%d' % k)\n",
        "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
        "            product1 = tf.matmul(inputs_row, relation)\n",
        "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
        "            product3 = tf.matmul(product2, relation)\n",
        "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DistMultDecoder(MultiLayer):\n",
        "    \"\"\"DistMult Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DistMultDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='relation_%d' % k)\n",
        "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['relation_%d' % k])\n",
        "            intermediate_product = tf.matmul(inputs_row, relation)\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BilinearDecoder(MultiLayer):\n",
        "    \"\"\"Bilinear Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(BilinearDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, input_dim, name='relation_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AIyMpZb5LfMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "class EdgeMinibatchIterator(object):\n",
        "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
        "    random pairs of co-occuring edges.\n",
        "    assoc -- numpy array with target edges\n",
        "    placeholders -- tensorflow placeholders object\n",
        "    batch_size -- size of the minibatches\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats\n",
        "        self.feat = feat\n",
        "        self.edge_types = edge_types\n",
        "        self.batch_size = batch_size\n",
        "        self.val_test_size = val_test_size\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
        "        self.batch_num = [0]*self.num_edge_types\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r\n",
        "                self.idx2edge_type[r] = i, j, k\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        # Function to build test and val sets with val_test_size positive links\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                print(\"Minibatch edge type:\", \"(%d, %d, %d)\" % (i, j, k))\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "                print(\"Train edges=\", \"%04d\" % len(self.train_edges[i,j][k]))\n",
        "                print(\"Val edges=\", \"%04d\" % len(self.val_edges[i,j][k]))\n",
        "                print(\"Test edges=\", \"%04d\" % len(self.test_edges[i,j][k]))\n",
        "\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])\n",
        "        num_test = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "        num_val = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if len(test_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing test edges=\", \"%04d/%04d\" % (len(test_edges_false), len(test_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if len(val_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing val edges=\", \"%04d/%04d\" % (len(val_edges_false), len(val_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        # Re-build adj matrices\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def end(self):\n",
        "        finished = len(self.freebatch_edge_types) == 0\n",
        "        return finished\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "        # construct feed dictionary\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
        "        while True:\n",
        "            if self.iter % 4 == 0:\n",
        "                # gene-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "            elif self.iter % 4 == 1:\n",
        "                # gene-drug relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
        "            elif self.iter % 4 == 2:\n",
        "                # drug-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
        "            else:\n",
        "                # random side effect relation\n",
        "                if len(self.freebatch_edge_types) > 0:\n",
        "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
        "                else:\n",
        "                    self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "                    self.iter = 0\n",
        "\n",
        "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
        "                break\n",
        "            else:\n",
        "                if self.iter % 4 in [0, 1, 2]:\n",
        "                    self.batch_num[self.current_edge_type_idx] = 0\n",
        "                else:\n",
        "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
        "\n",
        "        self.iter += 1\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "    def num_training_batches(self, edge_type, type_idx):\n",
        "        return len(self.train_edges[edge_type][type_idx]) // self.batch_size + 1\n",
        "\n",
        "    def val_feed_dict(self, edge_type, type_idx, placeholders, size=None):\n",
        "        edge_list = self.val_edges[edge_type][type_idx]\n",
        "        if size is None:\n",
        "            return self.batch_feed_dict(edge_list, edge_type, placeholders)\n",
        "        else:\n",
        "            ind = np.random.permutation(len(edge_list))\n",
        "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
        "            return self.batch_feed_dict(val_edges, edge_type, placeholders)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\" Re-shuffle the training set.\n",
        "            Also reset the batch number.\n",
        "        \"\"\"\n",
        "        for edge_type in self.edge_types:\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                self.train_edges[edge_type][k] = np.random.permutation(self.train_edges[edge_type][k])\n",
        "                self.batch_num[self.edge_type2idx[edge_type[0], edge_type[1], k]] = 0\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.freebatch_edge_types = list(range(self.num_edge_types))\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 0, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 1, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[1, 0, 0])\n",
        "        self.iter = 0"
      ],
      "metadata": {
        "id": "3FIO8W8tLjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ANMT3GmPLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "OibFC-vT2lcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1\n",
        "        self.decoders = decoders\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types}\n",
        "        self.input_dim = num_feat\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)\n",
        "        self.hidden12 = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "            \n",
        "            self.hidden12[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "\n",
        "        for (i, hid1), (i1, hid12) in zip(self.hidden1.items(), self.hidden12.items()):\n",
        "            #self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "            h1 = tf.nn.relu(tf.add_n(hid1))\n",
        "            h2 = tf.nn.relu(tf.add_n(hid12))\n",
        "            self.hidden1[i] = tf.divide(tf.add_n([h1, h2]), tf.constant([2.0]))\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=FLAGS.hidden1, output_dim=FLAGS.hidden2,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'distmult':\n",
        "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'bilinear':\n",
        "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'dedicom':\n",
        "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'distmult':\n",
        "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'bilinear':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'dedicom':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
        "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)"
      ],
      "metadata": {
        "id": "NvyZI6NFLnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decagon Optimizer**"
      ],
      "metadata": {
        "id": "bcuYtIKOLggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "        neg_samples_list = []\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                    true_classes=labels,\n",
        "                    num_true=1,\n",
        "                    num_sampled=self.batch_size,\n",
        "                    unique=False,\n",
        "                    range_max=len(self.degrees[i][k]),\n",
        "                    distortion=0.75,\n",
        "                    unigrams=self.degrees[i][k].tolist())\n",
        "                neg_samples_list.append(neg_samples)\n",
        "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
        "        self.outputs = tf.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "        return preds\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
        "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        \"\"\"Cross-entropy optimization.\"\"\"\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def gather_cols(params, indices, name=None):\n",
        "    \"\"\"Gather columns of a 2D tensor.\n",
        "    Args:\n",
        "        params: A 2D tensor.\n",
        "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
        "        name: A name for the operation (optional).\n",
        "    Returns:\n",
        "        A 2D Tensor. Has the same type as ``params``.\n",
        "    \"\"\"\n",
        "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "metadata": {
        "id": "hftqMu6_LrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "ruZ8gtknLkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU (hide GPU) due to memory constraints\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "# Train on GPU\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=50)\n",
        "\n",
        "    return roc_sc, aupr_sc, apk_sc\n",
        "\n",
        "\n",
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.placeholder(tf.int32),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    }\n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    return placeholders\n",
        "\n",
        "val_test_size = 0.05\n",
        "n_genes = 500\n",
        "n_drugs = 400\n",
        "n_drugdrug_rel_types = 3\n",
        "gene_net = nx.planted_partition_graph(50, 10, 0.2, 0.05, seed=42)\n",
        "\n",
        "gene_adj = nx.adjacency_matrix(gene_net)\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "\n",
        "gene_drug_adj = sp.csr_matrix((10 * np.random.randn(n_genes, n_drugs) > 15).astype(int))\n",
        "drug_gene_adj = gene_drug_adj.transpose(copy=True)\n",
        "\n",
        "drug_drug_adj_list = []\n",
        "tmp = np.dot(drug_gene_adj, gene_drug_adj)\n",
        "for i in range(n_drugdrug_rel_types):\n",
        "    mat = np.zeros((n_drugs, n_drugs))\n",
        "    for d1, d2 in combinations(list(range(n_drugs)), 2):\n",
        "        if tmp[d1, d2] == i + 4:\n",
        "            mat[d1, d2] = mat[d2, d1] = 1.\n",
        "    drug_drug_adj_list.append(sp.csr_matrix(mat))\n",
        "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
        "\n",
        "\n",
        "# data representation\n",
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_drug_adj],\n",
        "    (1, 0): [drug_gene_adj],\n",
        "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: drug_degrees_list + drug_degrees_list,\n",
        "}\n",
        "\n",
        "# featureless (genes)\n",
        "gene_feat = sp.identity(n_genes)\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "# features (drugs)\n",
        "drug_feat = sp.identity(n_drugs)\n",
        "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
        "drug_feat = sparse_to_tuple(drug_feat.tocoo())\n",
        "\n",
        "# data representation\n",
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: drug_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: drug_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: drug_feat,\n",
        "}\n",
        "\n",
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'bilinear',\n",
        "    (0, 1): 'bilinear',\n",
        "    (1, 0): 'bilinear',\n",
        "    (1, 1): 'dedicom',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
        "flags.DEFINE_integer('batch_size', 100, 'minibatch size.')  # Changed from 512\n",
        "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
        "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
        "# substantial amount of time\n",
        "PRINT_PROGRESS_EVERY = 150\n",
        "\n",
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)\n",
        "\n",
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "    adj_mats=adj_mats_orig,\n",
        "    feat=feat,\n",
        "    edge_types=edge_types,\n",
        "    #batch_size=FLAGS.batch_size,\n",
        "    val_test_size=val_test_size\n",
        ")\n",
        "\n",
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "    placeholders=placeholders,\n",
        "    num_feat=num_feat,\n",
        "    nonzero_feat=nonzero_feat,\n",
        "    edge_types=edge_types,\n",
        "    decoders=edge_type2decoder\n",
        ")\n",
        "\n",
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "    opt = DecagonOptimizer(\n",
        "        embeddings=model.embeddings,\n",
        "        latent_inters=model.latent_inters,\n",
        "        latent_varies=model.latent_varies,\n",
        "        degrees=degrees,\n",
        "        edge_types=edge_types,\n",
        "        edge_type2dim=edge_type2dim,\n",
        "        placeholders=placeholders,\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        margin=FLAGS.max_margin\n",
        "    )\n",
        "\n",
        "print(\"Initialize session\")\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "feed_dict = {}\n",
        "\n",
        "print(\"Train model\")\n",
        "for epoch in range(FLAGS.epochs):\n",
        "#for epoch in range(2):\n",
        "\n",
        "    minibatch.shuffle()\n",
        "    itr = 0\n",
        "    while not minibatch.end():\n",
        "        # Construct feed dictionary\n",
        "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "        feed_dict = minibatch.update_feed_dict(\n",
        "            feed_dict=feed_dict,\n",
        "            dropout=FLAGS.dropout,\n",
        "            placeholders=placeholders)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        # Training step: run single weight update\n",
        "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
        "        train_cost = outs[1]\n",
        "        batch_edge_type = outs[2]\n",
        "\n",
        "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
        "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false,\n",
        "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
        "\n",
        "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
        "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
        "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "print(\"Optimization finished!\")\n",
        "\n",
        "for et in range(num_edge_types):\n",
        "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
        "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tWB8QcL-Nd",
        "outputId": "24d47a78-5882-4277-9ef7-0668c03c0b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge types: 10\n",
            "Defining placeholders\n",
            "Create minibatch iterator\n",
            "Minibatch edge type: (0, 0, 0)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 0, 1)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 1, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 0, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 1, 0)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 1)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 2)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Minibatch edge type: (1, 1, 3)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 4)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 5)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Create model\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "WARNING:tensorflow:From <ipython-input-6-5d25ff456bb0>:97: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "{0: 500, 1: 400}\n",
            "WARNING:tensorflow:From <ipython-input-6-5d25ff456bb0>:118: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Create optimizer\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_grad.py:202: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize session\n",
            "Train model\n",
            "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 11.71862 val_roc= 0.51458 val_auprc= 0.50370 val_apk= 0.18872 time= 1.65190\n",
            "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 9.55891 val_roc= 0.50697 val_auprc= 0.50072 val_apk= 0.14405 time= 0.10547\n",
            "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 9.37123 val_roc= 0.50081 val_auprc= 0.49773 val_apk= 0.15557 time= 0.10660\n",
            "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 8.84637 val_roc= 0.57529 val_auprc= 0.56841 val_apk= 0.43791 time= 0.10606\n",
            "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 9.47257 val_roc= 0.53260 val_auprc= 0.53163 val_apk= 0.35241 time= 0.10654\n",
            "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 7.38044 val_roc= 0.59312 val_auprc= 0.57975 val_apk= 0.49923 time= 0.10732\n",
            "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 8.64550 val_roc= 0.56912 val_auprc= 0.57460 val_apk= 0.52966 time= 0.10560\n",
            "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 8.09268 val_roc= 0.59506 val_auprc= 0.58505 val_apk= 0.56650 time= 0.11264\n",
            "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 7.17471 val_roc= 0.63233 val_auprc= 0.60443 val_apk= 0.46227 time= 0.10870\n",
            "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 7.29465 val_roc= 0.61653 val_auprc= 0.60499 val_apk= 0.59694 time= 0.10636\n",
            "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 7.05748 val_roc= 0.60330 val_auprc= 0.58335 val_apk= 0.51248 time= 0.10627\n",
            "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 7.14260 val_roc= 0.61208 val_auprc= 0.60220 val_apk= 0.53829 time= 0.10597\n",
            "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 6.81170 val_roc= 0.62396 val_auprc= 0.59738 val_apk= 0.47768 time= 0.10620\n",
            "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 7.06356 val_roc= 0.64593 val_auprc= 0.63017 val_apk= 0.53716 time= 0.10884\n",
            "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 7.06384 val_roc= 0.63466 val_auprc= 0.60538 val_apk= 0.46068 time= 0.10969\n",
            "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 5.90253 val_roc= 0.64166 val_auprc= 0.63221 val_apk= 0.70372 time= 0.10501\n",
            "Epoch: 0001 Iter: 2401 Edge: 0000 train_loss= 7.81858 val_roc= 0.63913 val_auprc= 0.61491 val_apk= 0.46858 time= 0.10489\n",
            "Epoch: 0002 Iter: 0001 Edge: 0000 train_loss= 7.85113 val_roc= 0.63742 val_auprc= 0.62844 val_apk= 0.65228 time= 0.11829\n",
            "Epoch: 0002 Iter: 0151 Edge: 0003 train_loss= 6.86941 val_roc= 0.66470 val_auprc= 0.64124 val_apk= 0.52984 time= 0.10708\n",
            "Epoch: 0002 Iter: 0301 Edge: 0000 train_loss= 5.65415 val_roc= 0.66587 val_auprc= 0.64797 val_apk= 0.58630 time= 0.11186\n",
            "Epoch: 0002 Iter: 0451 Edge: 0003 train_loss= 5.19836 val_roc= 0.66726 val_auprc= 0.65371 val_apk= 0.70986 time= 0.11035\n",
            "Epoch: 0002 Iter: 0601 Edge: 0000 train_loss= 6.84197 val_roc= 0.65412 val_auprc= 0.62932 val_apk= 0.60901 time= 0.10862\n",
            "Epoch: 0002 Iter: 0751 Edge: 0003 train_loss= 5.21548 val_roc= 0.66954 val_auprc= 0.66026 val_apk= 0.73057 time= 0.10717\n",
            "Epoch: 0002 Iter: 0901 Edge: 0000 train_loss= 6.00397 val_roc= 0.66838 val_auprc= 0.64553 val_apk= 0.64670 time= 0.11586\n",
            "Epoch: 0002 Iter: 1051 Edge: 0003 train_loss= 4.65194 val_roc= 0.67365 val_auprc= 0.66096 val_apk= 0.58086 time= 0.13832\n",
            "Epoch: 0002 Iter: 1201 Edge: 0000 train_loss= 6.21401 val_roc= 0.66155 val_auprc= 0.64529 val_apk= 0.69431 time= 0.10589\n",
            "Epoch: 0002 Iter: 1351 Edge: 0003 train_loss= 5.95252 val_roc= 0.67837 val_auprc= 0.65903 val_apk= 0.64863 time= 0.10595\n",
            "Epoch: 0002 Iter: 1501 Edge: 0000 train_loss= 5.95300 val_roc= 0.66900 val_auprc= 0.64047 val_apk= 0.59915 time= 0.10710\n",
            "Epoch: 0002 Iter: 1651 Edge: 0003 train_loss= 5.64904 val_roc= 0.69100 val_auprc= 0.68365 val_apk= 0.83707 time= 0.10690\n",
            "Epoch: 0002 Iter: 1801 Edge: 0000 train_loss= 5.70488 val_roc= 0.68014 val_auprc= 0.65481 val_apk= 0.56569 time= 0.10832\n",
            "Epoch: 0002 Iter: 1951 Edge: 0003 train_loss= 4.78989 val_roc= 0.68990 val_auprc= 0.67064 val_apk= 0.67618 time= 0.11116\n",
            "Epoch: 0002 Iter: 2101 Edge: 0000 train_loss= 5.75149 val_roc= 0.68074 val_auprc= 0.65675 val_apk= 0.64348 time= 0.10969\n",
            "Epoch: 0002 Iter: 2251 Edge: 0003 train_loss= 5.67650 val_roc= 0.69044 val_auprc= 0.66283 val_apk= 0.56066 time= 0.10455\n",
            "Epoch: 0002 Iter: 2401 Edge: 0000 train_loss= 4.65475 val_roc= 0.66414 val_auprc= 0.63728 val_apk= 0.50545 time= 0.10741\n",
            "Epoch: 0003 Iter: 0001 Edge: 0000 train_loss= 4.74265 val_roc= 0.67651 val_auprc= 0.64313 val_apk= 0.52178 time= 0.10891\n",
            "Epoch: 0003 Iter: 0151 Edge: 0003 train_loss= 4.76442 val_roc= 0.70257 val_auprc= 0.68117 val_apk= 0.75911 time= 0.10662\n",
            "Epoch: 0003 Iter: 0301 Edge: 0000 train_loss= 5.35652 val_roc= 0.66950 val_auprc= 0.63982 val_apk= 0.59746 time= 0.11040\n",
            "Epoch: 0003 Iter: 0451 Edge: 0003 train_loss= 4.73919 val_roc= 0.70646 val_auprc= 0.68530 val_apk= 0.70575 time= 0.11887\n",
            "Epoch: 0003 Iter: 0601 Edge: 0000 train_loss= 4.40618 val_roc= 0.69125 val_auprc= 0.65563 val_apk= 0.51272 time= 0.11869\n",
            "Epoch: 0003 Iter: 0751 Edge: 0003 train_loss= 5.82123 val_roc= 0.70995 val_auprc= 0.69430 val_apk= 0.78535 time= 0.12151\n",
            "Epoch: 0003 Iter: 0901 Edge: 0000 train_loss= 4.88687 val_roc= 0.69206 val_auprc= 0.66235 val_apk= 0.58831 time= 0.10763\n",
            "Epoch: 0003 Iter: 1051 Edge: 0003 train_loss= 6.06924 val_roc= 0.71444 val_auprc= 0.69709 val_apk= 0.74839 time= 0.10602\n",
            "Epoch: 0003 Iter: 1201 Edge: 0000 train_loss= 4.64976 val_roc= 0.69600 val_auprc= 0.67018 val_apk= 0.56776 time= 0.10389\n",
            "Epoch: 0003 Iter: 1351 Edge: 0003 train_loss= 3.72848 val_roc= 0.71699 val_auprc= 0.69973 val_apk= 0.75045 time= 0.11724\n",
            "Epoch: 0003 Iter: 1501 Edge: 0000 train_loss= 4.61952 val_roc= 0.69466 val_auprc= 0.66426 val_apk= 0.61506 time= 0.10595\n",
            "Epoch: 0003 Iter: 1651 Edge: 0003 train_loss= 5.62497 val_roc= 0.71828 val_auprc= 0.68945 val_apk= 0.63169 time= 0.12187\n",
            "Epoch: 0003 Iter: 1801 Edge: 0000 train_loss= 4.12817 val_roc= 0.69937 val_auprc= 0.66725 val_apk= 0.58865 time= 0.10745\n",
            "Epoch: 0003 Iter: 1951 Edge: 0003 train_loss= 3.59521 val_roc= 0.72364 val_auprc= 0.71017 val_apk= 0.77217 time= 0.13472\n",
            "Epoch: 0003 Iter: 2101 Edge: 0000 train_loss= 5.40410 val_roc= 0.70470 val_auprc= 0.67361 val_apk= 0.56352 time= 0.10785\n",
            "Epoch: 0003 Iter: 2251 Edge: 0003 train_loss= 4.63316 val_roc= 0.71781 val_auprc= 0.70185 val_apk= 0.83380 time= 0.10984\n",
            "Epoch: 0003 Iter: 2401 Edge: 0000 train_loss= 5.62704 val_roc= 0.70837 val_auprc= 0.67859 val_apk= 0.57888 time= 0.10772\n",
            "Epoch: 0004 Iter: 0001 Edge: 0000 train_loss= 4.15470 val_roc= 0.70826 val_auprc= 0.66872 val_apk= 0.53986 time= 0.12955\n",
            "Epoch: 0004 Iter: 0151 Edge: 0003 train_loss= 4.19737 val_roc= 0.72990 val_auprc= 0.70981 val_apk= 0.76297 time= 0.10535\n",
            "Epoch: 0004 Iter: 0301 Edge: 0000 train_loss= 4.53855 val_roc= 0.71615 val_auprc= 0.67886 val_apk= 0.55001 time= 0.10900\n",
            "Epoch: 0004 Iter: 0451 Edge: 0003 train_loss= 4.00297 val_roc= 0.72972 val_auprc= 0.69978 val_apk= 0.73108 time= 0.10606\n",
            "Epoch: 0004 Iter: 0601 Edge: 0000 train_loss= 5.40205 val_roc= 0.71487 val_auprc= 0.68072 val_apk= 0.55561 time= 0.10968\n",
            "Epoch: 0004 Iter: 0751 Edge: 0003 train_loss= 4.36187 val_roc= 0.72972 val_auprc= 0.71186 val_apk= 0.76797 time= 0.10528\n",
            "Epoch: 0004 Iter: 0901 Edge: 0000 train_loss= 5.71863 val_roc= 0.70131 val_auprc= 0.67025 val_apk= 0.62583 time= 0.10474\n",
            "Epoch: 0004 Iter: 1051 Edge: 0003 train_loss= 4.93947 val_roc= 0.73044 val_auprc= 0.71791 val_apk= 0.83907 time= 0.10792\n",
            "Epoch: 0004 Iter: 1201 Edge: 0000 train_loss= 4.15792 val_roc= 0.69629 val_auprc= 0.67408 val_apk= 0.80025 time= 0.10550\n",
            "Epoch: 0004 Iter: 1351 Edge: 0003 train_loss= 4.41909 val_roc= 0.73981 val_auprc= 0.72692 val_apk= 0.77545 time= 0.10975\n",
            "Epoch: 0004 Iter: 1501 Edge: 0000 train_loss= 4.79421 val_roc= 0.70520 val_auprc= 0.67874 val_apk= 0.62049 time= 0.11518\n",
            "Epoch: 0004 Iter: 1651 Edge: 0003 train_loss= 5.13654 val_roc= 0.73731 val_auprc= 0.71845 val_apk= 0.75157 time= 0.12041\n",
            "Epoch: 0004 Iter: 1801 Edge: 0000 train_loss= 6.47921 val_roc= 0.71218 val_auprc= 0.68638 val_apk= 0.76825 time= 0.11292\n",
            "Epoch: 0004 Iter: 1951 Edge: 0003 train_loss= 4.74527 val_roc= 0.73816 val_auprc= 0.72109 val_apk= 0.76968 time= 0.11626\n",
            "Epoch: 0004 Iter: 2101 Edge: 0000 train_loss= 5.35084 val_roc= 0.70935 val_auprc= 0.68302 val_apk= 0.67414 time= 0.13364\n",
            "Epoch: 0004 Iter: 2251 Edge: 0003 train_loss= 4.92622 val_roc= 0.74177 val_auprc= 0.72108 val_apk= 0.79726 time= 0.11900\n",
            "Epoch: 0004 Iter: 2401 Edge: 0000 train_loss= 6.08359 val_roc= 0.71506 val_auprc= 0.68576 val_apk= 0.67351 time= 0.11213\n",
            "Epoch: 0005 Iter: 0001 Edge: 0000 train_loss= 4.40077 val_roc= 0.71748 val_auprc= 0.68192 val_apk= 0.58225 time= 0.10598\n",
            "Epoch: 0005 Iter: 0151 Edge: 0003 train_loss= 4.33007 val_roc= 0.73292 val_auprc= 0.71044 val_apk= 0.72446 time= 0.10535\n",
            "Epoch: 0005 Iter: 0301 Edge: 0000 train_loss= 5.22574 val_roc= 0.71601 val_auprc= 0.68227 val_apk= 0.61552 time= 0.10624\n",
            "Epoch: 0005 Iter: 0451 Edge: 0003 train_loss= 4.08168 val_roc= 0.73264 val_auprc= 0.71712 val_apk= 0.80789 time= 0.10810\n",
            "Epoch: 0005 Iter: 0601 Edge: 0000 train_loss= 4.43375 val_roc= 0.72175 val_auprc= 0.69313 val_apk= 0.66410 time= 0.10465\n",
            "Epoch: 0005 Iter: 0751 Edge: 0003 train_loss= 4.52513 val_roc= 0.73554 val_auprc= 0.71627 val_apk= 0.80497 time= 0.11061\n",
            "Epoch: 0005 Iter: 0901 Edge: 0000 train_loss= 4.56020 val_roc= 0.71372 val_auprc= 0.68753 val_apk= 0.69065 time= 0.10684\n",
            "Epoch: 0005 Iter: 1051 Edge: 0003 train_loss= 4.30464 val_roc= 0.73616 val_auprc= 0.71049 val_apk= 0.75566 time= 0.10744\n",
            "Epoch: 0005 Iter: 1201 Edge: 0000 train_loss= 4.89552 val_roc= 0.71221 val_auprc= 0.68539 val_apk= 0.66417 time= 0.10734\n",
            "Epoch: 0005 Iter: 1351 Edge: 0003 train_loss= 4.92131 val_roc= 0.74257 val_auprc= 0.72764 val_apk= 0.85768 time= 0.10492\n",
            "Epoch: 0005 Iter: 1501 Edge: 0000 train_loss= 4.48334 val_roc= 0.71314 val_auprc= 0.68638 val_apk= 0.70765 time= 0.11131\n",
            "Epoch: 0005 Iter: 1651 Edge: 0003 train_loss= 4.83505 val_roc= 0.74372 val_auprc= 0.72897 val_apk= 0.83718 time= 0.10873\n",
            "Epoch: 0005 Iter: 1801 Edge: 0000 train_loss= 5.05632 val_roc= 0.71813 val_auprc= 0.69209 val_apk= 0.69790 time= 0.11197\n",
            "Epoch: 0005 Iter: 1951 Edge: 0003 train_loss= 4.75271 val_roc= 0.74946 val_auprc= 0.72713 val_apk= 0.88372 time= 0.10399\n",
            "Epoch: 0005 Iter: 2101 Edge: 0000 train_loss= 5.02676 val_roc= 0.71105 val_auprc= 0.67461 val_apk= 0.61028 time= 0.10567\n",
            "Epoch: 0005 Iter: 2251 Edge: 0003 train_loss= 5.57038 val_roc= 0.73765 val_auprc= 0.71284 val_apk= 0.69740 time= 0.10560\n",
            "Epoch: 0005 Iter: 2401 Edge: 0000 train_loss= 4.25876 val_roc= 0.72249 val_auprc= 0.68913 val_apk= 0.59555 time= 0.10534\n",
            "Epoch: 0006 Iter: 0001 Edge: 0000 train_loss= 5.43611 val_roc= 0.72545 val_auprc= 0.69647 val_apk= 0.61532 time= 0.10671\n",
            "Epoch: 0006 Iter: 0151 Edge: 0003 train_loss= 3.49874 val_roc= 0.74393 val_auprc= 0.72442 val_apk= 0.86493 time= 0.10580\n",
            "Epoch: 0006 Iter: 0301 Edge: 0000 train_loss= 4.15749 val_roc= 0.73006 val_auprc= 0.70048 val_apk= 0.64763 time= 0.10638\n",
            "Epoch: 0006 Iter: 0451 Edge: 0003 train_loss= 3.34529 val_roc= 0.74857 val_auprc= 0.73250 val_apk= 0.82666 time= 0.10700\n",
            "Epoch: 0006 Iter: 0601 Edge: 0000 train_loss= 4.05985 val_roc= 0.72150 val_auprc= 0.69010 val_apk= 0.63113 time= 0.12705\n",
            "Epoch: 0006 Iter: 0751 Edge: 0003 train_loss= 4.01474 val_roc= 0.74736 val_auprc= 0.72581 val_apk= 0.78938 time= 0.10439\n",
            "Epoch: 0006 Iter: 0901 Edge: 0000 train_loss= 5.19641 val_roc= 0.71713 val_auprc= 0.68426 val_apk= 0.65779 time= 0.11735\n",
            "Epoch: 0006 Iter: 1051 Edge: 0003 train_loss= 4.16383 val_roc= 0.75210 val_auprc= 0.72029 val_apk= 0.76815 time= 0.10417\n",
            "Epoch: 0006 Iter: 1201 Edge: 0000 train_loss= 4.73301 val_roc= 0.72888 val_auprc= 0.68891 val_apk= 0.58398 time= 0.10449\n",
            "Epoch: 0006 Iter: 1351 Edge: 0003 train_loss= 3.64992 val_roc= 0.74900 val_auprc= 0.72418 val_apk= 0.80532 time= 0.12063\n",
            "Epoch: 0006 Iter: 1501 Edge: 0000 train_loss= 5.32122 val_roc= 0.72823 val_auprc= 0.69230 val_apk= 0.58784 time= 0.11026\n",
            "Epoch: 0006 Iter: 1651 Edge: 0003 train_loss= 5.32939 val_roc= 0.74125 val_auprc= 0.70948 val_apk= 0.73733 time= 0.10709\n",
            "Epoch: 0006 Iter: 1801 Edge: 0000 train_loss= 6.39970 val_roc= 0.72719 val_auprc= 0.69391 val_apk= 0.55224 time= 0.10920\n",
            "Epoch: 0006 Iter: 1951 Edge: 0003 train_loss= 3.63741 val_roc= 0.73828 val_auprc= 0.71116 val_apk= 0.67125 time= 0.10784\n",
            "Epoch: 0006 Iter: 2101 Edge: 0000 train_loss= 2.69162 val_roc= 0.72977 val_auprc= 0.69506 val_apk= 0.55437 time= 0.10733\n",
            "Epoch: 0006 Iter: 2251 Edge: 0003 train_loss= 4.15898 val_roc= 0.74759 val_auprc= 0.72402 val_apk= 0.77907 time= 0.10989\n",
            "Epoch: 0006 Iter: 2401 Edge: 0000 train_loss= 5.40463 val_roc= 0.72038 val_auprc= 0.67920 val_apk= 0.52462 time= 0.10806\n",
            "Epoch: 0007 Iter: 0001 Edge: 0000 train_loss= 3.93042 val_roc= 0.71982 val_auprc= 0.67956 val_apk= 0.51517 time= 0.10584\n",
            "Epoch: 0007 Iter: 0151 Edge: 0003 train_loss= 3.24784 val_roc= 0.74636 val_auprc= 0.72235 val_apk= 0.75513 time= 0.10690\n",
            "Epoch: 0007 Iter: 0301 Edge: 0000 train_loss= 4.25231 val_roc= 0.72510 val_auprc= 0.69055 val_apk= 0.59859 time= 0.11048\n",
            "Epoch: 0007 Iter: 0451 Edge: 0003 train_loss= 3.69356 val_roc= 0.74852 val_auprc= 0.72962 val_apk= 0.84728 time= 0.10568\n",
            "Epoch: 0007 Iter: 0601 Edge: 0000 train_loss= 3.61134 val_roc= 0.72208 val_auprc= 0.69581 val_apk= 0.69707 time= 0.10815\n",
            "Epoch: 0007 Iter: 0751 Edge: 0003 train_loss= 3.84574 val_roc= 0.74396 val_auprc= 0.72763 val_apk= 0.88196 time= 0.10680\n",
            "Epoch: 0007 Iter: 0901 Edge: 0000 train_loss= 4.64532 val_roc= 0.73147 val_auprc= 0.69269 val_apk= 0.67068 time= 0.10708\n",
            "Epoch: 0007 Iter: 1051 Edge: 0003 train_loss= 2.04242 val_roc= 0.75179 val_auprc= 0.72494 val_apk= 0.73068 time= 0.12769\n",
            "Epoch: 0007 Iter: 1201 Edge: 0000 train_loss= 4.12176 val_roc= 0.73047 val_auprc= 0.69680 val_apk= 0.50020 time= 0.10559\n",
            "Epoch: 0007 Iter: 1351 Edge: 0003 train_loss= 3.50645 val_roc= 0.74163 val_auprc= 0.72263 val_apk= 0.83865 time= 0.10517\n",
            "Epoch: 0007 Iter: 1501 Edge: 0000 train_loss= 4.90242 val_roc= 0.72884 val_auprc= 0.69787 val_apk= 0.65842 time= 0.10839\n",
            "Epoch: 0007 Iter: 1651 Edge: 0003 train_loss= 3.48883 val_roc= 0.75660 val_auprc= 0.74322 val_apk= 0.93674 time= 0.11266\n",
            "Epoch: 0007 Iter: 1801 Edge: 0000 train_loss= 4.43949 val_roc= 0.73973 val_auprc= 0.70612 val_apk= 0.60479 time= 0.10808\n",
            "Epoch: 0007 Iter: 1951 Edge: 0003 train_loss= 3.32631 val_roc= 0.75337 val_auprc= 0.73335 val_apk= 0.81620 time= 0.10771\n",
            "Epoch: 0007 Iter: 2101 Edge: 0000 train_loss= 4.27997 val_roc= 0.73467 val_auprc= 0.69388 val_apk= 0.57951 time= 0.10439\n",
            "Epoch: 0007 Iter: 2251 Edge: 0003 train_loss= 5.47508 val_roc= 0.73656 val_auprc= 0.70766 val_apk= 0.72809 time= 0.10640\n",
            "Epoch: 0007 Iter: 2401 Edge: 0000 train_loss= 4.19048 val_roc= 0.73830 val_auprc= 0.68854 val_apk= 0.50243 time= 0.11183\n",
            "Epoch: 0008 Iter: 0001 Edge: 0000 train_loss= 3.83495 val_roc= 0.73762 val_auprc= 0.69611 val_apk= 0.55425 time= 0.10475\n",
            "Epoch: 0008 Iter: 0151 Edge: 0003 train_loss= 4.28346 val_roc= 0.75406 val_auprc= 0.72893 val_apk= 0.80532 time= 0.10535\n",
            "Epoch: 0008 Iter: 0301 Edge: 0000 train_loss= 5.02396 val_roc= 0.73438 val_auprc= 0.69148 val_apk= 0.51239 time= 0.12259\n",
            "Epoch: 0008 Iter: 0451 Edge: 0003 train_loss= 3.00736 val_roc= 0.74266 val_auprc= 0.71189 val_apk= 0.73617 time= 0.10695\n",
            "Epoch: 0008 Iter: 0601 Edge: 0000 train_loss= 4.62747 val_roc= 0.73032 val_auprc= 0.68710 val_apk= 0.56740 time= 0.10942\n",
            "Epoch: 0008 Iter: 0751 Edge: 0003 train_loss= 3.23184 val_roc= 0.75710 val_auprc= 0.73704 val_apk= 0.81601 time= 0.10595\n",
            "Epoch: 0008 Iter: 0901 Edge: 0000 train_loss= 3.94145 val_roc= 0.74222 val_auprc= 0.70588 val_apk= 0.60720 time= 0.10523\n",
            "Epoch: 0008 Iter: 1051 Edge: 0003 train_loss= 4.29811 val_roc= 0.74991 val_auprc= 0.71498 val_apk= 0.70076 time= 0.10516\n",
            "Epoch: 0008 Iter: 1201 Edge: 0000 train_loss= 3.95188 val_roc= 0.74263 val_auprc= 0.70265 val_apk= 0.60887 time= 0.10691\n",
            "Epoch: 0008 Iter: 1351 Edge: 0003 train_loss= 4.48792 val_roc= 0.75261 val_auprc= 0.72788 val_apk= 0.81036 time= 0.12583\n",
            "Epoch: 0008 Iter: 1501 Edge: 0000 train_loss= 4.96331 val_roc= 0.73798 val_auprc= 0.70831 val_apk= 0.64799 time= 0.11425\n",
            "Epoch: 0008 Iter: 1651 Edge: 0003 train_loss= 4.07095 val_roc= 0.75374 val_auprc= 0.72756 val_apk= 0.81872 time= 0.11726\n",
            "Epoch: 0008 Iter: 1801 Edge: 0000 train_loss= 3.24208 val_roc= 0.73888 val_auprc= 0.69738 val_apk= 0.62635 time= 0.12584\n",
            "Epoch: 0008 Iter: 1951 Edge: 0003 train_loss= 3.85914 val_roc= 0.75233 val_auprc= 0.72716 val_apk= 0.74724 time= 0.10777\n",
            "Epoch: 0008 Iter: 2101 Edge: 0000 train_loss= 4.09155 val_roc= 0.73947 val_auprc= 0.70081 val_apk= 0.56780 time= 0.11430\n",
            "Epoch: 0008 Iter: 2251 Edge: 0003 train_loss= 3.29353 val_roc= 0.76126 val_auprc= 0.72960 val_apk= 0.73701 time= 0.11699\n",
            "Epoch: 0008 Iter: 2401 Edge: 0000 train_loss= 4.24495 val_roc= 0.73306 val_auprc= 0.69117 val_apk= 0.58336 time= 0.10646\n",
            "Epoch: 0009 Iter: 0001 Edge: 0000 train_loss= 4.33579 val_roc= 0.74010 val_auprc= 0.70736 val_apk= 0.71151 time= 0.11554\n",
            "Epoch: 0009 Iter: 0151 Edge: 0003 train_loss= 3.13856 val_roc= 0.75793 val_auprc= 0.73558 val_apk= 0.80754 time= 0.10917\n",
            "Epoch: 0009 Iter: 0301 Edge: 0000 train_loss= 3.83923 val_roc= 0.74096 val_auprc= 0.70583 val_apk= 0.66692 time= 0.10654\n",
            "Epoch: 0009 Iter: 0451 Edge: 0003 train_loss= 4.77172 val_roc= 0.75570 val_auprc= 0.72251 val_apk= 0.76376 time= 0.10620\n",
            "Epoch: 0009 Iter: 0601 Edge: 0000 train_loss= 3.56514 val_roc= 0.73498 val_auprc= 0.70594 val_apk= 0.63494 time= 0.10893\n",
            "Epoch: 0009 Iter: 0751 Edge: 0003 train_loss= 4.06154 val_roc= 0.75951 val_auprc= 0.72873 val_apk= 0.69457 time= 0.11100\n",
            "Epoch: 0009 Iter: 0901 Edge: 0000 train_loss= 3.09139 val_roc= 0.72923 val_auprc= 0.70317 val_apk= 0.70581 time= 0.10450\n",
            "Epoch: 0009 Iter: 1051 Edge: 0003 train_loss= 3.83602 val_roc= 0.77047 val_auprc= 0.73560 val_apk= 0.70905 time= 0.10633\n",
            "Epoch: 0009 Iter: 1201 Edge: 0000 train_loss= 4.14956 val_roc= 0.73576 val_auprc= 0.70658 val_apk= 0.66143 time= 0.10929\n",
            "Epoch: 0009 Iter: 1351 Edge: 0003 train_loss= 4.26026 val_roc= 0.75885 val_auprc= 0.72373 val_apk= 0.65628 time= 0.10620\n",
            "Epoch: 0009 Iter: 1501 Edge: 0000 train_loss= 3.54671 val_roc= 0.74575 val_auprc= 0.71256 val_apk= 0.62712 time= 0.13636\n",
            "Epoch: 0009 Iter: 1651 Edge: 0003 train_loss= 4.50498 val_roc= 0.76482 val_auprc= 0.73093 val_apk= 0.69594 time= 0.10719\n",
            "Epoch: 0009 Iter: 1801 Edge: 0000 train_loss= 4.78023 val_roc= 0.74192 val_auprc= 0.70778 val_apk= 0.50187 time= 0.11158\n",
            "Epoch: 0009 Iter: 1951 Edge: 0003 train_loss= 3.56671 val_roc= 0.76300 val_auprc= 0.73458 val_apk= 0.69849 time= 0.10764\n",
            "Epoch: 0009 Iter: 2101 Edge: 0000 train_loss= 4.67292 val_roc= 0.73685 val_auprc= 0.70264 val_apk= 0.57719 time= 0.12366\n",
            "Epoch: 0009 Iter: 2251 Edge: 0003 train_loss= 4.16469 val_roc= 0.75504 val_auprc= 0.72007 val_apk= 0.68160 time= 0.11256\n",
            "Epoch: 0009 Iter: 2401 Edge: 0000 train_loss= 3.68627 val_roc= 0.74642 val_auprc= 0.71535 val_apk= 0.61035 time= 0.11118\n",
            "Epoch: 0010 Iter: 0001 Edge: 0000 train_loss= 3.60032 val_roc= 0.74484 val_auprc= 0.71354 val_apk= 0.62794 time= 0.10653\n",
            "Epoch: 0010 Iter: 0151 Edge: 0003 train_loss= 3.94320 val_roc= 0.76180 val_auprc= 0.72134 val_apk= 0.68432 time= 0.10792\n",
            "Epoch: 0010 Iter: 0301 Edge: 0000 train_loss= 4.42078 val_roc= 0.74403 val_auprc= 0.70808 val_apk= 0.57788 time= 0.10571\n",
            "Epoch: 0010 Iter: 0451 Edge: 0003 train_loss= 3.88555 val_roc= 0.76418 val_auprc= 0.72812 val_apk= 0.68652 time= 0.11416\n",
            "Epoch: 0010 Iter: 0601 Edge: 0000 train_loss= 4.56490 val_roc= 0.74876 val_auprc= 0.70932 val_apk= 0.54046 time= 0.10676\n",
            "Epoch: 0010 Iter: 0751 Edge: 0003 train_loss= 4.38259 val_roc= 0.76137 val_auprc= 0.72642 val_apk= 0.59841 time= 0.10835\n",
            "Epoch: 0010 Iter: 0901 Edge: 0000 train_loss= 3.47261 val_roc= 0.73729 val_auprc= 0.69778 val_apk= 0.51428 time= 0.10320\n",
            "Epoch: 0010 Iter: 1051 Edge: 0003 train_loss= 4.08655 val_roc= 0.74940 val_auprc= 0.71975 val_apk= 0.75244 time= 0.10790\n",
            "Epoch: 0010 Iter: 1201 Edge: 0000 train_loss= 3.93189 val_roc= 0.74711 val_auprc= 0.71579 val_apk= 0.62835 time= 0.10573\n",
            "Epoch: 0010 Iter: 1351 Edge: 0003 train_loss= 3.89882 val_roc= 0.76158 val_auprc= 0.73539 val_apk= 0.71370 time= 0.10844\n",
            "Epoch: 0010 Iter: 1501 Edge: 0000 train_loss= 4.26803 val_roc= 0.74272 val_auprc= 0.70204 val_apk= 0.59029 time= 0.10434\n",
            "Epoch: 0010 Iter: 1651 Edge: 0003 train_loss= 2.26507 val_roc= 0.73967 val_auprc= 0.70823 val_apk= 0.71459 time= 0.26262\n",
            "Epoch: 0010 Iter: 1801 Edge: 0000 train_loss= 3.85051 val_roc= 0.74520 val_auprc= 0.70831 val_apk= 0.57643 time= 0.11670\n",
            "Epoch: 0010 Iter: 1951 Edge: 0003 train_loss= 4.41618 val_roc= 0.73603 val_auprc= 0.70526 val_apk= 0.66956 time= 0.10980\n",
            "Epoch: 0010 Iter: 2101 Edge: 0000 train_loss= 4.28977 val_roc= 0.73513 val_auprc= 0.69627 val_apk= 0.58984 time= 0.10912\n",
            "Epoch: 0010 Iter: 2251 Edge: 0003 train_loss= 3.49802 val_roc= 0.75745 val_auprc= 0.73246 val_apk= 0.75155 time= 0.10660\n",
            "Epoch: 0010 Iter: 2401 Edge: 0000 train_loss= 2.86326 val_roc= 0.74259 val_auprc= 0.70321 val_apk= 0.56961 time= 0.11878\n",
            "Optimization finished!\n",
            "Edge type= [00, 00, 00]\n",
            "Edge type: 0000 Test AUROC score 0.71648\n",
            "Edge type: 0000 Test AUPRC score 0.66991\n",
            "Edge type: 0000 Test AP@k score 0.55484\n",
            "\n",
            "Edge type= [00, 00, 01]\n",
            "Edge type: 0001 Test AUROC score 0.80256\n",
            "Edge type: 0001 Test AUPRC score 0.76861\n",
            "Edge type: 0001 Test AP@k score 0.71904\n",
            "\n",
            "Edge type= [00, 01, 00]\n",
            "Edge type: 0002 Test AUROC score 0.74439\n",
            "Edge type: 0002 Test AUPRC score 0.71812\n",
            "Edge type: 0002 Test AP@k score 0.80950\n",
            "\n",
            "Edge type= [01, 00, 00]\n",
            "Edge type: 0003 Test AUROC score 0.73720\n",
            "Edge type: 0003 Test AUPRC score 0.69960\n",
            "Edge type: 0003 Test AP@k score 0.56177\n",
            "\n",
            "Edge type= [01, 01, 00]\n",
            "Edge type: 0004 Test AUROC score 0.70823\n",
            "Edge type: 0004 Test AUPRC score 0.67004\n",
            "Edge type: 0004 Test AP@k score 0.56823\n",
            "\n",
            "Edge type= [01, 01, 01]\n",
            "Edge type: 0005 Test AUROC score 0.77173\n",
            "Edge type: 0005 Test AUPRC score 0.70973\n",
            "Edge type: 0005 Test AP@k score 0.54575\n",
            "\n",
            "Edge type= [01, 01, 02]\n",
            "Edge type: 0006 Test AUROC score 0.76211\n",
            "Edge type: 0006 Test AUPRC score 0.72895\n",
            "Edge type: 0006 Test AP@k score 0.67244\n",
            "\n",
            "Edge type= [01, 01, 03]\n",
            "Edge type: 0007 Test AUROC score 0.69689\n",
            "Edge type: 0007 Test AUPRC score 0.65899\n",
            "Edge type: 0007 Test AP@k score 0.47979\n",
            "\n",
            "Edge type= [01, 01, 04]\n",
            "Edge type: 0008 Test AUROC score 0.76564\n",
            "Edge type: 0008 Test AUPRC score 0.72973\n",
            "Edge type: 0008 Test AP@k score 0.64218\n",
            "\n",
            "Edge type= [01, 01, 05]\n",
            "Edge type: 0009 Test AUROC score 0.78806\n",
            "Edge type: 0009 Test AUPRC score 0.74551\n",
            "Edge type: 0009 Test AP@k score 0.61473\n",
            "\n"
          ]
        }
      ]
    }
  ]
}