{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMProject-Version3-SampledGraphConv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "t4zEC-3gLROH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "zPDFxiwWyTPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0325e9-9640-437c-ba78-b09016c30111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rijm4ze6LPl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05673801-f852-4a1e-c2eb-482bada70ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "import time\n",
        "import os\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from sklearn import metrics\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "G3YKFnJBLUdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sparse_mat(a2b, a2idx, b2idx):\n",
        "    n = len(a2idx)\n",
        "    m = len(b2idx)\n",
        "    assoc = np.zeros((n, m))\n",
        "    for a, b_assoc in a2b.iteritems():\n",
        "        if a not in a2idx:\n",
        "            continue\n",
        "        for b in b_assoc:\n",
        "            if b not in b2idx:\n",
        "                continue\n",
        "            assoc[a2idx[a], b2idx[b]] = 1.\n",
        "    assoc = sp.coo_matrix(assoc)\n",
        "    return assoc\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "Zxu8LLY8LxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "    This function computes the mean average precision at k between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])"
      ],
      "metadata": {
        "id": "_lxl2O1cL1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def zeros(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def ones(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "metadata": {
        "id": "Af-xGZdwLaQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "class MultiLayer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    # Properties    \n",
        "        name: String, defines the variable scope of the layer.\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, support=None, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.rank = 100\n",
        "\n",
        "        if support is None:\n",
        "            self.support = placeholders['support']\n",
        "        else:\n",
        "            self.support = support\n",
        "\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]])\n",
        "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])\n",
        "\n",
        "\n",
        "            # THIS PART IS TAKEN FROM SampledGraphConv in FastGCN    \n",
        "            sup = self.support[self.edge_type][k]\n",
        "            \n",
        "            norm_x = tf.nn.l2_normalize(x)\n",
        "            #norm_sup = tf.nn.l2_normalize(tf.sparse.to_dense(sup))\n",
        "\n",
        "            # should use norm_sup, but doesnt work for some idiotic tf reason\n",
        "            norm_mix = tf.sparse_tensor_dense_matmul(sup, x)\n",
        "            norm_mix = norm_mix*tf.transpose(tf.reduce_sum(norm_mix))\n",
        "            sampledIndex = tf.multinomial(tf.log(norm_mix), self.rank)\n",
        "\n",
        "            out = norm_mix \n",
        "            \n",
        "            outputs.append(self.act(out))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DEDICOMDecoder(MultiLayer):\n",
        "    \"\"\"DEDICOM Tensor Factorization Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            self.vars['global_interaction'] = weight_variable_glorot(\n",
        "                input_dim, input_dim, name='global_interaction')\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='local_variation_%d' % k)\n",
        "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
        "            product1 = tf.matmul(inputs_row, relation)\n",
        "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
        "            product3 = tf.matmul(product2, relation)\n",
        "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DistMultDecoder(MultiLayer):\n",
        "    \"\"\"DistMult Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DistMultDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='relation_%d' % k)\n",
        "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['relation_%d' % k])\n",
        "            intermediate_product = tf.matmul(inputs_row, relation)\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BilinearDecoder(MultiLayer):\n",
        "    \"\"\"Bilinear Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(BilinearDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, input_dim, name='relation_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AIyMpZb5LfMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "class EdgeMinibatchIterator(object):\n",
        "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
        "    random pairs of co-occuring edges.\n",
        "    assoc -- numpy array with target edges\n",
        "    placeholders -- tensorflow placeholders object\n",
        "    batch_size -- size of the minibatches\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats\n",
        "        self.feat = feat\n",
        "        self.edge_types = edge_types\n",
        "        self.batch_size = batch_size\n",
        "        self.val_test_size = val_test_size\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
        "        self.batch_num = [0]*self.num_edge_types\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r\n",
        "                self.idx2edge_type[r] = i, j, k\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        # Function to build test and val sets with val_test_size positive links\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                print(\"Minibatch edge type:\", \"(%d, %d, %d)\" % (i, j, k))\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "                print(\"Train edges=\", \"%04d\" % len(self.train_edges[i,j][k]))\n",
        "                print(\"Val edges=\", \"%04d\" % len(self.val_edges[i,j][k]))\n",
        "                print(\"Test edges=\", \"%04d\" % len(self.test_edges[i,j][k]))\n",
        "\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])\n",
        "        num_test = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "        num_val = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if len(test_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing test edges=\", \"%04d/%04d\" % (len(test_edges_false), len(test_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if len(val_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing val edges=\", \"%04d/%04d\" % (len(val_edges_false), len(val_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        # Re-build adj matrices\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def end(self):\n",
        "        finished = len(self.freebatch_edge_types) == 0\n",
        "        return finished\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "        # construct feed dictionary\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
        "        while True:\n",
        "            if self.iter % 4 == 0:\n",
        "                # gene-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "            elif self.iter % 4 == 1:\n",
        "                # gene-drug relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
        "            elif self.iter % 4 == 2:\n",
        "                # drug-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
        "            else:\n",
        "                # random side effect relation\n",
        "                if len(self.freebatch_edge_types) > 0:\n",
        "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
        "                else:\n",
        "                    self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "                    self.iter = 0\n",
        "\n",
        "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
        "                break\n",
        "            else:\n",
        "                if self.iter % 4 in [0, 1, 2]:\n",
        "                    self.batch_num[self.current_edge_type_idx] = 0\n",
        "                else:\n",
        "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
        "\n",
        "        self.iter += 1\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "    def num_training_batches(self, edge_type, type_idx):\n",
        "        return len(self.train_edges[edge_type][type_idx]) // self.batch_size + 1\n",
        "\n",
        "    def val_feed_dict(self, edge_type, type_idx, placeholders, size=None):\n",
        "        edge_list = self.val_edges[edge_type][type_idx]\n",
        "        if size is None:\n",
        "            return self.batch_feed_dict(edge_list, edge_type, placeholders)\n",
        "        else:\n",
        "            ind = np.random.permutation(len(edge_list))\n",
        "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
        "            return self.batch_feed_dict(val_edges, edge_type, placeholders)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\" Re-shuffle the training set.\n",
        "            Also reset the batch number.\n",
        "        \"\"\"\n",
        "        for edge_type in self.edge_types:\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                self.train_edges[edge_type][k] = np.random.permutation(self.train_edges[edge_type][k])\n",
        "                self.batch_num[self.edge_type2idx[edge_type[0], edge_type[1], k]] = 0\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.freebatch_edge_types = list(range(self.num_edge_types))\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 0, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 1, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[1, 0, 0])\n",
        "        self.iter = 0"
      ],
      "metadata": {
        "id": "3FIO8W8tLjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ANMT3GmPLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "OibFC-vT2lcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1\n",
        "        self.decoders = decoders\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types}\n",
        "        self.input_dim = num_feat\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.support = placeholders['support']\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout, support=self.support,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "\n",
        "        for i, hid1 in self.hidden1.items():\n",
        "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=FLAGS.hidden1, output_dim=FLAGS.hidden2,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'distmult':\n",
        "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'bilinear':\n",
        "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'dedicom':\n",
        "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'distmult':\n",
        "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'bilinear':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'dedicom':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
        "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)"
      ],
      "metadata": {
        "id": "NvyZI6NFLnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decagon Optimizer**"
      ],
      "metadata": {
        "id": "bcuYtIKOLggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "        neg_samples_list = []\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                    true_classes=labels,\n",
        "                    num_true=1,\n",
        "                    num_sampled=self.batch_size,\n",
        "                    unique=False,\n",
        "                    range_max=len(self.degrees[i][k]),\n",
        "                    distortion=0.75,\n",
        "                    unigrams=self.degrees[i][k].tolist())\n",
        "                neg_samples_list.append(neg_samples)\n",
        "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
        "        self.outputs = tf.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "        return preds\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
        "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        \"\"\"Cross-entropy optimization.\"\"\"\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def gather_cols(params, indices, name=None):\n",
        "    \"\"\"Gather columns of a 2D tensor.\n",
        "    Args:\n",
        "        params: A 2D tensor.\n",
        "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
        "        name: A name for the operation (optional).\n",
        "    Returns:\n",
        "        A 2D Tensor. Has the same type as ``params``.\n",
        "    \"\"\"\n",
        "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "metadata": {
        "id": "hftqMu6_LrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "ruZ8gtknLkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU (hide GPU) due to memory constraints\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "# Train on GPU\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def prepare_data(placeholders, edge_types):\n",
        "    adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in edge_types.items()}\n",
        "\n",
        "    return adj_mats\n",
        "    \n",
        "    \n",
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=50)\n",
        "\n",
        "    return roc_sc, aupr_sc, apk_sc\n",
        "\n",
        "\n",
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'support': tf.sparse_placeholder(tf.float32),\n",
        "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.placeholder(tf.int32),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    }\n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    placeholders.update({'support': prepare_data(placeholders, edge_types)})\n",
        "    return placeholders\n",
        "\n",
        "val_test_size = 0.05\n",
        "n_genes = 500\n",
        "n_drugs = 400\n",
        "n_drugdrug_rel_types = 3\n",
        "gene_net = nx.planted_partition_graph(50, 10, 0.2, 0.05, seed=42)\n",
        "\n",
        "gene_adj = nx.adjacency_matrix(gene_net)\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "\n",
        "gene_drug_adj = sp.csr_matrix((10 * np.random.randn(n_genes, n_drugs) > 15).astype(int))\n",
        "drug_gene_adj = gene_drug_adj.transpose(copy=True)\n",
        "\n",
        "drug_drug_adj_list = []\n",
        "tmp = np.dot(drug_gene_adj, gene_drug_adj)\n",
        "for i in range(n_drugdrug_rel_types):\n",
        "    mat = np.zeros((n_drugs, n_drugs))\n",
        "    for d1, d2 in combinations(list(range(n_drugs)), 2):\n",
        "        if tmp[d1, d2] == i + 4:\n",
        "            mat[d1, d2] = mat[d2, d1] = 1.\n",
        "    drug_drug_adj_list.append(sp.csr_matrix(mat))\n",
        "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
        "\n",
        "\n",
        "# data representation\n",
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_drug_adj],\n",
        "    (1, 0): [drug_gene_adj],\n",
        "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: drug_degrees_list + drug_degrees_list,\n",
        "}\n",
        "\n",
        "# featureless (genes)\n",
        "gene_feat = sp.identity(n_genes)\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "# features (drugs)\n",
        "drug_feat = sp.identity(n_drugs)\n",
        "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
        "drug_feat = sparse_to_tuple(drug_feat.tocoo())\n",
        "\n",
        "# data representation\n",
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: drug_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: drug_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: drug_feat,\n",
        "}\n",
        "\n",
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'bilinear',\n",
        "    (0, 1): 'bilinear',\n",
        "    (1, 0): 'bilinear',\n",
        "    (1, 1): 'dedicom',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
        "flags.DEFINE_integer('batch_size', 100, 'minibatch size.')  # Changed from 512\n",
        "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
        "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
        "# substantial amount of time\n",
        "PRINT_PROGRESS_EVERY = 150\n",
        "\n",
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)\n",
        "\n",
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "    adj_mats=adj_mats_orig,\n",
        "    feat=feat,\n",
        "    edge_types=edge_types,\n",
        "    #batch_size=FLAGS.batch_size,\n",
        "    val_test_size=val_test_size\n",
        ")\n",
        "\n",
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "    placeholders=placeholders,\n",
        "    num_feat=num_feat,\n",
        "    nonzero_feat=nonzero_feat,\n",
        "    edge_types=edge_types,\n",
        "    decoders=edge_type2decoder\n",
        ")\n",
        "\n",
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "    opt = DecagonOptimizer(\n",
        "        embeddings=model.embeddings,\n",
        "        latent_inters=model.latent_inters,\n",
        "        latent_varies=model.latent_varies,\n",
        "        degrees=degrees,\n",
        "        edge_types=edge_types,\n",
        "        edge_type2dim=edge_type2dim,\n",
        "        placeholders=placeholders,\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        margin=FLAGS.max_margin\n",
        "    )\n",
        "\n",
        "print(\"Initialize session\")\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "feed_dict = {}\n",
        "\n",
        "print(\"Train model\")\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    minibatch.shuffle()\n",
        "    itr = 0\n",
        "    while not minibatch.end():\n",
        "        # Construct feed dictionary\n",
        "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "        feed_dict = minibatch.update_feed_dict(\n",
        "            feed_dict=feed_dict,\n",
        "            dropout=FLAGS.dropout,\n",
        "            placeholders=placeholders)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        # Training step: run single weight update\n",
        "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
        "        train_cost = outs[1]\n",
        "        batch_edge_type = outs[2]\n",
        "\n",
        "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
        "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false,\n",
        "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
        "\n",
        "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
        "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
        "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "print(\"Optimization finished!\")\n",
        "\n",
        "for et in range(num_edge_types):\n",
        "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
        "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tWB8QcL-Nd",
        "outputId": "7cfbfdc9-c641-4cec-e68e-4aeff034fb8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge types: 10\n",
            "Defining placeholders\n",
            "Create minibatch iterator\n",
            "Minibatch edge type: (0, 0, 0)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 0, 1)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 1, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 0, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 1, 0)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 1)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 2)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Minibatch edge type: (1, 1, 3)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 4)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 5)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Create model\n",
            "WARNING:tensorflow:From <ipython-input-6-77b142306a03>:111: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-77b142306a03>:118: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From <ipython-input-6-77b142306a03>:138: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Create optimizer\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_grad.py:202: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize session\n",
            "Train model\n",
            "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 13.34754 val_roc= 0.49402 val_auprc= 0.49774 val_apk= 0.28571 time= 2.32082\n",
            "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 10.20962 val_roc= 0.47968 val_auprc= 0.48673 val_apk= 0.17677 time= 0.08764\n",
            "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 9.72815 val_roc= 0.51617 val_auprc= 0.50762 val_apk= 0.28840 time= 0.08770\n",
            "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 9.74091 val_roc= 0.52695 val_auprc= 0.52556 val_apk= 0.41532 time= 0.08910\n",
            "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 9.54607 val_roc= 0.49598 val_auprc= 0.50060 val_apk= 0.25416 time= 0.08931\n",
            "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 9.48077 val_roc= 0.54731 val_auprc= 0.53654 val_apk= 0.37176 time= 0.09741\n",
            "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 9.87364 val_roc= 0.50551 val_auprc= 0.50607 val_apk= 0.30366 time= 0.09211\n",
            "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 7.92498 val_roc= 0.55183 val_auprc= 0.54293 val_apk= 0.38095 time= 0.09574\n",
            "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 9.57457 val_roc= 0.55824 val_auprc= 0.53713 val_apk= 0.22623 time= 0.08978\n",
            "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 8.60394 val_roc= 0.60237 val_auprc= 0.59270 val_apk= 0.58913 time= 0.11259\n",
            "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 9.12555 val_roc= 0.58073 val_auprc= 0.56758 val_apk= 0.39743 time= 0.09377\n",
            "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 7.13069 val_roc= 0.64694 val_auprc= 0.63031 val_apk= 0.59069 time= 0.09465\n",
            "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 7.02208 val_roc= 0.60719 val_auprc= 0.58625 val_apk= 0.36666 time= 0.09213\n",
            "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 8.68310 val_roc= 0.65057 val_auprc= 0.62500 val_apk= 0.59275 time= 0.09898\n",
            "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 6.78499 val_roc= 0.62070 val_auprc= 0.59583 val_apk= 0.40497 time= 0.09297\n",
            "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 6.80825 val_roc= 0.62901 val_auprc= 0.60795 val_apk= 0.58068 time= 0.11320\n",
            "Epoch: 0001 Iter: 2401 Edge: 0000 train_loss= 7.22145 val_roc= 0.63360 val_auprc= 0.61335 val_apk= 0.52556 time= 0.08674\n",
            "Epoch: 0002 Iter: 0001 Edge: 0000 train_loss= 7.77902 val_roc= 0.63215 val_auprc= 0.60895 val_apk= 0.45247 time= 0.09103\n",
            "Epoch: 0002 Iter: 0151 Edge: 0003 train_loss= 8.05169 val_roc= 0.66367 val_auprc= 0.65013 val_apk= 0.67415 time= 0.08955\n",
            "Epoch: 0002 Iter: 0301 Edge: 0000 train_loss= 8.47783 val_roc= 0.63089 val_auprc= 0.60192 val_apk= 0.39322 time= 0.09464\n",
            "Epoch: 0002 Iter: 0451 Edge: 0003 train_loss= 7.53472 val_roc= 0.70234 val_auprc= 0.68621 val_apk= 0.75226 time= 0.08944\n",
            "Epoch: 0002 Iter: 0601 Edge: 0000 train_loss= 6.26570 val_roc= 0.62734 val_auprc= 0.60210 val_apk= 0.45035 time= 0.09983\n",
            "Epoch: 0002 Iter: 0751 Edge: 0003 train_loss= 8.15550 val_roc= 0.67943 val_auprc= 0.65938 val_apk= 0.58422 time= 0.09418\n",
            "Epoch: 0002 Iter: 0901 Edge: 0000 train_loss= 7.43176 val_roc= 0.64380 val_auprc= 0.61062 val_apk= 0.46209 time= 0.09066\n",
            "Epoch: 0002 Iter: 1051 Edge: 0003 train_loss= 5.10069 val_roc= 0.69469 val_auprc= 0.68577 val_apk= 0.78658 time= 0.10184\n",
            "Epoch: 0002 Iter: 1201 Edge: 0000 train_loss= 6.29156 val_roc= 0.63547 val_auprc= 0.61187 val_apk= 0.52414 time= 0.10327\n",
            "Epoch: 0002 Iter: 1351 Edge: 0003 train_loss= 7.14890 val_roc= 0.69596 val_auprc= 0.66334 val_apk= 0.50431 time= 0.08800\n",
            "Epoch: 0002 Iter: 1501 Edge: 0000 train_loss= 7.16974 val_roc= 0.64172 val_auprc= 0.61047 val_apk= 0.43926 time= 0.09019\n",
            "Epoch: 0002 Iter: 1651 Edge: 0003 train_loss= 6.56384 val_roc= 0.69545 val_auprc= 0.65751 val_apk= 0.44394 time= 0.08827\n",
            "Epoch: 0002 Iter: 1801 Edge: 0000 train_loss= 6.86407 val_roc= 0.63737 val_auprc= 0.60688 val_apk= 0.45252 time= 0.09282\n",
            "Epoch: 0002 Iter: 1951 Edge: 0003 train_loss= 6.42712 val_roc= 0.68937 val_auprc= 0.66094 val_apk= 0.60038 time= 0.08754\n",
            "Epoch: 0002 Iter: 2101 Edge: 0000 train_loss= 8.51044 val_roc= 0.63943 val_auprc= 0.60528 val_apk= 0.46745 time= 0.09009\n",
            "Epoch: 0002 Iter: 2251 Edge: 0003 train_loss= 5.71519 val_roc= 0.69431 val_auprc= 0.66304 val_apk= 0.59551 time= 0.08682\n",
            "Epoch: 0002 Iter: 2401 Edge: 0000 train_loss= 6.83773 val_roc= 0.64917 val_auprc= 0.61642 val_apk= 0.41609 time= 0.08982\n",
            "Epoch: 0003 Iter: 0001 Edge: 0000 train_loss= 6.22655 val_roc= 0.64914 val_auprc= 0.61910 val_apk= 0.44172 time= 0.08957\n",
            "Epoch: 0003 Iter: 0151 Edge: 0003 train_loss= 4.75080 val_roc= 0.70130 val_auprc= 0.67553 val_apk= 0.56210 time= 0.08983\n",
            "Epoch: 0003 Iter: 0301 Edge: 0000 train_loss= 7.31012 val_roc= 0.66668 val_auprc= 0.64253 val_apk= 0.50064 time= 0.10303\n",
            "Epoch: 0003 Iter: 0451 Edge: 0003 train_loss= 6.80903 val_roc= 0.69209 val_auprc= 0.66209 val_apk= 0.48526 time= 0.08872\n",
            "Epoch: 0003 Iter: 0601 Edge: 0000 train_loss= 6.47127 val_roc= 0.67542 val_auprc= 0.64153 val_apk= 0.51068 time= 0.09117\n",
            "Epoch: 0003 Iter: 0751 Edge: 0003 train_loss= 5.72701 val_roc= 0.72389 val_auprc= 0.69274 val_apk= 0.55503 time= 0.08983\n",
            "Epoch: 0003 Iter: 0901 Edge: 0000 train_loss= 6.04027 val_roc= 0.64264 val_auprc= 0.60233 val_apk= 0.41509 time= 0.08920\n",
            "Epoch: 0003 Iter: 1051 Edge: 0003 train_loss= 6.04938 val_roc= 0.72187 val_auprc= 0.69167 val_apk= 0.65369 time= 0.10356\n",
            "Epoch: 0003 Iter: 1201 Edge: 0000 train_loss= 7.92620 val_roc= 0.67257 val_auprc= 0.64906 val_apk= 0.52177 time= 0.10371\n",
            "Epoch: 0003 Iter: 1351 Edge: 0003 train_loss= 6.83919 val_roc= 0.70913 val_auprc= 0.67845 val_apk= 0.58671 time= 0.10102\n",
            "Epoch: 0003 Iter: 1501 Edge: 0000 train_loss= 8.57445 val_roc= 0.64825 val_auprc= 0.62780 val_apk= 0.53323 time= 0.08983\n",
            "Epoch: 0003 Iter: 1651 Edge: 0003 train_loss= 4.97365 val_roc= 0.72878 val_auprc= 0.70859 val_apk= 0.69843 time= 0.08726\n",
            "Epoch: 0003 Iter: 1801 Edge: 0000 train_loss= 6.52508 val_roc= 0.66428 val_auprc= 0.64084 val_apk= 0.57746 time= 0.09389\n",
            "Epoch: 0003 Iter: 1951 Edge: 0003 train_loss= 4.85080 val_roc= 0.72207 val_auprc= 0.70181 val_apk= 0.67070 time= 0.09825\n",
            "Epoch: 0003 Iter: 2101 Edge: 0000 train_loss= 6.52305 val_roc= 0.66390 val_auprc= 0.62103 val_apk= 0.43743 time= 0.08935\n",
            "Epoch: 0003 Iter: 2251 Edge: 0003 train_loss= 5.89555 val_roc= 0.72640 val_auprc= 0.70676 val_apk= 0.78019 time= 0.08753\n",
            "Epoch: 0003 Iter: 2401 Edge: 0000 train_loss= 5.39469 val_roc= 0.67568 val_auprc= 0.64103 val_apk= 0.51445 time= 0.09786\n",
            "Epoch: 0004 Iter: 0001 Edge: 0000 train_loss= 5.84599 val_roc= 0.67256 val_auprc= 0.63709 val_apk= 0.45767 time= 0.09067\n",
            "Epoch: 0004 Iter: 0151 Edge: 0003 train_loss= 5.95194 val_roc= 0.70635 val_auprc= 0.67719 val_apk= 0.61033 time= 0.09930\n",
            "Epoch: 0004 Iter: 0301 Edge: 0000 train_loss= 5.48996 val_roc= 0.66950 val_auprc= 0.63016 val_apk= 0.57442 time= 0.08842\n",
            "Epoch: 0004 Iter: 0451 Edge: 0003 train_loss= 6.40949 val_roc= 0.71888 val_auprc= 0.70746 val_apk= 0.72326 time= 0.08875\n",
            "Epoch: 0004 Iter: 0601 Edge: 0000 train_loss= 5.79527 val_roc= 0.67313 val_auprc= 0.63448 val_apk= 0.45296 time= 0.09794\n",
            "Epoch: 0004 Iter: 0751 Edge: 0003 train_loss= 4.42695 val_roc= 0.74897 val_auprc= 0.72941 val_apk= 0.71738 time= 0.09163\n",
            "Epoch: 0004 Iter: 0901 Edge: 0000 train_loss= 6.50728 val_roc= 0.66735 val_auprc= 0.63134 val_apk= 0.50401 time= 0.09960\n",
            "Epoch: 0004 Iter: 1051 Edge: 0003 train_loss= 4.68272 val_roc= 0.73027 val_auprc= 0.71095 val_apk= 0.65329 time= 0.09374\n",
            "Epoch: 0004 Iter: 1201 Edge: 0000 train_loss= 5.63294 val_roc= 0.68006 val_auprc= 0.64612 val_apk= 0.49692 time= 0.09294\n",
            "Epoch: 0004 Iter: 1351 Edge: 0003 train_loss= 4.00604 val_roc= 0.75082 val_auprc= 0.73950 val_apk= 0.72756 time= 0.09197\n",
            "Epoch: 0004 Iter: 1501 Edge: 0000 train_loss= 6.68200 val_roc= 0.66412 val_auprc= 0.63535 val_apk= 0.51910 time= 0.08821\n",
            "Epoch: 0004 Iter: 1651 Edge: 0003 train_loss= 4.95505 val_roc= 0.73658 val_auprc= 0.71673 val_apk= 0.75254 time= 0.09498\n",
            "Epoch: 0004 Iter: 1801 Edge: 0000 train_loss= 6.17829 val_roc= 0.66874 val_auprc= 0.63740 val_apk= 0.51530 time= 0.09133\n",
            "Epoch: 0004 Iter: 1951 Edge: 0003 train_loss= 3.72556 val_roc= 0.75029 val_auprc= 0.72501 val_apk= 0.75358 time= 0.09240\n",
            "Epoch: 0004 Iter: 2101 Edge: 0000 train_loss= 7.64686 val_roc= 0.68368 val_auprc= 0.64565 val_apk= 0.46915 time= 0.09569\n",
            "Epoch: 0004 Iter: 2251 Edge: 0003 train_loss= 4.29460 val_roc= 0.74252 val_auprc= 0.72412 val_apk= 0.72528 time= 0.09253\n",
            "Epoch: 0004 Iter: 2401 Edge: 0000 train_loss= 6.38140 val_roc= 0.68047 val_auprc= 0.65083 val_apk= 0.59265 time= 0.08905\n",
            "Epoch: 0005 Iter: 0001 Edge: 0000 train_loss= 5.91780 val_roc= 0.67401 val_auprc= 0.63806 val_apk= 0.42160 time= 0.09349\n",
            "Epoch: 0005 Iter: 0151 Edge: 0003 train_loss= 3.94743 val_roc= 0.73009 val_auprc= 0.72180 val_apk= 0.83825 time= 0.08882\n",
            "Epoch: 0005 Iter: 0301 Edge: 0000 train_loss= 5.66391 val_roc= 0.68620 val_auprc= 0.64877 val_apk= 0.47971 time= 0.08787\n",
            "Epoch: 0005 Iter: 0451 Edge: 0003 train_loss= 3.41614 val_roc= 0.73803 val_auprc= 0.72487 val_apk= 0.84924 time= 0.08976\n",
            "Epoch: 0005 Iter: 0601 Edge: 0000 train_loss= 6.56095 val_roc= 0.68812 val_auprc= 0.64197 val_apk= 0.41971 time= 0.09005\n",
            "Epoch: 0005 Iter: 0751 Edge: 0003 train_loss= 5.78715 val_roc= 0.76655 val_auprc= 0.75069 val_apk= 0.83650 time= 0.12432\n",
            "Epoch: 0005 Iter: 0901 Edge: 0000 train_loss= 5.05876 val_roc= 0.67681 val_auprc= 0.64994 val_apk= 0.56445 time= 0.08906\n",
            "Epoch: 0005 Iter: 1051 Edge: 0003 train_loss= 5.39588 val_roc= 0.75836 val_auprc= 0.73693 val_apk= 0.75835 time= 0.09745\n",
            "Epoch: 0005 Iter: 1201 Edge: 0000 train_loss= 5.20166 val_roc= 0.67631 val_auprc= 0.63487 val_apk= 0.40333 time= 0.10401\n",
            "Epoch: 0005 Iter: 1351 Edge: 0003 train_loss= 4.63458 val_roc= 0.73320 val_auprc= 0.72439 val_apk= 0.82722 time= 0.08907\n",
            "Epoch: 0005 Iter: 1501 Edge: 0000 train_loss= 5.52446 val_roc= 0.70373 val_auprc= 0.67116 val_apk= 0.57335 time= 0.08910\n",
            "Epoch: 0005 Iter: 1651 Edge: 0003 train_loss= 4.35414 val_roc= 0.73809 val_auprc= 0.72387 val_apk= 0.83976 time= 0.08813\n",
            "Epoch: 0005 Iter: 1801 Edge: 0000 train_loss= 6.62929 val_roc= 0.69788 val_auprc= 0.65854 val_apk= 0.50541 time= 0.09574\n",
            "Epoch: 0005 Iter: 1951 Edge: 0003 train_loss= 5.11330 val_roc= 0.75332 val_auprc= 0.73978 val_apk= 0.93039 time= 0.09290\n",
            "Epoch: 0005 Iter: 2101 Edge: 0000 train_loss= 5.46724 val_roc= 0.70162 val_auprc= 0.65511 val_apk= 0.45994 time= 0.08818\n",
            "Epoch: 0005 Iter: 2251 Edge: 0003 train_loss= 5.15616 val_roc= 0.73651 val_auprc= 0.72265 val_apk= 0.84035 time= 0.09013\n",
            "Epoch: 0005 Iter: 2401 Edge: 0000 train_loss= 5.42956 val_roc= 0.68447 val_auprc= 0.64185 val_apk= 0.47226 time= 0.09447\n",
            "Epoch: 0006 Iter: 0001 Edge: 0000 train_loss= 5.42773 val_roc= 0.71459 val_auprc= 0.67063 val_apk= 0.50289 time= 0.09927\n",
            "Epoch: 0006 Iter: 0151 Edge: 0003 train_loss= 3.81187 val_roc= 0.76069 val_auprc= 0.74315 val_apk= 0.82135 time= 0.10009\n",
            "Epoch: 0006 Iter: 0301 Edge: 0000 train_loss= 5.31979 val_roc= 0.68996 val_auprc= 0.65558 val_apk= 0.56027 time= 0.09354\n",
            "Epoch: 0006 Iter: 0451 Edge: 0003 train_loss= 4.11282 val_roc= 0.74434 val_auprc= 0.73157 val_apk= 0.83881 time= 0.09031\n",
            "Epoch: 0006 Iter: 0601 Edge: 0000 train_loss= 6.72745 val_roc= 0.68638 val_auprc= 0.65624 val_apk= 0.54932 time= 0.11171\n",
            "Epoch: 0006 Iter: 0751 Edge: 0003 train_loss= 5.31101 val_roc= 0.77378 val_auprc= 0.75133 val_apk= 0.75056 time= 0.09518\n",
            "Epoch: 0006 Iter: 0901 Edge: 0000 train_loss= 6.01736 val_roc= 0.71046 val_auprc= 0.67415 val_apk= 0.48972 time= 0.08901\n",
            "Epoch: 0006 Iter: 1051 Edge: 0003 train_loss= 5.69136 val_roc= 0.76297 val_auprc= 0.73397 val_apk= 0.75520 time= 0.09239\n",
            "Epoch: 0006 Iter: 1201 Edge: 0000 train_loss= 6.96223 val_roc= 0.70322 val_auprc= 0.66567 val_apk= 0.56010 time= 0.10022\n",
            "Epoch: 0006 Iter: 1351 Edge: 0003 train_loss= 3.20444 val_roc= 0.76968 val_auprc= 0.74249 val_apk= 0.78558 time= 0.09380\n",
            "Epoch: 0006 Iter: 1501 Edge: 0000 train_loss= 4.93992 val_roc= 0.70967 val_auprc= 0.67112 val_apk= 0.54735 time= 0.09417\n",
            "Epoch: 0006 Iter: 1651 Edge: 0003 train_loss= 5.03054 val_roc= 0.75450 val_auprc= 0.73700 val_apk= 0.87444 time= 0.09531\n",
            "Epoch: 0006 Iter: 1801 Edge: 0000 train_loss= 6.13417 val_roc= 0.72135 val_auprc= 0.67304 val_apk= 0.41249 time= 0.09838\n",
            "Epoch: 0006 Iter: 1951 Edge: 0003 train_loss= 4.00882 val_roc= 0.77353 val_auprc= 0.75219 val_apk= 0.73164 time= 0.10020\n",
            "Epoch: 0006 Iter: 2101 Edge: 0000 train_loss= 6.05058 val_roc= 0.71001 val_auprc= 0.66922 val_apk= 0.48736 time= 0.09698\n",
            "Epoch: 0006 Iter: 2251 Edge: 0003 train_loss= 4.00456 val_roc= 0.79230 val_auprc= 0.76927 val_apk= 0.76885 time= 0.08796\n",
            "Epoch: 0006 Iter: 2401 Edge: 0000 train_loss= 4.97495 val_roc= 0.71115 val_auprc= 0.68350 val_apk= 0.67443 time= 0.10136\n",
            "Epoch: 0007 Iter: 0001 Edge: 0000 train_loss= 4.85288 val_roc= 0.71299 val_auprc= 0.67304 val_apk= 0.53132 time= 0.09805\n",
            "Epoch: 0007 Iter: 0151 Edge: 0003 train_loss= 3.93431 val_roc= 0.77731 val_auprc= 0.75721 val_apk= 0.71125 time= 0.09231\n",
            "Epoch: 0007 Iter: 0301 Edge: 0000 train_loss= 5.66102 val_roc= 0.70690 val_auprc= 0.66422 val_apk= 0.53868 time= 0.09050\n",
            "Epoch: 0007 Iter: 0451 Edge: 0003 train_loss= 2.93836 val_roc= 0.77492 val_auprc= 0.75794 val_apk= 0.86326 time= 0.08743\n",
            "Epoch: 0007 Iter: 0601 Edge: 0000 train_loss= 5.00133 val_roc= 0.71755 val_auprc= 0.67816 val_apk= 0.50035 time= 0.09091\n",
            "Epoch: 0007 Iter: 0751 Edge: 0003 train_loss= 5.30235 val_roc= 0.76658 val_auprc= 0.75691 val_apk= 0.94860 time= 0.09052\n",
            "Epoch: 0007 Iter: 0901 Edge: 0000 train_loss= 5.33039 val_roc= 0.69714 val_auprc= 0.65152 val_apk= 0.44055 time= 0.08831\n",
            "Epoch: 0007 Iter: 1051 Edge: 0003 train_loss= 5.99737 val_roc= 0.76423 val_auprc= 0.76056 val_apk= 0.97607 time= 0.09519\n",
            "Epoch: 0007 Iter: 1201 Edge: 0000 train_loss= 5.16313 val_roc= 0.71026 val_auprc= 0.67512 val_apk= 0.59647 time= 0.08909\n",
            "Epoch: 0007 Iter: 1351 Edge: 0003 train_loss= 4.53251 val_roc= 0.75566 val_auprc= 0.74479 val_apk= 0.86173 time= 0.08887\n",
            "Epoch: 0007 Iter: 1501 Edge: 0000 train_loss= 6.34371 val_roc= 0.71356 val_auprc= 0.67623 val_apk= 0.58421 time= 0.11044\n",
            "Epoch: 0007 Iter: 1651 Edge: 0003 train_loss= 4.97880 val_roc= 0.77624 val_auprc= 0.75260 val_apk= 0.89587 time= 0.09910\n",
            "Epoch: 0007 Iter: 1801 Edge: 0000 train_loss= 7.25812 val_roc= 0.71074 val_auprc= 0.66566 val_apk= 0.47387 time= 0.09910\n",
            "Epoch: 0007 Iter: 1951 Edge: 0003 train_loss= 4.94080 val_roc= 0.78923 val_auprc= 0.76759 val_apk= 0.83840 time= 0.08882\n",
            "Epoch: 0007 Iter: 2101 Edge: 0000 train_loss= 5.96621 val_roc= 0.70086 val_auprc= 0.66749 val_apk= 0.60954 time= 0.08778\n",
            "Epoch: 0007 Iter: 2251 Edge: 0003 train_loss= 4.94570 val_roc= 0.78567 val_auprc= 0.77433 val_apk= 0.91062 time= 0.08942\n",
            "Epoch: 0007 Iter: 2401 Edge: 0000 train_loss= 3.44948 val_roc= 0.70012 val_auprc= 0.65095 val_apk= 0.46165 time= 0.09120\n",
            "Epoch: 0008 Iter: 0001 Edge: 0000 train_loss= 5.25754 val_roc= 0.71300 val_auprc= 0.68195 val_apk= 0.68527 time= 0.11713\n",
            "Epoch: 0008 Iter: 0151 Edge: 0003 train_loss= 5.43705 val_roc= 0.76495 val_auprc= 0.73567 val_apk= 0.76503 time= 0.08776\n",
            "Epoch: 0008 Iter: 0301 Edge: 0000 train_loss= 4.71470 val_roc= 0.71925 val_auprc= 0.68288 val_apk= 0.65615 time= 0.08702\n",
            "Epoch: 0008 Iter: 0451 Edge: 0003 train_loss= 3.89045 val_roc= 0.78291 val_auprc= 0.76630 val_apk= 0.83523 time= 0.09215\n",
            "Epoch: 0008 Iter: 0601 Edge: 0000 train_loss= 7.04376 val_roc= 0.70219 val_auprc= 0.66241 val_apk= 0.54654 time= 0.09041\n",
            "Epoch: 0008 Iter: 0751 Edge: 0003 train_loss= 4.02504 val_roc= 0.76577 val_auprc= 0.74497 val_apk= 0.83470 time= 0.09213\n",
            "Epoch: 0008 Iter: 0901 Edge: 0000 train_loss= 6.18872 val_roc= 0.72545 val_auprc= 0.69063 val_apk= 0.64859 time= 0.11194\n",
            "Epoch: 0008 Iter: 1051 Edge: 0003 train_loss= 4.29082 val_roc= 0.78990 val_auprc= 0.77803 val_apk= 0.95039 time= 0.08842\n",
            "Epoch: 0008 Iter: 1201 Edge: 0000 train_loss= 6.49785 val_roc= 0.71583 val_auprc= 0.67333 val_apk= 0.52192 time= 0.10117\n",
            "Epoch: 0008 Iter: 1351 Edge: 0003 train_loss= 4.31282 val_roc= 0.77893 val_auprc= 0.75997 val_apk= 0.88857 time= 0.10079\n",
            "Epoch: 0008 Iter: 1501 Edge: 0000 train_loss= 4.56395 val_roc= 0.72574 val_auprc= 0.69255 val_apk= 0.64700 time= 0.09117\n",
            "Epoch: 0008 Iter: 1651 Edge: 0003 train_loss= 4.32483 val_roc= 0.77526 val_auprc= 0.75760 val_apk= 0.83321 time= 0.09256\n",
            "Epoch: 0008 Iter: 1801 Edge: 0000 train_loss= 5.57112 val_roc= 0.72534 val_auprc= 0.68053 val_apk= 0.56497 time= 0.09297\n",
            "Epoch: 0008 Iter: 1951 Edge: 0003 train_loss= 4.43420 val_roc= 0.78520 val_auprc= 0.76289 val_apk= 0.76064 time= 0.08812\n",
            "Epoch: 0008 Iter: 2101 Edge: 0000 train_loss= 6.34153 val_roc= 0.72873 val_auprc= 0.69575 val_apk= 0.68792 time= 0.09029\n",
            "Epoch: 0008 Iter: 2251 Edge: 0003 train_loss= 3.28273 val_roc= 0.77983 val_auprc= 0.75842 val_apk= 0.82078 time= 0.08987\n",
            "Epoch: 0008 Iter: 2401 Edge: 0000 train_loss= 5.78551 val_roc= 0.72116 val_auprc= 0.69150 val_apk= 0.65883 time= 0.09772\n",
            "Epoch: 0009 Iter: 0001 Edge: 0000 train_loss= 3.43615 val_roc= 0.72436 val_auprc= 0.68481 val_apk= 0.63409 time= 0.08765\n",
            "Epoch: 0009 Iter: 0151 Edge: 0003 train_loss= 4.28665 val_roc= 0.78055 val_auprc= 0.76147 val_apk= 0.87641 time= 0.08952\n",
            "Epoch: 0009 Iter: 0301 Edge: 0000 train_loss= 4.87257 val_roc= 0.72460 val_auprc= 0.68633 val_apk= 0.61453 time= 0.08863\n",
            "Epoch: 0009 Iter: 0451 Edge: 0003 train_loss= 4.59008 val_roc= 0.77501 val_auprc= 0.75740 val_apk= 0.89180 time= 0.09111\n",
            "Epoch: 0009 Iter: 0601 Edge: 0000 train_loss= 4.84119 val_roc= 0.71938 val_auprc= 0.68777 val_apk= 0.60325 time= 0.08784\n",
            "Epoch: 0009 Iter: 0751 Edge: 0003 train_loss= 4.47427 val_roc= 0.76079 val_auprc= 0.73551 val_apk= 0.80151 time= 0.08763\n",
            "Epoch: 0009 Iter: 0901 Edge: 0000 train_loss= 5.14881 val_roc= 0.71887 val_auprc= 0.68493 val_apk= 0.63966 time= 0.08805\n",
            "Epoch: 0009 Iter: 1051 Edge: 0003 train_loss= 3.38791 val_roc= 0.78392 val_auprc= 0.76452 val_apk= 0.90829 time= 0.08873\n",
            "Epoch: 0009 Iter: 1201 Edge: 0000 train_loss= 5.87032 val_roc= 0.70778 val_auprc= 0.66997 val_apk= 0.50318 time= 0.09335\n",
            "Epoch: 0009 Iter: 1351 Edge: 0003 train_loss= 4.06869 val_roc= 0.79207 val_auprc= 0.77035 val_apk= 0.88965 time= 0.09190\n",
            "Epoch: 0009 Iter: 1501 Edge: 0000 train_loss= 4.77762 val_roc= 0.71626 val_auprc= 0.68325 val_apk= 0.61799 time= 0.10283\n",
            "Epoch: 0009 Iter: 1651 Edge: 0003 train_loss= 2.98772 val_roc= 0.78826 val_auprc= 0.76960 val_apk= 0.89719 time= 0.08920\n",
            "Epoch: 0009 Iter: 1801 Edge: 0000 train_loss= 5.22640 val_roc= 0.71654 val_auprc= 0.67380 val_apk= 0.48289 time= 0.10072\n",
            "Epoch: 0009 Iter: 1951 Edge: 0003 train_loss= 4.21386 val_roc= 0.76389 val_auprc= 0.75147 val_apk= 0.89511 time= 0.09600\n",
            "Epoch: 0009 Iter: 2101 Edge: 0000 train_loss= 5.07549 val_roc= 0.73229 val_auprc= 0.68327 val_apk= 0.51344 time= 0.09548\n",
            "Epoch: 0009 Iter: 2251 Edge: 0003 train_loss= 4.96792 val_roc= 0.78374 val_auprc= 0.75895 val_apk= 0.78523 time= 0.08906\n",
            "Epoch: 0009 Iter: 2401 Edge: 0000 train_loss= 4.76388 val_roc= 0.71377 val_auprc= 0.67058 val_apk= 0.51411 time= 0.08805\n",
            "Epoch: 0010 Iter: 0001 Edge: 0000 train_loss= 3.99696 val_roc= 0.72922 val_auprc= 0.68131 val_apk= 0.47094 time= 0.10228\n",
            "Epoch: 0010 Iter: 0151 Edge: 0003 train_loss= 4.28152 val_roc= 0.77406 val_auprc= 0.75742 val_apk= 0.84795 time= 0.09148\n",
            "Epoch: 0010 Iter: 0301 Edge: 0000 train_loss= 4.89178 val_roc= 0.72929 val_auprc= 0.69002 val_apk= 0.62723 time= 0.08823\n",
            "Epoch: 0010 Iter: 0451 Edge: 0003 train_loss= 3.26178 val_roc= 0.77598 val_auprc= 0.76349 val_apk= 0.87374 time= 0.08792\n",
            "Epoch: 0010 Iter: 0601 Edge: 0000 train_loss= 5.49912 val_roc= 0.72438 val_auprc= 0.68010 val_apk= 0.48946 time= 0.08757\n",
            "Epoch: 0010 Iter: 0751 Edge: 0003 train_loss= 3.06566 val_roc= 0.76730 val_auprc= 0.74942 val_apk= 0.89758 time= 0.09366\n",
            "Epoch: 0010 Iter: 0901 Edge: 0000 train_loss= 5.16857 val_roc= 0.72987 val_auprc= 0.68541 val_apk= 0.53976 time= 0.11071\n",
            "Epoch: 0010 Iter: 1051 Edge: 0003 train_loss= 2.84704 val_roc= 0.77174 val_auprc= 0.75384 val_apk= 0.87097 time= 0.08873\n",
            "Epoch: 0010 Iter: 1201 Edge: 0000 train_loss= 4.93873 val_roc= 0.72220 val_auprc= 0.67130 val_apk= 0.42092 time= 0.08941\n",
            "Epoch: 0010 Iter: 1351 Edge: 0003 train_loss= 3.72076 val_roc= 0.78587 val_auprc= 0.77014 val_apk= 0.85143 time= 0.08921\n",
            "Epoch: 0010 Iter: 1501 Edge: 0000 train_loss= 4.52347 val_roc= 0.73196 val_auprc= 0.69280 val_apk= 0.53221 time= 0.09727\n",
            "Epoch: 0010 Iter: 1651 Edge: 0003 train_loss= 4.31962 val_roc= 0.79279 val_auprc= 0.77615 val_apk= 0.82932 time= 0.08925\n",
            "Epoch: 0010 Iter: 1801 Edge: 0000 train_loss= 4.51389 val_roc= 0.73036 val_auprc= 0.69287 val_apk= 0.55893 time= 0.08845\n",
            "Epoch: 0010 Iter: 1951 Edge: 0003 train_loss= 4.70919 val_roc= 0.77423 val_auprc= 0.75559 val_apk= 0.88033 time= 0.09264\n",
            "Epoch: 0010 Iter: 2101 Edge: 0000 train_loss= 5.07143 val_roc= 0.72701 val_auprc= 0.69533 val_apk= 0.60251 time= 0.08892\n",
            "Epoch: 0010 Iter: 2251 Edge: 0003 train_loss= 3.69922 val_roc= 0.78630 val_auprc= 0.75601 val_apk= 0.73929 time= 0.09093\n",
            "Epoch: 0010 Iter: 2401 Edge: 0000 train_loss= 3.56438 val_roc= 0.72606 val_auprc= 0.69920 val_apk= 0.66148 time= 0.08984\n",
            "Optimization finished!\n",
            "Edge type= [00, 00, 00]\n",
            "Edge type: 0000 Test AUROC score 0.71476\n",
            "Edge type: 0000 Test AUPRC score 0.68706\n",
            "Edge type: 0000 Test AP@k score 0.63549\n",
            "\n",
            "Edge type= [00, 00, 01]\n",
            "Edge type: 0001 Test AUROC score 0.76646\n",
            "Edge type: 0001 Test AUPRC score 0.73084\n",
            "Edge type: 0001 Test AP@k score 0.69764\n",
            "\n",
            "Edge type= [00, 01, 00]\n",
            "Edge type: 0002 Test AUROC score 0.71933\n",
            "Edge type: 0002 Test AUPRC score 0.69518\n",
            "Edge type: 0002 Test AP@k score 0.71317\n",
            "\n",
            "Edge type= [01, 00, 00]\n",
            "Edge type: 0003 Test AUROC score 0.73267\n",
            "Edge type: 0003 Test AUPRC score 0.71328\n",
            "Edge type: 0003 Test AP@k score 0.74978\n",
            "\n",
            "Edge type= [01, 01, 00]\n",
            "Edge type: 0004 Test AUROC score 0.68413\n",
            "Edge type: 0004 Test AUPRC score 0.65009\n",
            "Edge type: 0004 Test AP@k score 0.52575\n",
            "\n",
            "Edge type= [01, 01, 01]\n",
            "Edge type: 0005 Test AUROC score 0.77356\n",
            "Edge type: 0005 Test AUPRC score 0.71838\n",
            "Edge type: 0005 Test AP@k score 0.60002\n",
            "\n",
            "Edge type= [01, 01, 02]\n",
            "Edge type: 0006 Test AUROC score 0.79071\n",
            "Edge type: 0006 Test AUPRC score 0.75118\n",
            "Edge type: 0006 Test AP@k score 0.63571\n",
            "\n",
            "Edge type= [01, 01, 03]\n",
            "Edge type: 0007 Test AUROC score 0.68855\n",
            "Edge type: 0007 Test AUPRC score 0.65945\n",
            "Edge type: 0007 Test AP@k score 0.57096\n",
            "\n",
            "Edge type= [01, 01, 04]\n",
            "Edge type: 0008 Test AUROC score 0.77508\n",
            "Edge type: 0008 Test AUPRC score 0.72437\n",
            "Edge type: 0008 Test AP@k score 0.60196\n",
            "\n",
            "Edge type= [01, 01, 05]\n",
            "Edge type: 0009 Test AUROC score 0.82321\n",
            "Edge type: 0009 Test AUPRC score 0.79490\n",
            "Edge type: 0009 Test AP@k score 0.75741\n",
            "\n"
          ]
        }
      ]
    }
  ]
}