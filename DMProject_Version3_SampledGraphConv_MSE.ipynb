{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMProject-Version3-SampledGraphConv-MSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "t4zEC-3gLROH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "zPDFxiwWyTPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rijm4ze6LPl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90147c1f-cf01-47b4-d4b4-601a8bba8be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "import time\n",
        "import os\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from sklearn import metrics\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "G3YKFnJBLUdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sparse_mat(a2b, a2idx, b2idx):\n",
        "    n = len(a2idx)\n",
        "    m = len(b2idx)\n",
        "    assoc = np.zeros((n, m))\n",
        "    for a, b_assoc in a2b.iteritems():\n",
        "        if a not in a2idx:\n",
        "            continue\n",
        "        for b in b_assoc:\n",
        "            if b not in b2idx:\n",
        "                continue\n",
        "            assoc[a2idx[a], b2idx[b]] = 1.\n",
        "    assoc = sp.coo_matrix(assoc)\n",
        "    return assoc\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "Zxu8LLY8LxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "    This function computes the mean average precision at k between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])"
      ],
      "metadata": {
        "id": "_lxl2O1cL1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def zeros(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def ones(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "metadata": {
        "id": "Af-xGZdwLaQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "class MultiLayer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    # Properties    \n",
        "        name: String, defines the variable scope of the layer.\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, support=None, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.rank = 100\n",
        "\n",
        "        if support is None:\n",
        "            self.support = placeholders['support']\n",
        "        else:\n",
        "            self.support = support\n",
        "\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]])\n",
        "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])\n",
        "\n",
        "\n",
        "            # THIS PART IS TAKEN FROM SampledGraphConv in FastGCN    \n",
        "            sup = self.support[self.edge_type][k]\n",
        "            \n",
        "            norm_x = tf.nn.l2_normalize(x)\n",
        "            #norm_sup = tf.nn.l2_normalize(tf.sparse.to_dense(sup))\n",
        "\n",
        "            # should use norm_sup, but doesnt work for some idiotic tf reason\n",
        "            norm_mix = tf.sparse_tensor_dense_matmul(sup, x)\n",
        "            norm_mix = norm_mix*tf.transpose(tf.reduce_sum(norm_mix))\n",
        "            sampledIndex = tf.multinomial(tf.log(norm_mix), self.rank)\n",
        "\n",
        "            out = norm_mix \n",
        "            \n",
        "            outputs.append(self.act(out))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DEDICOMDecoder(MultiLayer):\n",
        "    \"\"\"DEDICOM Tensor Factorization Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            self.vars['global_interaction'] = weight_variable_glorot(\n",
        "                input_dim, input_dim, name='global_interaction')\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='local_variation_%d' % k)\n",
        "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
        "            product1 = tf.matmul(inputs_row, relation)\n",
        "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
        "            product3 = tf.matmul(product2, relation)\n",
        "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DistMultDecoder(MultiLayer):\n",
        "    \"\"\"DistMult Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DistMultDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='relation_%d' % k)\n",
        "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['relation_%d' % k])\n",
        "            intermediate_product = tf.matmul(inputs_row, relation)\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BilinearDecoder(MultiLayer):\n",
        "    \"\"\"Bilinear Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(BilinearDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, input_dim, name='relation_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AIyMpZb5LfMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "class EdgeMinibatchIterator(object):\n",
        "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
        "    random pairs of co-occuring edges.\n",
        "    assoc -- numpy array with target edges\n",
        "    placeholders -- tensorflow placeholders object\n",
        "    batch_size -- size of the minibatches\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats\n",
        "        self.feat = feat\n",
        "        self.edge_types = edge_types\n",
        "        self.batch_size = batch_size\n",
        "        self.val_test_size = val_test_size\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
        "        self.batch_num = [0]*self.num_edge_types\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r\n",
        "                self.idx2edge_type[r] = i, j, k\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        # Function to build test and val sets with val_test_size positive links\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                print(\"Minibatch edge type:\", \"(%d, %d, %d)\" % (i, j, k))\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "                print(\"Train edges=\", \"%04d\" % len(self.train_edges[i,j][k]))\n",
        "                print(\"Val edges=\", \"%04d\" % len(self.val_edges[i,j][k]))\n",
        "                print(\"Test edges=\", \"%04d\" % len(self.test_edges[i,j][k]))\n",
        "\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])\n",
        "        num_test = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "        num_val = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if len(test_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing test edges=\", \"%04d/%04d\" % (len(test_edges_false), len(test_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if len(val_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing val edges=\", \"%04d/%04d\" % (len(val_edges_false), len(val_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        # Re-build adj matrices\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def end(self):\n",
        "        finished = len(self.freebatch_edge_types) == 0\n",
        "        return finished\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "        # construct feed dictionary\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
        "        while True:\n",
        "            if self.iter % 4 == 0:\n",
        "                # gene-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "            elif self.iter % 4 == 1:\n",
        "                # gene-drug relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
        "            elif self.iter % 4 == 2:\n",
        "                # drug-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
        "            else:\n",
        "                # random side effect relation\n",
        "                if len(self.freebatch_edge_types) > 0:\n",
        "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
        "                else:\n",
        "                    self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "                    self.iter = 0\n",
        "\n",
        "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
        "                break\n",
        "            else:\n",
        "                if self.iter % 4 in [0, 1, 2]:\n",
        "                    self.batch_num[self.current_edge_type_idx] = 0\n",
        "                else:\n",
        "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
        "\n",
        "        self.iter += 1\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "    def num_training_batches(self, edge_type, type_idx):\n",
        "        return len(self.train_edges[edge_type][type_idx]) // self.batch_size + 1\n",
        "\n",
        "    def val_feed_dict(self, edge_type, type_idx, placeholders, size=None):\n",
        "        edge_list = self.val_edges[edge_type][type_idx]\n",
        "        if size is None:\n",
        "            return self.batch_feed_dict(edge_list, edge_type, placeholders)\n",
        "        else:\n",
        "            ind = np.random.permutation(len(edge_list))\n",
        "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
        "            return self.batch_feed_dict(val_edges, edge_type, placeholders)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\" Re-shuffle the training set.\n",
        "            Also reset the batch number.\n",
        "        \"\"\"\n",
        "        for edge_type in self.edge_types:\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                self.train_edges[edge_type][k] = np.random.permutation(self.train_edges[edge_type][k])\n",
        "                self.batch_num[self.edge_type2idx[edge_type[0], edge_type[1], k]] = 0\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.freebatch_edge_types = list(range(self.num_edge_types))\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 0, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 1, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[1, 0, 0])\n",
        "        self.iter = 0"
      ],
      "metadata": {
        "id": "3FIO8W8tLjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ANMT3GmPLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "OibFC-vT2lcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1\n",
        "        self.decoders = decoders\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types}\n",
        "        self.input_dim = num_feat\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.support = placeholders['support']\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout, support=self.support,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "\n",
        "        for i, hid1 in self.hidden1.items():\n",
        "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=FLAGS.hidden1, output_dim=FLAGS.hidden2,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'distmult':\n",
        "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'bilinear':\n",
        "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'dedicom':\n",
        "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'distmult':\n",
        "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'bilinear':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'dedicom':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
        "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)"
      ],
      "metadata": {
        "id": "NvyZI6NFLnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decagon Optimizer**"
      ],
      "metadata": {
        "id": "bcuYtIKOLggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "        neg_samples_list = []\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                    true_classes=labels,\n",
        "                    num_true=1,\n",
        "                    num_sampled=self.batch_size,\n",
        "                    unique=False,\n",
        "                    range_max=len(self.degrees[i][k]),\n",
        "                    distortion=0.75,\n",
        "                    unigrams=self.degrees[i][k].tolist())\n",
        "                neg_samples_list.append(neg_samples)\n",
        "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
        "        self.outputs = tf.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "        return preds\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._mse_loss(self.outputs, self.neg_outputs)\n",
        "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "    def _mse_loss(self, aff, neg_aff):\n",
        "        #Mean squared error\n",
        "        diff = tf.nn.relu((tf.subtract(neg_aff, tf.expand_dims(aff, 0))), name='diff')\n",
        "        loss = tf.reduce_sum(tf.math.square(diff))\n",
        "        return loss  \n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        \"\"\"Cross-entropy optimization.\"\"\"\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def gather_cols(params, indices, name=None):\n",
        "    \"\"\"Gather columns of a 2D tensor.\n",
        "    Args:\n",
        "        params: A 2D tensor.\n",
        "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
        "        name: A name for the operation (optional).\n",
        "    Returns:\n",
        "        A 2D Tensor. Has the same type as ``params``.\n",
        "    \"\"\"\n",
        "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "metadata": {
        "id": "hftqMu6_LrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "ruZ8gtknLkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU (hide GPU) due to memory constraints\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "# Train on GPU\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def prepare_data(placeholders, edge_types):\n",
        "    adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in edge_types.items()}\n",
        "\n",
        "    return adj_mats\n",
        "    \n",
        "    \n",
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=50)\n",
        "\n",
        "    return roc_sc, aupr_sc, apk_sc\n",
        "\n",
        "\n",
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'support': tf.sparse_placeholder(tf.float32),\n",
        "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.placeholder(tf.int32),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    }\n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    placeholders.update({'support': prepare_data(placeholders, edge_types)})\n",
        "    return placeholders\n",
        "\n",
        "val_test_size = 0.05\n",
        "n_genes = 500\n",
        "n_drugs = 400\n",
        "n_drugdrug_rel_types = 3\n",
        "gene_net = nx.planted_partition_graph(50, 10, 0.2, 0.05, seed=42)\n",
        "\n",
        "gene_adj = nx.adjacency_matrix(gene_net)\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "\n",
        "gene_drug_adj = sp.csr_matrix((10 * np.random.randn(n_genes, n_drugs) > 15).astype(int))\n",
        "drug_gene_adj = gene_drug_adj.transpose(copy=True)\n",
        "\n",
        "drug_drug_adj_list = []\n",
        "tmp = np.dot(drug_gene_adj, gene_drug_adj)\n",
        "for i in range(n_drugdrug_rel_types):\n",
        "    mat = np.zeros((n_drugs, n_drugs))\n",
        "    for d1, d2 in combinations(list(range(n_drugs)), 2):\n",
        "        if tmp[d1, d2] == i + 4:\n",
        "            mat[d1, d2] = mat[d2, d1] = 1.\n",
        "    drug_drug_adj_list.append(sp.csr_matrix(mat))\n",
        "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
        "\n",
        "\n",
        "# data representation\n",
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_drug_adj],\n",
        "    (1, 0): [drug_gene_adj],\n",
        "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: drug_degrees_list + drug_degrees_list,\n",
        "}\n",
        "\n",
        "# featureless (genes)\n",
        "gene_feat = sp.identity(n_genes)\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "# features (drugs)\n",
        "drug_feat = sp.identity(n_drugs)\n",
        "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
        "drug_feat = sparse_to_tuple(drug_feat.tocoo())\n",
        "\n",
        "# data representation\n",
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: drug_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: drug_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: drug_feat,\n",
        "}\n",
        "\n",
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'bilinear',\n",
        "    (0, 1): 'bilinear',\n",
        "    (1, 0): 'bilinear',\n",
        "    (1, 1): 'dedicom',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
        "flags.DEFINE_integer('batch_size', 100, 'minibatch size.')  # Changed from 512\n",
        "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
        "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
        "# substantial amount of time\n",
        "PRINT_PROGRESS_EVERY = 150\n",
        "\n",
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)\n",
        "\n",
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "    adj_mats=adj_mats_orig,\n",
        "    feat=feat,\n",
        "    edge_types=edge_types,\n",
        "    #batch_size=FLAGS.batch_size,\n",
        "    val_test_size=val_test_size\n",
        ")\n",
        "\n",
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "    placeholders=placeholders,\n",
        "    num_feat=num_feat,\n",
        "    nonzero_feat=nonzero_feat,\n",
        "    edge_types=edge_types,\n",
        "    decoders=edge_type2decoder\n",
        ")\n",
        "\n",
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "    opt = DecagonOptimizer(\n",
        "        embeddings=model.embeddings,\n",
        "        latent_inters=model.latent_inters,\n",
        "        latent_varies=model.latent_varies,\n",
        "        degrees=degrees,\n",
        "        edge_types=edge_types,\n",
        "        edge_type2dim=edge_type2dim,\n",
        "        placeholders=placeholders,\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        margin=FLAGS.max_margin\n",
        "    )\n",
        "\n",
        "print(\"Initialize session\")\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "feed_dict = {}\n",
        "\n",
        "print(\"Train model\")\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    minibatch.shuffle()\n",
        "    itr = 0\n",
        "    while not minibatch.end():\n",
        "        # Construct feed dictionary\n",
        "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "        feed_dict = minibatch.update_feed_dict(\n",
        "            feed_dict=feed_dict,\n",
        "            dropout=FLAGS.dropout,\n",
        "            placeholders=placeholders)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        # Training step: run single weight update\n",
        "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
        "        train_cost = outs[1]\n",
        "        batch_edge_type = outs[2]\n",
        "\n",
        "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
        "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false,\n",
        "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
        "\n",
        "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
        "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
        "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "print(\"Optimization finished!\")\n",
        "\n",
        "for et in range(num_edge_types):\n",
        "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
        "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tWB8QcL-Nd",
        "outputId": "a9db42dd-8649-4a2a-9687-8bf42e43a382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge types: 10\n",
            "Defining placeholders\n",
            "Create minibatch iterator\n",
            "Minibatch edge type: (0, 0, 0)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 0, 1)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 1, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 0, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 1, 0)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 1)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 2)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Minibatch edge type: (1, 1, 3)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 4)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 5)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Create model\n",
            "Create optimizer\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize session\n",
            "Train model\n",
            "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 1.87780 val_roc= 0.50254 val_auprc= 0.51475 val_apk= 0.38630 time= 1.54675\n",
            "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 0.00884 val_roc= 0.48841 val_auprc= 0.49471 val_apk= 0.27299 time= 0.10052\n",
            "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 0.00350 val_roc= 0.52873 val_auprc= 0.53674 val_apk= 0.51718 time= 0.09498\n",
            "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 0.00142 val_roc= 0.48777 val_auprc= 0.49403 val_apk= 0.25521 time= 0.09728\n",
            "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 0.00094 val_roc= 0.53101 val_auprc= 0.53690 val_apk= 0.48661 time= 0.09644\n",
            "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 0.00113 val_roc= 0.48562 val_auprc= 0.49171 val_apk= 0.25825 time= 0.09772\n",
            "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 0.00074 val_roc= 0.52958 val_auprc= 0.53750 val_apk= 0.48299 time= 0.09564\n",
            "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 0.00076 val_roc= 0.48530 val_auprc= 0.49172 val_apk= 0.25265 time= 0.11976\n",
            "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 0.00052 val_roc= 0.52817 val_auprc= 0.53516 val_apk= 0.47541 time= 0.09536\n",
            "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 0.00051 val_roc= 0.48493 val_auprc= 0.49140 val_apk= 0.25813 time= 0.09113\n",
            "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 0.00036 val_roc= 0.52972 val_auprc= 0.53654 val_apk= 0.47947 time= 0.09763\n",
            "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 0.00065 val_roc= 0.48466 val_auprc= 0.49098 val_apk= 0.26063 time= 0.09334\n",
            "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 0.00025 val_roc= 0.52848 val_auprc= 0.53639 val_apk= 0.45479 time= 0.10773\n",
            "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 0.00061 val_roc= 0.48380 val_auprc= 0.49040 val_apk= 0.25907 time= 0.11041\n",
            "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 0.00020 val_roc= 0.53063 val_auprc= 0.53743 val_apk= 0.46383 time= 0.09415\n",
            "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 0.00027 val_roc= 0.48359 val_auprc= 0.48976 val_apk= 0.24278 time= 0.09624\n",
            "Epoch: 0001 Iter: 2401 Edge: 0000 train_loss= 0.00018 val_roc= 0.52818 val_auprc= 0.53447 val_apk= 0.42868 time= 0.09374\n",
            "Epoch: 0002 Iter: 0001 Edge: 0000 train_loss= 0.00017 val_roc= 0.52645 val_auprc= 0.53378 val_apk= 0.44328 time= 0.10031\n",
            "Epoch: 0002 Iter: 0151 Edge: 0003 train_loss= 0.00012 val_roc= 0.48283 val_auprc= 0.48937 val_apk= 0.25572 time= 0.09870\n",
            "Epoch: 0002 Iter: 0301 Edge: 0000 train_loss= 0.00016 val_roc= 0.52462 val_auprc= 0.53390 val_apk= 0.43781 time= 0.10155\n",
            "Epoch: 0002 Iter: 0451 Edge: 0003 train_loss= 0.00018 val_roc= 0.48285 val_auprc= 0.48944 val_apk= 0.25541 time= 0.09638\n",
            "Epoch: 0002 Iter: 0601 Edge: 0000 train_loss= 0.00012 val_roc= 0.52805 val_auprc= 0.53546 val_apk= 0.41463 time= 0.10434\n",
            "Epoch: 0002 Iter: 0751 Edge: 0003 train_loss= 0.00008 val_roc= 0.48233 val_auprc= 0.48881 val_apk= 0.27457 time= 0.09861\n",
            "Epoch: 0002 Iter: 0901 Edge: 0000 train_loss= 0.00009 val_roc= 0.52526 val_auprc= 0.53574 val_apk= 0.42953 time= 0.09644\n",
            "Epoch: 0002 Iter: 1051 Edge: 0003 train_loss= 0.00010 val_roc= 0.48209 val_auprc= 0.48921 val_apk= 0.25600 time= 0.09407\n",
            "Epoch: 0002 Iter: 1201 Edge: 0000 train_loss= 0.00009 val_roc= 0.52930 val_auprc= 0.53754 val_apk= 0.47202 time= 0.09687\n",
            "Epoch: 0002 Iter: 1351 Edge: 0003 train_loss= 0.00012 val_roc= 0.48232 val_auprc= 0.48848 val_apk= 0.25870 time= 0.09056\n",
            "Epoch: 0002 Iter: 1501 Edge: 0000 train_loss= 0.00011 val_roc= 0.52369 val_auprc= 0.53375 val_apk= 0.41237 time= 0.10166\n",
            "Epoch: 0002 Iter: 1651 Edge: 0003 train_loss= 0.00009 val_roc= 0.48235 val_auprc= 0.48844 val_apk= 0.25847 time= 0.09355\n",
            "Epoch: 0002 Iter: 1801 Edge: 0000 train_loss= 0.00008 val_roc= 0.52305 val_auprc= 0.53265 val_apk= 0.41487 time= 0.09202\n",
            "Epoch: 0002 Iter: 1951 Edge: 0003 train_loss= 0.00010 val_roc= 0.48149 val_auprc= 0.48785 val_apk= 0.26631 time= 0.09450\n",
            "Epoch: 0002 Iter: 2101 Edge: 0000 train_loss= 0.00006 val_roc= 0.52583 val_auprc= 0.53567 val_apk= 0.43485 time= 0.09749\n",
            "Epoch: 0002 Iter: 2251 Edge: 0003 train_loss= 0.00003 val_roc= 0.48201 val_auprc= 0.48862 val_apk= 0.26951 time= 0.09949\n",
            "Epoch: 0002 Iter: 2401 Edge: 0000 train_loss= 0.00004 val_roc= 0.52687 val_auprc= 0.53306 val_apk= 0.40597 time= 0.09103\n",
            "Epoch: 0003 Iter: 0001 Edge: 0000 train_loss= 0.00006 val_roc= 0.52427 val_auprc= 0.53283 val_apk= 0.39672 time= 0.10509\n",
            "Epoch: 0003 Iter: 0151 Edge: 0003 train_loss= 0.00007 val_roc= 0.48195 val_auprc= 0.48794 val_apk= 0.25174 time= 0.12451\n",
            "Epoch: 0003 Iter: 0301 Edge: 0000 train_loss= 0.00004 val_roc= 0.52605 val_auprc= 0.53517 val_apk= 0.40742 time= 0.08981\n",
            "Epoch: 0003 Iter: 0451 Edge: 0003 train_loss= 0.00005 val_roc= 0.48192 val_auprc= 0.48816 val_apk= 0.24014 time= 0.10916\n",
            "Epoch: 0003 Iter: 0601 Edge: 0000 train_loss= 0.00002 val_roc= 0.52329 val_auprc= 0.53410 val_apk= 0.43000 time= 0.10341\n",
            "Epoch: 0003 Iter: 0751 Edge: 0003 train_loss= 0.00002 val_roc= 0.48091 val_auprc= 0.48639 val_apk= 0.23261 time= 0.09041\n",
            "Epoch: 0003 Iter: 0901 Edge: 0000 train_loss= 0.00004 val_roc= 0.52643 val_auprc= 0.53510 val_apk= 0.41064 time= 0.09847\n",
            "Epoch: 0003 Iter: 1051 Edge: 0003 train_loss= 0.00003 val_roc= 0.48177 val_auprc= 0.48680 val_apk= 0.22693 time= 0.12451\n",
            "Epoch: 0003 Iter: 1201 Edge: 0000 train_loss= 0.00003 val_roc= 0.52382 val_auprc= 0.53255 val_apk= 0.41398 time= 0.09139\n",
            "Epoch: 0003 Iter: 1351 Edge: 0003 train_loss= 0.00003 val_roc= 0.48201 val_auprc= 0.48689 val_apk= 0.22885 time= 0.09490\n",
            "Epoch: 0003 Iter: 1501 Edge: 0000 train_loss= 0.00002 val_roc= 0.52613 val_auprc= 0.53267 val_apk= 0.43172 time= 0.10004\n",
            "Epoch: 0003 Iter: 1651 Edge: 0003 train_loss= 0.00004 val_roc= 0.48165 val_auprc= 0.48666 val_apk= 0.22557 time= 0.10027\n",
            "Epoch: 0003 Iter: 1801 Edge: 0000 train_loss= 0.00002 val_roc= 0.52460 val_auprc= 0.53048 val_apk= 0.39186 time= 0.09529\n",
            "Epoch: 0003 Iter: 1951 Edge: 0003 train_loss= 0.00001 val_roc= 0.48086 val_auprc= 0.48683 val_apk= 0.23562 time= 0.09354\n",
            "Epoch: 0003 Iter: 2101 Edge: 0000 train_loss= 0.00002 val_roc= 0.52122 val_auprc= 0.52577 val_apk= 0.32613 time= 0.10244\n",
            "Epoch: 0003 Iter: 2251 Edge: 0003 train_loss= 0.00001 val_roc= 0.48180 val_auprc= 0.48636 val_apk= 0.21087 time= 0.10447\n",
            "Epoch: 0003 Iter: 2401 Edge: 0000 train_loss= 0.00002 val_roc= 0.52215 val_auprc= 0.52615 val_apk= 0.37988 time= 0.09790\n",
            "Epoch: 0004 Iter: 0001 Edge: 0000 train_loss= 0.00001 val_roc= 0.52314 val_auprc= 0.52881 val_apk= 0.41832 time= 0.09290\n",
            "Epoch: 0004 Iter: 0151 Edge: 0003 train_loss= 0.00001 val_roc= 0.48316 val_auprc= 0.48680 val_apk= 0.19366 time= 0.09537\n",
            "Epoch: 0004 Iter: 0301 Edge: 0000 train_loss= 0.00002 val_roc= 0.52180 val_auprc= 0.52487 val_apk= 0.33613 time= 0.09474\n",
            "Epoch: 0004 Iter: 0451 Edge: 0003 train_loss= 0.00002 val_roc= 0.48387 val_auprc= 0.48756 val_apk= 0.19961 time= 0.12776\n",
            "Epoch: 0004 Iter: 0601 Edge: 0000 train_loss= 0.00001 val_roc= 0.51810 val_auprc= 0.52459 val_apk= 0.34093 time= 0.09985\n",
            "Epoch: 0004 Iter: 0751 Edge: 0003 train_loss= 0.00001 val_roc= 0.48371 val_auprc= 0.48699 val_apk= 0.19823 time= 0.09727\n",
            "Epoch: 0004 Iter: 0901 Edge: 0000 train_loss= 0.00001 val_roc= 0.52023 val_auprc= 0.52686 val_apk= 0.37215 time= 0.09357\n",
            "Epoch: 0004 Iter: 1051 Edge: 0003 train_loss= 0.00002 val_roc= 0.48326 val_auprc= 0.48617 val_apk= 0.18788 time= 0.09111\n",
            "Epoch: 0004 Iter: 1201 Edge: 0000 train_loss= 0.00001 val_roc= 0.51855 val_auprc= 0.52444 val_apk= 0.36273 time= 0.10044\n",
            "Epoch: 0004 Iter: 1351 Edge: 0003 train_loss= 0.00001 val_roc= 0.48351 val_auprc= 0.48746 val_apk= 0.19862 time= 0.09866\n",
            "Epoch: 0004 Iter: 1501 Edge: 0000 train_loss= 0.00001 val_roc= 0.51661 val_auprc= 0.52186 val_apk= 0.34748 time= 0.10066\n",
            "Epoch: 0004 Iter: 1651 Edge: 0003 train_loss= 0.00001 val_roc= 0.48434 val_auprc= 0.48886 val_apk= 0.20833 time= 0.09730\n",
            "Epoch: 0004 Iter: 1801 Edge: 0000 train_loss= 0.00001 val_roc= 0.51771 val_auprc= 0.52477 val_apk= 0.38026 time= 0.09331\n",
            "Epoch: 0004 Iter: 1951 Edge: 0003 train_loss= 0.00001 val_roc= 0.48286 val_auprc= 0.48710 val_apk= 0.18798 time= 0.09404\n",
            "Epoch: 0004 Iter: 2101 Edge: 0000 train_loss= 0.00001 val_roc= 0.52033 val_auprc= 0.52473 val_apk= 0.36894 time= 0.09784\n",
            "Epoch: 0004 Iter: 2251 Edge: 0003 train_loss= 0.00001 val_roc= 0.48369 val_auprc= 0.48785 val_apk= 0.17282 time= 0.16195\n",
            "Epoch: 0004 Iter: 2401 Edge: 0000 train_loss= 0.00001 val_roc= 0.51785 val_auprc= 0.52129 val_apk= 0.32130 time= 0.10572\n",
            "Epoch: 0005 Iter: 0001 Edge: 0000 train_loss= 0.00001 val_roc= 0.51766 val_auprc= 0.52038 val_apk= 0.34746 time= 0.09442\n",
            "Epoch: 0005 Iter: 0151 Edge: 0003 train_loss= 0.00001 val_roc= 0.48414 val_auprc= 0.48802 val_apk= 0.17405 time= 0.11321\n",
            "Epoch: 0005 Iter: 0301 Edge: 0000 train_loss= 0.00000 val_roc= 0.51643 val_auprc= 0.52125 val_apk= 0.31138 time= 0.10896\n",
            "Epoch: 0005 Iter: 0451 Edge: 0003 train_loss= 0.00000 val_roc= 0.48434 val_auprc= 0.48888 val_apk= 0.17860 time= 0.09832\n",
            "Epoch: 0005 Iter: 0601 Edge: 0000 train_loss= 0.00000 val_roc= 0.51216 val_auprc= 0.51528 val_apk= 0.28303 time= 0.08994\n",
            "Epoch: 0005 Iter: 0751 Edge: 0003 train_loss= 0.00000 val_roc= 0.48454 val_auprc= 0.48778 val_apk= 0.17448 time= 0.10073\n",
            "Epoch: 0005 Iter: 0901 Edge: 0000 train_loss= 0.00000 val_roc= 0.51227 val_auprc= 0.51364 val_apk= 0.25652 time= 0.09394\n",
            "Epoch: 0005 Iter: 1051 Edge: 0003 train_loss= 0.00001 val_roc= 0.48538 val_auprc= 0.48997 val_apk= 0.19697 time= 0.09350\n",
            "Epoch: 0005 Iter: 1201 Edge: 0000 train_loss= 0.00000 val_roc= 0.50922 val_auprc= 0.51355 val_apk= 0.30384 time= 0.10770\n",
            "Epoch: 0005 Iter: 1351 Edge: 0003 train_loss= 0.00000 val_roc= 0.48648 val_auprc= 0.49063 val_apk= 0.18490 time= 0.09474\n",
            "Epoch: 0005 Iter: 1501 Edge: 0000 train_loss= 0.00000 val_roc= 0.51204 val_auprc= 0.51542 val_apk= 0.28419 time= 0.11936\n",
            "Epoch: 0005 Iter: 1651 Edge: 0003 train_loss= 0.00000 val_roc= 0.48769 val_auprc= 0.49133 val_apk= 0.22696 time= 0.09364\n",
            "Epoch: 0005 Iter: 1801 Edge: 0000 train_loss= 0.00000 val_roc= 0.50589 val_auprc= 0.51128 val_apk= 0.32213 time= 0.09261\n",
            "Epoch: 0005 Iter: 1951 Edge: 0003 train_loss= 0.00000 val_roc= 0.48914 val_auprc= 0.49143 val_apk= 0.21291 time= 0.09189\n",
            "Epoch: 0005 Iter: 2101 Edge: 0000 train_loss= 0.00000 val_roc= 0.51283 val_auprc= 0.51393 val_apk= 0.30723 time= 0.09304\n",
            "Epoch: 0005 Iter: 2251 Edge: 0003 train_loss= 0.00000 val_roc= 0.48890 val_auprc= 0.49070 val_apk= 0.21074 time= 0.16042\n",
            "Epoch: 0005 Iter: 2401 Edge: 0000 train_loss= 0.00000 val_roc= 0.50765 val_auprc= 0.50941 val_apk= 0.29565 time= 0.10902\n",
            "Epoch: 0006 Iter: 0001 Edge: 0000 train_loss= 0.00000 val_roc= 0.50028 val_auprc= 0.50351 val_apk= 0.29420 time= 0.09747\n",
            "Epoch: 0006 Iter: 0151 Edge: 0003 train_loss= 0.00000 val_roc= 0.48657 val_auprc= 0.48884 val_apk= 0.21073 time= 0.10585\n",
            "Epoch: 0006 Iter: 0301 Edge: 0000 train_loss= 0.00000 val_roc= 0.50613 val_auprc= 0.50689 val_apk= 0.25172 time= 0.10473\n",
            "Epoch: 0006 Iter: 0451 Edge: 0003 train_loss= 0.00000 val_roc= 0.48526 val_auprc= 0.48948 val_apk= 0.21600 time= 0.10806\n",
            "Epoch: 0006 Iter: 0601 Edge: 0000 train_loss= 0.00000 val_roc= 0.50698 val_auprc= 0.50762 val_apk= 0.23313 time= 0.10447\n",
            "Epoch: 0006 Iter: 0751 Edge: 0003 train_loss= 0.00000 val_roc= 0.48397 val_auprc= 0.48899 val_apk= 0.24145 time= 0.09486\n",
            "Epoch: 0006 Iter: 0901 Edge: 0000 train_loss= 0.00000 val_roc= 0.49641 val_auprc= 0.50087 val_apk= 0.24801 time= 0.10124\n",
            "Epoch: 0006 Iter: 1051 Edge: 0003 train_loss= 0.00000 val_roc= 0.48673 val_auprc= 0.49037 val_apk= 0.21351 time= 0.09271\n",
            "Epoch: 0006 Iter: 1201 Edge: 0000 train_loss= 0.00000 val_roc= 0.49473 val_auprc= 0.50392 val_apk= 0.33720 time= 0.09063\n",
            "Epoch: 0006 Iter: 1351 Edge: 0003 train_loss= 0.00000 val_roc= 0.48633 val_auprc= 0.49297 val_apk= 0.25092 time= 0.09942\n",
            "Epoch: 0006 Iter: 1501 Edge: 0000 train_loss= 0.00000 val_roc= 0.49674 val_auprc= 0.50561 val_apk= 0.32605 time= 0.09045\n",
            "Epoch: 0006 Iter: 1651 Edge: 0003 train_loss= 0.00000 val_roc= 0.48959 val_auprc= 0.49539 val_apk= 0.24450 time= 0.09759\n",
            "Epoch: 0006 Iter: 1801 Edge: 0000 train_loss= 0.00000 val_roc= 0.50001 val_auprc= 0.50536 val_apk= 0.26323 time= 0.11625\n",
            "Epoch: 0006 Iter: 1951 Edge: 0003 train_loss= 0.00000 val_roc= 0.48860 val_auprc= 0.49410 val_apk= 0.23142 time= 0.09999\n",
            "Epoch: 0006 Iter: 2101 Edge: 0000 train_loss= 0.00000 val_roc= 0.50080 val_auprc= 0.50383 val_apk= 0.28096 time= 0.16209\n",
            "Epoch: 0006 Iter: 2251 Edge: 0003 train_loss= 0.00000 val_roc= 0.48641 val_auprc= 0.49581 val_apk= 0.24630 time= 0.10316\n",
            "Epoch: 0006 Iter: 2401 Edge: 0000 train_loss= 0.00000 val_roc= 0.50013 val_auprc= 0.50797 val_apk= 0.33769 time= 0.09242\n",
            "Epoch: 0007 Iter: 0001 Edge: 0000 train_loss= 0.00000 val_roc= 0.49951 val_auprc= 0.50595 val_apk= 0.30453 time= 0.09326\n",
            "Epoch: 0007 Iter: 0151 Edge: 0003 train_loss= 0.00000 val_roc= 0.48521 val_auprc= 0.49283 val_apk= 0.22857 time= 0.09372\n",
            "Epoch: 0007 Iter: 0301 Edge: 0000 train_loss= 0.00000 val_roc= 0.49978 val_auprc= 0.50388 val_apk= 0.29833 time= 0.10788\n",
            "Epoch: 0007 Iter: 0451 Edge: 0003 train_loss= 0.00000 val_roc= 0.48762 val_auprc= 0.49513 val_apk= 0.26421 time= 0.10374\n",
            "Epoch: 0007 Iter: 0601 Edge: 0000 train_loss= 0.00000 val_roc= 0.50192 val_auprc= 0.50762 val_apk= 0.29633 time= 0.09282\n",
            "Epoch: 0007 Iter: 0751 Edge: 0003 train_loss= 0.00000 val_roc= 0.48729 val_auprc= 0.49466 val_apk= 0.26803 time= 0.10476\n",
            "Epoch: 0007 Iter: 0901 Edge: 0000 train_loss= 0.00000 val_roc= 0.50001 val_auprc= 0.50060 val_apk= 0.26022 time= 0.09539\n",
            "Epoch: 0007 Iter: 1051 Edge: 0003 train_loss= 0.00000 val_roc= 0.48549 val_auprc= 0.49133 val_apk= 0.22615 time= 0.10132\n",
            "Epoch: 0007 Iter: 1201 Edge: 0000 train_loss= 0.00000 val_roc= 0.50560 val_auprc= 0.51199 val_apk= 0.34315 time= 0.09607\n",
            "Epoch: 0007 Iter: 1351 Edge: 0003 train_loss= 0.00000 val_roc= 0.48623 val_auprc= 0.49152 val_apk= 0.21375 time= 0.09416\n",
            "Epoch: 0007 Iter: 1501 Edge: 0000 train_loss= 0.00000 val_roc= 0.50529 val_auprc= 0.50609 val_apk= 0.30406 time= 0.09125\n",
            "Epoch: 0007 Iter: 1651 Edge: 0003 train_loss= 0.00000 val_roc= 0.48893 val_auprc= 0.49295 val_apk= 0.20975 time= 0.09597\n",
            "Epoch: 0007 Iter: 1801 Edge: 0000 train_loss= 0.00000 val_roc= 0.50980 val_auprc= 0.51711 val_apk= 0.36953 time= 0.09889\n",
            "Epoch: 0007 Iter: 1951 Edge: 0003 train_loss= 0.00000 val_roc= 0.48964 val_auprc= 0.49515 val_apk= 0.22268 time= 0.09881\n",
            "Epoch: 0007 Iter: 2101 Edge: 0000 train_loss= 0.00000 val_roc= 0.50505 val_auprc= 0.51205 val_apk= 0.33116 time= 0.10148\n",
            "Epoch: 0007 Iter: 2251 Edge: 0003 train_loss= 0.00000 val_roc= 0.49221 val_auprc= 0.49840 val_apk= 0.20370 time= 0.10879\n",
            "Epoch: 0007 Iter: 2401 Edge: 0000 train_loss= 0.00000 val_roc= 0.50864 val_auprc= 0.51443 val_apk= 0.33070 time= 0.10049\n",
            "Epoch: 0008 Iter: 0001 Edge: 0000 train_loss= 0.00000 val_roc= 0.49570 val_auprc= 0.50050 val_apk= 0.27621 time= 0.09322\n",
            "Epoch: 0008 Iter: 0151 Edge: 0003 train_loss= 0.00000 val_roc= 0.49101 val_auprc= 0.49539 val_apk= 0.18760 time= 0.09185\n",
            "Epoch: 0008 Iter: 0301 Edge: 0000 train_loss= 0.00000 val_roc= 0.49391 val_auprc= 0.50225 val_apk= 0.28177 time= 0.10825\n",
            "Epoch: 0008 Iter: 0451 Edge: 0003 train_loss= 0.00000 val_roc= 0.49555 val_auprc= 0.49927 val_apk= 0.22322 time= 0.09034\n",
            "Epoch: 0008 Iter: 0601 Edge: 0000 train_loss= 0.00000 val_roc= 0.48765 val_auprc= 0.49020 val_apk= 0.23167 time= 0.10908\n",
            "Epoch: 0008 Iter: 0751 Edge: 0003 train_loss= 0.00000 val_roc= 0.49571 val_auprc= 0.50193 val_apk= 0.24015 time= 0.09485\n",
            "Epoch: 0008 Iter: 0901 Edge: 0000 train_loss= 0.00000 val_roc= 0.50176 val_auprc= 0.49737 val_apk= 0.24714 time= 0.09082\n",
            "Epoch: 0008 Iter: 1051 Edge: 0003 train_loss= 0.00000 val_roc= 0.49425 val_auprc= 0.49591 val_apk= 0.21813 time= 0.09501\n",
            "Epoch: 0008 Iter: 1201 Edge: 0000 train_loss= 0.00000 val_roc= 0.48762 val_auprc= 0.49399 val_apk= 0.24935 time= 0.09192\n",
            "Epoch: 0008 Iter: 1351 Edge: 0003 train_loss= 0.00000 val_roc= 0.49980 val_auprc= 0.50104 val_apk= 0.22555 time= 0.09106\n",
            "Epoch: 0008 Iter: 1501 Edge: 0000 train_loss= 0.00000 val_roc= 0.48641 val_auprc= 0.48860 val_apk= 0.19397 time= 0.10457\n",
            "Epoch: 0008 Iter: 1651 Edge: 0003 train_loss= 0.00000 val_roc= 0.49946 val_auprc= 0.50207 val_apk= 0.23287 time= 0.14234\n",
            "Epoch: 0008 Iter: 1801 Edge: 0000 train_loss= 0.00000 val_roc= 0.48265 val_auprc= 0.48906 val_apk= 0.24384 time= 0.09049\n",
            "Epoch: 0008 Iter: 1951 Edge: 0003 train_loss= 0.00000 val_roc= 0.49889 val_auprc= 0.49798 val_apk= 0.19264 time= 0.11069\n",
            "Epoch: 0008 Iter: 2101 Edge: 0000 train_loss= 0.00000 val_roc= 0.48797 val_auprc= 0.48496 val_apk= 0.21302 time= 0.09377\n",
            "Epoch: 0008 Iter: 2251 Edge: 0003 train_loss= 0.00000 val_roc= 0.49666 val_auprc= 0.49656 val_apk= 0.19534 time= 0.09454\n",
            "Epoch: 0008 Iter: 2401 Edge: 0000 train_loss= 0.00000 val_roc= 0.50054 val_auprc= 0.50265 val_apk= 0.21786 time= 0.09111\n",
            "Epoch: 0009 Iter: 0001 Edge: 0000 train_loss= 0.00000 val_roc= 0.49607 val_auprc= 0.50632 val_apk= 0.30986 time= 0.09508\n",
            "Epoch: 0009 Iter: 0151 Edge: 0003 train_loss= 0.00000 val_roc= 0.49474 val_auprc= 0.49829 val_apk= 0.23789 time= 0.10404\n",
            "Epoch: 0009 Iter: 0301 Edge: 0000 train_loss= 0.00000 val_roc= 0.50520 val_auprc= 0.49994 val_apk= 0.17485 time= 0.08744\n",
            "Epoch: 0009 Iter: 0451 Edge: 0003 train_loss= 0.00000 val_roc= 0.49516 val_auprc= 0.49655 val_apk= 0.22427 time= 0.10956\n",
            "Epoch: 0009 Iter: 0601 Edge: 0000 train_loss= 0.00000 val_roc= 0.49352 val_auprc= 0.50384 val_apk= 0.39271 time= 0.09505\n",
            "Epoch: 0009 Iter: 0751 Edge: 0003 train_loss= 0.00000 val_roc= 0.49556 val_auprc= 0.50014 val_apk= 0.27302 time= 0.11059\n",
            "Epoch: 0009 Iter: 0901 Edge: 0000 train_loss= 0.00000 val_roc= 0.49062 val_auprc= 0.49498 val_apk= 0.29691 time= 0.09709\n",
            "Epoch: 0009 Iter: 1051 Edge: 0003 train_loss= 0.00000 val_roc= 0.48891 val_auprc= 0.49666 val_apk= 0.19298 time= 0.09461\n",
            "Epoch: 0009 Iter: 1201 Edge: 0000 train_loss= 0.00000 val_roc= 0.47082 val_auprc= 0.47848 val_apk= 0.21255 time= 0.09179\n",
            "Epoch: 0009 Iter: 1351 Edge: 0003 train_loss= 0.00000 val_roc= 0.49345 val_auprc= 0.49953 val_apk= 0.21294 time= 0.11877\n",
            "Epoch: 0009 Iter: 1501 Edge: 0000 train_loss= 0.00000 val_roc= 0.48081 val_auprc= 0.47569 val_apk= 0.12104 time= 0.09564\n",
            "Epoch: 0009 Iter: 1651 Edge: 0003 train_loss= 0.00000 val_roc= 0.48733 val_auprc= 0.49414 val_apk= 0.21911 time= 0.09243\n",
            "Epoch: 0009 Iter: 1801 Edge: 0000 train_loss= 0.00000 val_roc= 0.50173 val_auprc= 0.49855 val_apk= 0.23721 time= 0.12644\n",
            "Epoch: 0009 Iter: 1951 Edge: 0003 train_loss= 0.00000 val_roc= 0.48818 val_auprc= 0.49438 val_apk= 0.22131 time= 0.09649\n",
            "Epoch: 0009 Iter: 2101 Edge: 0000 train_loss= 0.00000 val_roc= 0.51482 val_auprc= 0.51849 val_apk= 0.39843 time= 0.09157\n",
            "Epoch: 0009 Iter: 2251 Edge: 0003 train_loss= 0.00000 val_roc= 0.49025 val_auprc= 0.49782 val_apk= 0.23532 time= 0.08902\n",
            "Epoch: 0009 Iter: 2401 Edge: 0000 train_loss= 0.00000 val_roc= 0.51336 val_auprc= 0.52784 val_apk= 0.41601 time= 0.10035\n",
            "Epoch: 0010 Iter: 0001 Edge: 0000 train_loss= 0.00000 val_roc= 0.50838 val_auprc= 0.50479 val_apk= 0.32271 time= 0.10671\n",
            "Epoch: 0010 Iter: 0151 Edge: 0003 train_loss= 0.00000 val_roc= 0.48835 val_auprc= 0.49546 val_apk= 0.25371 time= 0.11044\n",
            "Epoch: 0010 Iter: 0301 Edge: 0000 train_loss= 0.00000 val_roc= 0.50058 val_auprc= 0.49630 val_apk= 0.21721 time= 0.09655\n",
            "Epoch: 0010 Iter: 0451 Edge: 0003 train_loss= 0.00000 val_roc= 0.48880 val_auprc= 0.49365 val_apk= 0.23134 time= 0.10844\n",
            "Epoch: 0010 Iter: 0601 Edge: 0000 train_loss= 0.00000 val_roc= 0.49483 val_auprc= 0.49007 val_apk= 0.19438 time= 0.09633\n",
            "Epoch: 0010 Iter: 0751 Edge: 0003 train_loss= 0.00000 val_roc= 0.49134 val_auprc= 0.49299 val_apk= 0.23707 time= 0.09559\n",
            "Epoch: 0010 Iter: 0901 Edge: 0000 train_loss= 0.00000 val_roc= 0.49886 val_auprc= 0.50242 val_apk= 0.35284 time= 0.10973\n",
            "Epoch: 0010 Iter: 1051 Edge: 0003 train_loss= 0.00000 val_roc= 0.48571 val_auprc= 0.49209 val_apk= 0.28012 time= 0.09501\n",
            "Epoch: 0010 Iter: 1201 Edge: 0000 train_loss= 0.00000 val_roc= 0.50413 val_auprc= 0.50029 val_apk= 0.20719 time= 0.09106\n",
            "Epoch: 0010 Iter: 1351 Edge: 0003 train_loss= 0.00000 val_roc= 0.48418 val_auprc= 0.48889 val_apk= 0.28397 time= 0.09315\n",
            "Epoch: 0010 Iter: 1501 Edge: 0000 train_loss= 0.00000 val_roc= 0.52683 val_auprc= 0.52440 val_apk= 0.32058 time= 0.09599\n",
            "Epoch: 0010 Iter: 1651 Edge: 0003 train_loss= 0.00000 val_roc= 0.48382 val_auprc= 0.49147 val_apk= 0.25511 time= 0.09482\n",
            "Epoch: 0010 Iter: 1801 Edge: 0000 train_loss= 0.00000 val_roc= 0.51336 val_auprc= 0.52044 val_apk= 0.36106 time= 0.09548\n",
            "Epoch: 0010 Iter: 1951 Edge: 0003 train_loss= 0.00000 val_roc= 0.48203 val_auprc= 0.49116 val_apk= 0.26766 time= 0.10528\n",
            "Epoch: 0010 Iter: 2101 Edge: 0000 train_loss= 0.00000 val_roc= 0.51191 val_auprc= 0.51385 val_apk= 0.30364 time= 0.14998\n",
            "Epoch: 0010 Iter: 2251 Edge: 0003 train_loss= 0.00000 val_roc= 0.48293 val_auprc= 0.49252 val_apk= 0.28559 time= 0.10115\n",
            "Epoch: 0010 Iter: 2401 Edge: 0000 train_loss= 0.00000 val_roc= 0.49808 val_auprc= 0.50787 val_apk= 0.30747 time= 0.11770\n",
            "Optimization finished!\n",
            "Edge type= [00, 00, 00]\n",
            "Edge type: 0000 Test AUROC score 0.50833\n",
            "Edge type: 0000 Test AUPRC score 0.49660\n",
            "Edge type: 0000 Test AP@k score 0.15746\n",
            "\n",
            "Edge type= [00, 00, 01]\n",
            "Edge type: 0001 Test AUROC score 0.50994\n",
            "Edge type: 0001 Test AUPRC score 0.50071\n",
            "Edge type: 0001 Test AP@k score 0.19710\n",
            "\n",
            "Edge type= [00, 01, 00]\n",
            "Edge type: 0002 Test AUROC score 0.54152\n",
            "Edge type: 0002 Test AUPRC score 0.52573\n",
            "Edge type: 0002 Test AP@k score 0.31624\n",
            "\n",
            "Edge type= [01, 00, 00]\n",
            "Edge type: 0003 Test AUROC score 0.49489\n",
            "Edge type: 0003 Test AUPRC score 0.50306\n",
            "Edge type: 0003 Test AP@k score 0.35485\n",
            "\n",
            "Edge type= [01, 01, 00]\n",
            "Edge type: 0004 Test AUROC score 0.52423\n",
            "Edge type: 0004 Test AUPRC score 0.51164\n",
            "Edge type: 0004 Test AP@k score 0.32388\n",
            "\n",
            "Edge type= [01, 01, 01]\n",
            "Edge type: 0005 Test AUROC score 0.48037\n",
            "Edge type: 0005 Test AUPRC score 0.48807\n",
            "Edge type: 0005 Test AP@k score 0.23312\n",
            "\n",
            "Edge type= [01, 01, 02]\n",
            "Edge type: 0006 Test AUROC score 0.52598\n",
            "Edge type: 0006 Test AUPRC score 0.51981\n",
            "Edge type: 0006 Test AP@k score 0.24375\n",
            "\n",
            "Edge type= [01, 01, 03]\n",
            "Edge type: 0007 Test AUROC score 0.47027\n",
            "Edge type: 0007 Test AUPRC score 0.47958\n",
            "Edge type: 0007 Test AP@k score 0.24578\n",
            "\n",
            "Edge type= [01, 01, 04]\n",
            "Edge type: 0008 Test AUROC score 0.51394\n",
            "Edge type: 0008 Test AUPRC score 0.49935\n",
            "Edge type: 0008 Test AP@k score 0.14168\n",
            "\n",
            "Edge type= [01, 01, 05]\n",
            "Edge type: 0009 Test AUROC score 0.54423\n",
            "Edge type: 0009 Test AUPRC score 0.53906\n",
            "Edge type: 0009 Test AP@k score 0.31199\n",
            "\n"
          ]
        }
      ]
    }
  ]
}