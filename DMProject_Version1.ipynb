{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMProject-Version1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "t4zEC-3gLROH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "zPDFxiwWyTPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rijm4ze6LPl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5317035d-2c30-4053-c907-cc4aa2958f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "import time\n",
        "import os\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from sklearn import metrics\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "G3YKFnJBLUdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sparse_mat(a2b, a2idx, b2idx):\n",
        "    n = len(a2idx)\n",
        "    m = len(b2idx)\n",
        "    assoc = np.zeros((n, m))\n",
        "    for a, b_assoc in a2b.iteritems():\n",
        "        if a not in a2idx:\n",
        "            continue\n",
        "        for b in b_assoc:\n",
        "            if b not in b2idx:\n",
        "                continue\n",
        "            assoc[a2idx[a], b2idx[b]] = 1.\n",
        "    assoc = sp.coo_matrix(assoc)\n",
        "    return assoc\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "Zxu8LLY8LxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "    This function computes the mean average precision at k between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])"
      ],
      "metadata": {
        "id": "_lxl2O1cL1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def zeros(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def ones(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "metadata": {
        "id": "Af-xGZdwLaQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "class MultiLayer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    # Properties    \n",
        "        name: String, defines the variable scope of the layer.\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]])\n",
        "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DEDICOMDecoder(MultiLayer):\n",
        "    \"\"\"DEDICOM Tensor Factorization Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            self.vars['global_interaction'] = weight_variable_glorot(\n",
        "                input_dim, input_dim, name='global_interaction')\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='local_variation_%d' % k)\n",
        "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
        "            product1 = tf.matmul(inputs_row, relation)\n",
        "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
        "            product3 = tf.matmul(product2, relation)\n",
        "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DistMultDecoder(MultiLayer):\n",
        "    \"\"\"DistMult Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DistMultDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='relation_%d' % k)\n",
        "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['relation_%d' % k])\n",
        "            intermediate_product = tf.matmul(inputs_row, relation)\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BilinearDecoder(MultiLayer):\n",
        "    \"\"\"Bilinear Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(BilinearDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, input_dim, name='relation_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AIyMpZb5LfMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "class EdgeMinibatchIterator(object):\n",
        "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
        "    random pairs of co-occuring edges.\n",
        "    assoc -- numpy array with target edges\n",
        "    placeholders -- tensorflow placeholders object\n",
        "    batch_size -- size of the minibatches\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats\n",
        "        self.feat = feat\n",
        "        self.edge_types = edge_types\n",
        "        self.batch_size = batch_size\n",
        "        self.val_test_size = val_test_size\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
        "        self.batch_num = [0]*self.num_edge_types\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r\n",
        "                self.idx2edge_type[r] = i, j, k\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        # Function to build test and val sets with val_test_size positive links\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                print(\"Minibatch edge type:\", \"(%d, %d, %d)\" % (i, j, k))\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "                print(\"Train edges=\", \"%04d\" % len(self.train_edges[i,j][k]))\n",
        "                print(\"Val edges=\", \"%04d\" % len(self.val_edges[i,j][k]))\n",
        "                print(\"Test edges=\", \"%04d\" % len(self.test_edges[i,j][k]))\n",
        "\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])\n",
        "        num_test = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "        num_val = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if len(test_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing test edges=\", \"%04d/%04d\" % (len(test_edges_false), len(test_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if len(val_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing val edges=\", \"%04d/%04d\" % (len(val_edges_false), len(val_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        # Re-build adj matrices\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def end(self):\n",
        "        finished = len(self.freebatch_edge_types) == 0\n",
        "        return finished\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "        # construct feed dictionary\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
        "        while True:\n",
        "            if self.iter % 4 == 0:\n",
        "                # gene-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "            elif self.iter % 4 == 1:\n",
        "                # gene-drug relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
        "            elif self.iter % 4 == 2:\n",
        "                # drug-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
        "            else:\n",
        "                # random side effect relation\n",
        "                if len(self.freebatch_edge_types) > 0:\n",
        "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
        "                else:\n",
        "                    self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "                    self.iter = 0\n",
        "\n",
        "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
        "                break\n",
        "            else:\n",
        "                if self.iter % 4 in [0, 1, 2]:\n",
        "                    self.batch_num[self.current_edge_type_idx] = 0\n",
        "                else:\n",
        "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
        "\n",
        "        self.iter += 1\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "    def num_training_batches(self, edge_type, type_idx):\n",
        "        return len(self.train_edges[edge_type][type_idx]) // self.batch_size + 1\n",
        "\n",
        "    def val_feed_dict(self, edge_type, type_idx, placeholders, size=None):\n",
        "        edge_list = self.val_edges[edge_type][type_idx]\n",
        "        if size is None:\n",
        "            return self.batch_feed_dict(edge_list, edge_type, placeholders)\n",
        "        else:\n",
        "            ind = np.random.permutation(len(edge_list))\n",
        "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
        "            return self.batch_feed_dict(val_edges, edge_type, placeholders)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\" Re-shuffle the training set.\n",
        "            Also reset the batch number.\n",
        "        \"\"\"\n",
        "        for edge_type in self.edge_types:\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                self.train_edges[edge_type][k] = np.random.permutation(self.train_edges[edge_type][k])\n",
        "                self.batch_num[self.edge_type2idx[edge_type[0], edge_type[1], k]] = 0\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.freebatch_edge_types = list(range(self.num_edge_types))\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 0, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 1, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[1, 0, 0])\n",
        "        self.iter = 0"
      ],
      "metadata": {
        "id": "3FIO8W8tLjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ANMT3GmPLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1\n",
        "        self.decoders = decoders\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types}\n",
        "        self.input_dim = num_feat\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "\n",
        "        for i, hid1 in self.hidden1.items():\n",
        "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=FLAGS.hidden1, output_dim=FLAGS.hidden2,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'distmult':\n",
        "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'bilinear':\n",
        "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'dedicom':\n",
        "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'distmult':\n",
        "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'bilinear':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'dedicom':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
        "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)"
      ],
      "metadata": {
        "id": "NvyZI6NFLnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decagon Optimizer**"
      ],
      "metadata": {
        "id": "bcuYtIKOLggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "        neg_samples_list = []\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                    true_classes=labels,\n",
        "                    num_true=1,\n",
        "                    num_sampled=self.batch_size,\n",
        "                    unique=False,\n",
        "                    range_max=len(self.degrees[i][k]),\n",
        "                    distortion=0.75,\n",
        "                    unigrams=self.degrees[i][k].tolist())\n",
        "                neg_samples_list.append(neg_samples)\n",
        "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
        "        self.outputs = tf.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "        return preds\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
        "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        \"\"\"Cross-entropy optimization.\"\"\"\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def gather_cols(params, indices, name=None):\n",
        "    \"\"\"Gather columns of a 2D tensor.\n",
        "    Args:\n",
        "        params: A 2D tensor.\n",
        "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
        "        name: A name for the operation (optional).\n",
        "    Returns:\n",
        "        A 2D Tensor. Has the same type as ``params``.\n",
        "    \"\"\"\n",
        "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "metadata": {
        "id": "hftqMu6_LrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "ruZ8gtknLkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU (hide GPU) due to memory constraints\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "# Train on GPU\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=50)\n",
        "\n",
        "    return roc_sc, aupr_sc, apk_sc\n",
        "\n",
        "\n",
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.placeholder(tf.int32),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    }\n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    return placeholders\n",
        "\n",
        "val_test_size = 0.05\n",
        "n_genes = 500\n",
        "n_drugs = 400\n",
        "n_drugdrug_rel_types = 3\n",
        "gene_net = nx.planted_partition_graph(50, 10, 0.2, 0.05, seed=42)\n",
        "\n",
        "gene_adj = nx.adjacency_matrix(gene_net)\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "\n",
        "gene_drug_adj = sp.csr_matrix((10 * np.random.randn(n_genes, n_drugs) > 15).astype(int))\n",
        "drug_gene_adj = gene_drug_adj.transpose(copy=True)\n",
        "\n",
        "drug_drug_adj_list = []\n",
        "tmp = np.dot(drug_gene_adj, gene_drug_adj)\n",
        "for i in range(n_drugdrug_rel_types):\n",
        "    mat = np.zeros((n_drugs, n_drugs))\n",
        "    for d1, d2 in combinations(list(range(n_drugs)), 2):\n",
        "        if tmp[d1, d2] == i + 4:\n",
        "            mat[d1, d2] = mat[d2, d1] = 1.\n",
        "    drug_drug_adj_list.append(sp.csr_matrix(mat))\n",
        "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
        "\n",
        "\n",
        "# data representation\n",
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_drug_adj],\n",
        "    (1, 0): [drug_gene_adj],\n",
        "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: drug_degrees_list + drug_degrees_list,\n",
        "}\n",
        "\n",
        "# featureless (genes)\n",
        "gene_feat = sp.identity(n_genes)\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "# features (drugs)\n",
        "drug_feat = sp.identity(n_drugs)\n",
        "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
        "drug_feat = sparse_to_tuple(drug_feat.tocoo())\n",
        "\n",
        "# data representation\n",
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: drug_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: drug_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: drug_feat,\n",
        "}\n",
        "\n",
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'bilinear',\n",
        "    (0, 1): 'bilinear',\n",
        "    (1, 0): 'bilinear',\n",
        "    (1, 1): 'dedicom',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
        "print(\"Edge types:\", edge_types)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tWB8QcL-Nd",
        "outputId": "5f0cb551-0762-4883-889e-2b635de4d1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge types: 10\n",
            "Edge types: {(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
        "flags.DEFINE_integer('batch_size', 100, 'minibatch size.')  # Changed from 512\n",
        "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
        "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
        "# substantial amount of time\n",
        "PRINT_PROGRESS_EVERY = 150\n",
        "\n",
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)\n",
        "\n",
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "    adj_mats=adj_mats_orig,\n",
        "    feat=feat,\n",
        "    edge_types=edge_types,\n",
        "    #batch_size=FLAGS.batch_size,\n",
        "    val_test_size=val_test_size\n",
        ")\n",
        "\n",
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "    placeholders=placeholders,\n",
        "    num_feat=num_feat,\n",
        "    nonzero_feat=nonzero_feat,\n",
        "    edge_types=edge_types,\n",
        "    decoders=edge_type2decoder\n",
        ")\n",
        "\n",
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "    opt = DecagonOptimizer(\n",
        "        embeddings=model.embeddings,\n",
        "        latent_inters=model.latent_inters,\n",
        "        latent_varies=model.latent_varies,\n",
        "        degrees=degrees,\n",
        "        edge_types=edge_types,\n",
        "        edge_type2dim=edge_type2dim,\n",
        "        placeholders=placeholders,\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        margin=FLAGS.max_margin\n",
        "    )\n",
        "\n",
        "print(\"Initialize session\")\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "feed_dict = {}\n",
        "\n",
        "print(\"Train model\")\n",
        "for epoch in range(FLAGS.epochs):\n",
        "#for epoch in range(2):\n",
        "\n",
        "    minibatch.shuffle()\n",
        "    itr = 0\n",
        "    while not minibatch.end():\n",
        "        # Construct feed dictionary\n",
        "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "        feed_dict = minibatch.update_feed_dict(\n",
        "            feed_dict=feed_dict,\n",
        "            dropout=FLAGS.dropout,\n",
        "            placeholders=placeholders)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        # Training step: run single weight update\n",
        "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
        "        train_cost = outs[1]\n",
        "        batch_edge_type = outs[2]\n",
        "\n",
        "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
        "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false,\n",
        "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
        "\n",
        "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
        "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
        "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "print(\"Optimization finished!\")\n",
        "\n",
        "for et in range(num_edge_types):\n",
        "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
        "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRk6lxbkzY93",
        "outputId": "0a31224f-3eda-44fd-8a4c-d51e8115793d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining placeholders\n",
            "Create minibatch iterator\n",
            "Minibatch edge type: (0, 0, 0)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 0, 1)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 1, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 0, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 1, 0)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 1)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 2)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Minibatch edge type: (1, 1, 3)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 4)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 5)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Create model\n",
            "Create optimizer\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize session\n",
            "Train model\n",
            "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 10.76066 val_roc= 0.50710 val_auprc= 0.51447 val_apk= 0.35601 time= 1.35968\n",
            "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 10.34935 val_roc= 0.48633 val_auprc= 0.49508 val_apk= 0.27208 time= 0.08842\n",
            "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 9.76906 val_roc= 0.48736 val_auprc= 0.50846 val_apk= 0.49438 time= 0.09945\n",
            "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 9.79580 val_roc= 0.50902 val_auprc= 0.50696 val_apk= 0.22345 time= 0.09205\n",
            "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 8.55756 val_roc= 0.54606 val_auprc= 0.54075 val_apk= 0.41292 time= 0.09452\n",
            "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 7.20807 val_roc= 0.53966 val_auprc= 0.53646 val_apk= 0.41420 time= 0.09982\n",
            "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 8.98051 val_roc= 0.58815 val_auprc= 0.57670 val_apk= 0.45071 time= 0.10463\n",
            "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 7.33979 val_roc= 0.58291 val_auprc= 0.57389 val_apk= 0.45992 time= 0.09983\n",
            "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 7.84459 val_roc= 0.59428 val_auprc= 0.59015 val_apk= 0.59172 time= 0.09651\n",
            "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 6.72944 val_roc= 0.62027 val_auprc= 0.58722 val_apk= 0.40968 time= 0.09447\n",
            "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 6.61989 val_roc= 0.62182 val_auprc= 0.60639 val_apk= 0.51601 time= 0.08800\n",
            "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 6.62024 val_roc= 0.64000 val_auprc= 0.62781 val_apk= 0.69103 time= 0.09209\n",
            "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 6.73051 val_roc= 0.62087 val_auprc= 0.59979 val_apk= 0.45682 time= 0.09325\n",
            "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 5.91148 val_roc= 0.66620 val_auprc= 0.64729 val_apk= 0.61180 time= 0.08937\n",
            "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 7.19166 val_roc= 0.61810 val_auprc= 0.59394 val_apk= 0.52495 time= 0.09663\n",
            "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 6.38423 val_roc= 0.66642 val_auprc= 0.64885 val_apk= 0.59940 time= 0.09436\n",
            "Epoch: 0001 Iter: 2401 Edge: 0000 train_loss= 7.09487 val_roc= 0.64342 val_auprc= 0.63182 val_apk= 0.52485 time= 0.09183\n",
            "Epoch: 0002 Iter: 0001 Edge: 0000 train_loss= 7.27145 val_roc= 0.63388 val_auprc= 0.61180 val_apk= 0.37149 time= 0.08913\n",
            "Epoch: 0002 Iter: 0151 Edge: 0003 train_loss= 6.01085 val_roc= 0.68467 val_auprc= 0.65368 val_apk= 0.57588 time= 0.11904\n",
            "Epoch: 0002 Iter: 0301 Edge: 0000 train_loss= 6.55361 val_roc= 0.65179 val_auprc= 0.63389 val_apk= 0.60389 time= 0.10760\n",
            "Epoch: 0002 Iter: 0451 Edge: 0003 train_loss= 5.26237 val_roc= 0.69321 val_auprc= 0.67201 val_apk= 0.62320 time= 0.08950\n",
            "Epoch: 0002 Iter: 0601 Edge: 0000 train_loss= 5.89322 val_roc= 0.66266 val_auprc= 0.63994 val_apk= 0.58942 time= 0.09444\n",
            "Epoch: 0002 Iter: 0751 Edge: 0003 train_loss= 6.36641 val_roc= 0.67535 val_auprc= 0.66657 val_apk= 0.68734 time= 0.10642\n",
            "Epoch: 0002 Iter: 0901 Edge: 0000 train_loss= 5.30115 val_roc= 0.64975 val_auprc= 0.63480 val_apk= 0.58632 time= 0.09078\n",
            "Epoch: 0002 Iter: 1051 Edge: 0003 train_loss= 4.43286 val_roc= 0.69952 val_auprc= 0.68221 val_apk= 0.59650 time= 0.09128\n",
            "Epoch: 0002 Iter: 1201 Edge: 0000 train_loss= 5.90737 val_roc= 0.65245 val_auprc= 0.62957 val_apk= 0.52070 time= 0.09082\n",
            "Epoch: 0002 Iter: 1351 Edge: 0003 train_loss= 5.67261 val_roc= 0.70602 val_auprc= 0.67720 val_apk= 0.60831 time= 0.10380\n",
            "Epoch: 0002 Iter: 1501 Edge: 0000 train_loss= 5.22856 val_roc= 0.66746 val_auprc= 0.64422 val_apk= 0.64919 time= 0.10944\n",
            "Epoch: 0002 Iter: 1651 Edge: 0003 train_loss= 5.23005 val_roc= 0.69315 val_auprc= 0.68449 val_apk= 0.78411 time= 0.12401\n",
            "Epoch: 0002 Iter: 1801 Edge: 0000 train_loss= 5.91360 val_roc= 0.66792 val_auprc= 0.64099 val_apk= 0.56012 time= 0.09394\n",
            "Epoch: 0002 Iter: 1951 Edge: 0003 train_loss= 5.00921 val_roc= 0.70640 val_auprc= 0.70199 val_apk= 0.73653 time= 0.10251\n",
            "Epoch: 0002 Iter: 2101 Edge: 0000 train_loss= 7.38283 val_roc= 0.67463 val_auprc= 0.65858 val_apk= 0.68719 time= 0.09821\n",
            "Epoch: 0002 Iter: 2251 Edge: 0003 train_loss= 5.76749 val_roc= 0.69267 val_auprc= 0.68623 val_apk= 0.78066 time= 0.11515\n",
            "Epoch: 0002 Iter: 2401 Edge: 0000 train_loss= 5.98219 val_roc= 0.68219 val_auprc= 0.65399 val_apk= 0.58996 time= 0.09859\n",
            "Epoch: 0003 Iter: 0001 Edge: 0000 train_loss= 5.48799 val_roc= 0.67012 val_auprc= 0.65088 val_apk= 0.66344 time= 0.10951\n",
            "Epoch: 0003 Iter: 0151 Edge: 0003 train_loss= 5.93202 val_roc= 0.70273 val_auprc= 0.68996 val_apk= 0.74860 time= 0.09253\n",
            "Epoch: 0003 Iter: 0301 Edge: 0000 train_loss= 5.64007 val_roc= 0.67582 val_auprc= 0.65371 val_apk= 0.57533 time= 0.09214\n",
            "Epoch: 0003 Iter: 0451 Edge: 0003 train_loss= 6.38910 val_roc= 0.71382 val_auprc= 0.70397 val_apk= 0.78510 time= 0.09127\n",
            "Epoch: 0003 Iter: 0601 Edge: 0000 train_loss= 4.21815 val_roc= 0.67929 val_auprc= 0.65807 val_apk= 0.59765 time= 0.08895\n",
            "Epoch: 0003 Iter: 0751 Edge: 0003 train_loss= 5.02194 val_roc= 0.71343 val_auprc= 0.71302 val_apk= 0.83541 time= 0.09193\n",
            "Epoch: 0003 Iter: 0901 Edge: 0000 train_loss= 5.39208 val_roc= 0.68747 val_auprc= 0.66653 val_apk= 0.59106 time= 0.08661\n",
            "Epoch: 0003 Iter: 1051 Edge: 0003 train_loss= 5.88814 val_roc= 0.73234 val_auprc= 0.72998 val_apk= 0.83015 time= 0.09306\n",
            "Epoch: 0003 Iter: 1201 Edge: 0000 train_loss= 7.10567 val_roc= 0.69835 val_auprc= 0.67051 val_apk= 0.60014 time= 0.09182\n",
            "Epoch: 0003 Iter: 1351 Edge: 0003 train_loss= 5.59006 val_roc= 0.72229 val_auprc= 0.72241 val_apk= 0.90350 time= 0.09302\n",
            "Epoch: 0003 Iter: 1501 Edge: 0000 train_loss= 6.13622 val_roc= 0.69643 val_auprc= 0.68243 val_apk= 0.69711 time= 0.08995\n",
            "Epoch: 0003 Iter: 1651 Edge: 0003 train_loss= 3.95737 val_roc= 0.72141 val_auprc= 0.71245 val_apk= 0.82877 time= 0.11748\n",
            "Epoch: 0003 Iter: 1801 Edge: 0000 train_loss= 5.04495 val_roc= 0.70176 val_auprc= 0.67978 val_apk= 0.66497 time= 0.09275\n",
            "Epoch: 0003 Iter: 1951 Edge: 0003 train_loss= 4.88156 val_roc= 0.72908 val_auprc= 0.72081 val_apk= 0.81998 time= 0.10568\n",
            "Epoch: 0003 Iter: 2101 Edge: 0000 train_loss= 6.15319 val_roc= 0.68357 val_auprc= 0.65700 val_apk= 0.56644 time= 0.08697\n",
            "Epoch: 0003 Iter: 2251 Edge: 0003 train_loss= 4.82859 val_roc= 0.73960 val_auprc= 0.72674 val_apk= 0.84612 time= 0.10765\n",
            "Epoch: 0003 Iter: 2401 Edge: 0000 train_loss= 6.43945 val_roc= 0.69863 val_auprc= 0.66644 val_apk= 0.54797 time= 0.12552\n",
            "Epoch: 0004 Iter: 0001 Edge: 0000 train_loss= 4.15225 val_roc= 0.69339 val_auprc= 0.65549 val_apk= 0.48461 time= 0.10236\n",
            "Epoch: 0004 Iter: 0151 Edge: 0003 train_loss= 3.37831 val_roc= 0.73640 val_auprc= 0.72378 val_apk= 0.80406 time= 0.08952\n",
            "Epoch: 0004 Iter: 0301 Edge: 0000 train_loss= 5.27010 val_roc= 0.69207 val_auprc= 0.66557 val_apk= 0.63401 time= 0.09839\n",
            "Epoch: 0004 Iter: 0451 Edge: 0003 train_loss= 4.99123 val_roc= 0.73813 val_auprc= 0.71927 val_apk= 0.74751 time= 0.09159\n",
            "Epoch: 0004 Iter: 0601 Edge: 0000 train_loss= 6.32022 val_roc= 0.70558 val_auprc= 0.68386 val_apk= 0.66996 time= 0.08892\n",
            "Epoch: 0004 Iter: 0751 Edge: 0003 train_loss= 5.66555 val_roc= 0.73624 val_auprc= 0.72295 val_apk= 0.80232 time= 0.09413\n",
            "Epoch: 0004 Iter: 0901 Edge: 0000 train_loss= 4.26378 val_roc= 0.69411 val_auprc= 0.67939 val_apk= 0.70716 time= 0.11863\n",
            "Epoch: 0004 Iter: 1051 Edge: 0003 train_loss= 5.03605 val_roc= 0.74482 val_auprc= 0.73729 val_apk= 0.86532 time= 0.09520\n",
            "Epoch: 0004 Iter: 1201 Edge: 0000 train_loss= 4.45695 val_roc= 0.69820 val_auprc= 0.66466 val_apk= 0.58480 time= 0.10959\n",
            "Epoch: 0004 Iter: 1351 Edge: 0003 train_loss= 3.75209 val_roc= 0.73801 val_auprc= 0.72531 val_apk= 0.82775 time= 0.09217\n",
            "Epoch: 0004 Iter: 1501 Edge: 0000 train_loss= 3.92494 val_roc= 0.70569 val_auprc= 0.68527 val_apk= 0.66673 time= 0.09025\n",
            "Epoch: 0004 Iter: 1651 Edge: 0003 train_loss= 5.58112 val_roc= 0.73374 val_auprc= 0.71362 val_apk= 0.76449 time= 0.09221\n",
            "Epoch: 0004 Iter: 1801 Edge: 0000 train_loss= 4.96439 val_roc= 0.68090 val_auprc= 0.64725 val_apk= 0.58911 time= 0.10653\n",
            "Epoch: 0004 Iter: 1951 Edge: 0003 train_loss= 4.83878 val_roc= 0.74104 val_auprc= 0.72695 val_apk= 0.77762 time= 0.09877\n",
            "Epoch: 0004 Iter: 2101 Edge: 0000 train_loss= 5.38706 val_roc= 0.68935 val_auprc= 0.66448 val_apk= 0.64470 time= 0.08894\n",
            "Epoch: 0004 Iter: 2251 Edge: 0003 train_loss= 4.55259 val_roc= 0.75263 val_auprc= 0.74921 val_apk= 0.93168 time= 0.09992\n",
            "Epoch: 0004 Iter: 2401 Edge: 0000 train_loss= 4.72735 val_roc= 0.70240 val_auprc= 0.68654 val_apk= 0.71990 time= 0.09050\n",
            "Epoch: 0005 Iter: 0001 Edge: 0000 train_loss= 4.30651 val_roc= 0.70927 val_auprc= 0.68406 val_apk= 0.67744 time= 0.12602\n",
            "Epoch: 0005 Iter: 0151 Edge: 0003 train_loss= 4.64134 val_roc= 0.75073 val_auprc= 0.73434 val_apk= 0.79147 time= 0.10171\n",
            "Epoch: 0005 Iter: 0301 Edge: 0000 train_loss= 4.06608 val_roc= 0.71061 val_auprc= 0.69980 val_apk= 0.78844 time= 0.09800\n",
            "Epoch: 0005 Iter: 0451 Edge: 0003 train_loss= 5.16162 val_roc= 0.74881 val_auprc= 0.73071 val_apk= 0.78083 time= 0.09997\n",
            "Epoch: 0005 Iter: 0601 Edge: 0000 train_loss= 5.21439 val_roc= 0.71114 val_auprc= 0.70280 val_apk= 0.77967 time= 0.09066\n",
            "Epoch: 0005 Iter: 0751 Edge: 0003 train_loss= 5.61803 val_roc= 0.74387 val_auprc= 0.73296 val_apk= 0.86743 time= 0.09133\n",
            "Epoch: 0005 Iter: 0901 Edge: 0000 train_loss= 4.36492 val_roc= 0.70563 val_auprc= 0.67775 val_apk= 0.66830 time= 0.09306\n",
            "Epoch: 0005 Iter: 1051 Edge: 0003 train_loss= 4.71423 val_roc= 0.73689 val_auprc= 0.71707 val_apk= 0.73077 time= 0.13442\n",
            "Epoch: 0005 Iter: 1201 Edge: 0000 train_loss= 4.40333 val_roc= 0.71955 val_auprc= 0.69739 val_apk= 0.62382 time= 0.09479\n",
            "Epoch: 0005 Iter: 1351 Edge: 0003 train_loss= 5.10210 val_roc= 0.75079 val_auprc= 0.74117 val_apk= 0.86444 time= 0.09020\n",
            "Epoch: 0005 Iter: 1501 Edge: 0000 train_loss= 5.59557 val_roc= 0.71492 val_auprc= 0.69368 val_apk= 0.67782 time= 0.08803\n",
            "Epoch: 0005 Iter: 1651 Edge: 0003 train_loss= 4.30563 val_roc= 0.74483 val_auprc= 0.72678 val_apk= 0.80620 time= 0.10235\n",
            "Epoch: 0005 Iter: 1801 Edge: 0000 train_loss= 5.21356 val_roc= 0.71239 val_auprc= 0.68897 val_apk= 0.65489 time= 0.11763\n",
            "Epoch: 0005 Iter: 1951 Edge: 0003 train_loss= 4.13165 val_roc= 0.75919 val_auprc= 0.75190 val_apk= 0.90957 time= 0.08794\n",
            "Epoch: 0005 Iter: 2101 Edge: 0000 train_loss= 4.89385 val_roc= 0.71594 val_auprc= 0.69095 val_apk= 0.60959 time= 0.08741\n",
            "Epoch: 0005 Iter: 2251 Edge: 0003 train_loss= 4.26053 val_roc= 0.75211 val_auprc= 0.74205 val_apk= 0.85047 time= 0.09728\n",
            "Epoch: 0005 Iter: 2401 Edge: 0000 train_loss= 6.51914 val_roc= 0.70167 val_auprc= 0.68061 val_apk= 0.70042 time= 0.09315\n",
            "Epoch: 0006 Iter: 0001 Edge: 0000 train_loss= 5.58074 val_roc= 0.72103 val_auprc= 0.69137 val_apk= 0.57704 time= 0.09306\n",
            "Epoch: 0006 Iter: 0151 Edge: 0003 train_loss= 4.33396 val_roc= 0.75069 val_auprc= 0.72725 val_apk= 0.72075 time= 0.09623\n",
            "Epoch: 0006 Iter: 0301 Edge: 0000 train_loss= 4.67994 val_roc= 0.71760 val_auprc= 0.69634 val_apk= 0.70264 time= 0.09104\n",
            "Epoch: 0006 Iter: 0451 Edge: 0003 train_loss= 5.64292 val_roc= 0.75263 val_auprc= 0.73840 val_apk= 0.84667 time= 0.09577\n",
            "Epoch: 0006 Iter: 0601 Edge: 0000 train_loss= 5.61430 val_roc= 0.71164 val_auprc= 0.69679 val_apk= 0.68922 time= 0.10752\n",
            "Epoch: 0006 Iter: 0751 Edge: 0003 train_loss= 4.17408 val_roc= 0.74639 val_auprc= 0.74803 val_apk= 0.92002 time= 0.09212\n",
            "Epoch: 0006 Iter: 0901 Edge: 0000 train_loss= 5.84539 val_roc= 0.71326 val_auprc= 0.69729 val_apk= 0.72820 time= 0.12075\n",
            "Epoch: 0006 Iter: 1051 Edge: 0003 train_loss= 4.54056 val_roc= 0.73793 val_auprc= 0.71952 val_apk= 0.75888 time= 0.08933\n",
            "Epoch: 0006 Iter: 1201 Edge: 0000 train_loss= 3.69097 val_roc= 0.71446 val_auprc= 0.69683 val_apk= 0.76760 time= 0.08771\n",
            "Epoch: 0006 Iter: 1351 Edge: 0003 train_loss= 3.17583 val_roc= 0.75376 val_auprc= 0.73860 val_apk= 0.74715 time= 0.08707\n",
            "Epoch: 0006 Iter: 1501 Edge: 0000 train_loss= 5.42545 val_roc= 0.72846 val_auprc= 0.70072 val_apk= 0.58967 time= 0.09371\n",
            "Epoch: 0006 Iter: 1651 Edge: 0003 train_loss= 4.30792 val_roc= 0.75242 val_auprc= 0.74261 val_apk= 0.80742 time= 0.10118\n",
            "Epoch: 0006 Iter: 1801 Edge: 0000 train_loss= 4.85430 val_roc= 0.72598 val_auprc= 0.68927 val_apk= 0.59984 time= 0.09432\n",
            "Epoch: 0006 Iter: 1951 Edge: 0003 train_loss= 3.16180 val_roc= 0.75811 val_auprc= 0.74143 val_apk= 0.76061 time= 0.10448\n",
            "Epoch: 0006 Iter: 2101 Edge: 0000 train_loss= 4.58388 val_roc= 0.72574 val_auprc= 0.68872 val_apk= 0.59402 time= 0.09384\n",
            "Epoch: 0006 Iter: 2251 Edge: 0003 train_loss= 4.61003 val_roc= 0.74370 val_auprc= 0.72783 val_apk= 0.78032 time= 0.09059\n",
            "Epoch: 0006 Iter: 2401 Edge: 0000 train_loss= 4.94841 val_roc= 0.71913 val_auprc= 0.69864 val_apk= 0.76449 time= 0.08955\n",
            "Epoch: 0007 Iter: 0001 Edge: 0000 train_loss= 4.01397 val_roc= 0.72494 val_auprc= 0.69881 val_apk= 0.61491 time= 0.09023\n",
            "Epoch: 0007 Iter: 0151 Edge: 0003 train_loss= 3.30092 val_roc= 0.75129 val_auprc= 0.74351 val_apk= 0.84234 time= 0.08803\n",
            "Epoch: 0007 Iter: 0301 Edge: 0000 train_loss= 3.43083 val_roc= 0.71593 val_auprc= 0.67984 val_apk= 0.55704 time= 0.10422\n",
            "Epoch: 0007 Iter: 0451 Edge: 0003 train_loss= 3.96064 val_roc= 0.74963 val_auprc= 0.73685 val_apk= 0.79378 time= 0.10560\n",
            "Epoch: 0007 Iter: 0601 Edge: 0000 train_loss= 5.66436 val_roc= 0.74021 val_auprc= 0.71699 val_apk= 0.69896 time= 0.10687\n",
            "Epoch: 0007 Iter: 0751 Edge: 0003 train_loss= 3.97762 val_roc= 0.76972 val_auprc= 0.75732 val_apk= 0.84612 time= 0.09320\n",
            "Epoch: 0007 Iter: 0901 Edge: 0000 train_loss= 5.20662 val_roc= 0.72664 val_auprc= 0.71002 val_apk= 0.69361 time= 0.09156\n",
            "Epoch: 0007 Iter: 1051 Edge: 0003 train_loss= 4.08006 val_roc= 0.76605 val_auprc= 0.74901 val_apk= 0.79724 time= 0.09194\n",
            "Epoch: 0007 Iter: 1201 Edge: 0000 train_loss= 4.14120 val_roc= 0.73230 val_auprc= 0.70270 val_apk= 0.64963 time= 0.08777\n",
            "Epoch: 0007 Iter: 1351 Edge: 0003 train_loss= 4.59362 val_roc= 0.76903 val_auprc= 0.75145 val_apk= 0.76273 time= 0.08762\n",
            "Epoch: 0007 Iter: 1501 Edge: 0000 train_loss= 3.89638 val_roc= 0.73445 val_auprc= 0.71021 val_apk= 0.70129 time= 0.09588\n",
            "Epoch: 0007 Iter: 1651 Edge: 0003 train_loss= 3.27444 val_roc= 0.77450 val_auprc= 0.75617 val_apk= 0.80161 time= 0.09104\n",
            "Epoch: 0007 Iter: 1801 Edge: 0000 train_loss= 4.61975 val_roc= 0.74278 val_auprc= 0.71621 val_apk= 0.69437 time= 0.11762\n",
            "Epoch: 0007 Iter: 1951 Edge: 0003 train_loss= 3.77718 val_roc= 0.75577 val_auprc= 0.73497 val_apk= 0.78074 time= 0.09558\n",
            "Epoch: 0007 Iter: 2101 Edge: 0000 train_loss= 3.98250 val_roc= 0.73113 val_auprc= 0.70187 val_apk= 0.66167 time= 0.09183\n",
            "Epoch: 0007 Iter: 2251 Edge: 0003 train_loss= 3.35819 val_roc= 0.74683 val_auprc= 0.71947 val_apk= 0.65873 time= 0.10197\n",
            "Epoch: 0007 Iter: 2401 Edge: 0000 train_loss= 3.21904 val_roc= 0.73200 val_auprc= 0.70323 val_apk= 0.67483 time= 0.08910\n",
            "Epoch: 0008 Iter: 0001 Edge: 0000 train_loss= 4.79121 val_roc= 0.73629 val_auprc= 0.70632 val_apk= 0.66089 time= 0.10293\n",
            "Epoch: 0008 Iter: 0151 Edge: 0003 train_loss= 4.68104 val_roc= 0.75900 val_auprc= 0.73525 val_apk= 0.79859 time= 0.09027\n",
            "Epoch: 0008 Iter: 0301 Edge: 0000 train_loss= 4.80550 val_roc= 0.73934 val_auprc= 0.71678 val_apk= 0.71067 time= 0.09988\n",
            "Epoch: 0008 Iter: 0451 Edge: 0003 train_loss= 3.80427 val_roc= 0.76901 val_auprc= 0.74951 val_apk= 0.78214 time= 0.09287\n",
            "Epoch: 0008 Iter: 0601 Edge: 0000 train_loss= 3.71530 val_roc= 0.73795 val_auprc= 0.71537 val_apk= 0.72896 time= 0.09773\n",
            "Epoch: 0008 Iter: 0751 Edge: 0003 train_loss= 3.33916 val_roc= 0.76001 val_auprc= 0.73235 val_apk= 0.71829 time= 0.13110\n",
            "Epoch: 0008 Iter: 0901 Edge: 0000 train_loss= 3.57611 val_roc= 0.72560 val_auprc= 0.69919 val_apk= 0.66410 time= 0.08949\n",
            "Epoch: 0008 Iter: 1051 Edge: 0003 train_loss= 3.41580 val_roc= 0.76680 val_auprc= 0.73379 val_apk= 0.77683 time= 0.08942\n",
            "Epoch: 0008 Iter: 1201 Edge: 0000 train_loss= 3.11334 val_roc= 0.73145 val_auprc= 0.71186 val_apk= 0.71036 time= 0.10201\n",
            "Epoch: 0008 Iter: 1351 Edge: 0003 train_loss= 3.64295 val_roc= 0.76961 val_auprc= 0.74313 val_apk= 0.67787 time= 0.09036\n",
            "Epoch: 0008 Iter: 1501 Edge: 0000 train_loss= 3.58006 val_roc= 0.73025 val_auprc= 0.71251 val_apk= 0.68024 time= 0.08840\n",
            "Epoch: 0008 Iter: 1651 Edge: 0003 train_loss= 4.60947 val_roc= 0.75899 val_auprc= 0.73790 val_apk= 0.76286 time= 0.09687\n",
            "Epoch: 0008 Iter: 1801 Edge: 0000 train_loss= 3.51680 val_roc= 0.72788 val_auprc= 0.71392 val_apk= 0.74342 time= 0.09270\n",
            "Epoch: 0008 Iter: 1951 Edge: 0003 train_loss= 3.95451 val_roc= 0.76477 val_auprc= 0.73797 val_apk= 0.65510 time= 0.09355\n",
            "Epoch: 0008 Iter: 2101 Edge: 0000 train_loss= 3.66461 val_roc= 0.73591 val_auprc= 0.72192 val_apk= 0.77270 time= 0.09482\n",
            "Epoch: 0008 Iter: 2251 Edge: 0003 train_loss= 3.28467 val_roc= 0.75907 val_auprc= 0.74319 val_apk= 0.79181 time= 0.09224\n",
            "Epoch: 0008 Iter: 2401 Edge: 0000 train_loss= 3.98420 val_roc= 0.73459 val_auprc= 0.72025 val_apk= 0.74557 time= 0.08742\n",
            "Epoch: 0009 Iter: 0001 Edge: 0000 train_loss= 3.19571 val_roc= 0.73552 val_auprc= 0.71619 val_apk= 0.74104 time= 0.08591\n",
            "Epoch: 0009 Iter: 0151 Edge: 0003 train_loss= 4.41908 val_roc= 0.77247 val_auprc= 0.74870 val_apk= 0.79719 time= 0.09319\n",
            "Epoch: 0009 Iter: 0301 Edge: 0000 train_loss= 4.53801 val_roc= 0.74235 val_auprc= 0.72417 val_apk= 0.72944 time= 0.09069\n",
            "Epoch: 0009 Iter: 0451 Edge: 0003 train_loss= 3.83666 val_roc= 0.76790 val_auprc= 0.74125 val_apk= 0.70291 time= 0.08874\n",
            "Epoch: 0009 Iter: 0601 Edge: 0000 train_loss= 5.55004 val_roc= 0.74277 val_auprc= 0.72247 val_apk= 0.75136 time= 0.09428\n",
            "Epoch: 0009 Iter: 0751 Edge: 0003 train_loss= 3.83590 val_roc= 0.76088 val_auprc= 0.72821 val_apk= 0.66910 time= 0.09625\n",
            "Epoch: 0009 Iter: 0901 Edge: 0000 train_loss= 3.48965 val_roc= 0.73593 val_auprc= 0.72072 val_apk= 0.72533 time= 0.08983\n",
            "Epoch: 0009 Iter: 1051 Edge: 0003 train_loss= 4.07857 val_roc= 0.77647 val_auprc= 0.75234 val_apk= 0.75369 time= 0.10592\n",
            "Epoch: 0009 Iter: 1201 Edge: 0000 train_loss= 5.11484 val_roc= 0.74386 val_auprc= 0.71942 val_apk= 0.67125 time= 0.08896\n",
            "Epoch: 0009 Iter: 1351 Edge: 0003 train_loss= 3.91116 val_roc= 0.77537 val_auprc= 0.74435 val_apk= 0.71731 time= 0.08707\n",
            "Epoch: 0009 Iter: 1501 Edge: 0000 train_loss= 4.11080 val_roc= 0.73984 val_auprc= 0.71456 val_apk= 0.64554 time= 0.09879\n",
            "Epoch: 0009 Iter: 1651 Edge: 0003 train_loss= 4.23158 val_roc= 0.77080 val_auprc= 0.73892 val_apk= 0.69267 time= 0.09460\n",
            "Epoch: 0009 Iter: 1801 Edge: 0000 train_loss= 5.05086 val_roc= 0.73077 val_auprc= 0.70561 val_apk= 0.67481 time= 0.08819\n",
            "Epoch: 0009 Iter: 1951 Edge: 0003 train_loss= 2.76582 val_roc= 0.77282 val_auprc= 0.74127 val_apk= 0.73759 time= 0.09171\n",
            "Epoch: 0009 Iter: 2101 Edge: 0000 train_loss= 4.54021 val_roc= 0.74057 val_auprc= 0.72289 val_apk= 0.66878 time= 0.09215\n",
            "Epoch: 0009 Iter: 2251 Edge: 0003 train_loss= 3.95055 val_roc= 0.77360 val_auprc= 0.74396 val_apk= 0.77318 time= 0.09123\n",
            "Epoch: 0009 Iter: 2401 Edge: 0000 train_loss= 4.47594 val_roc= 0.74138 val_auprc= 0.71406 val_apk= 0.67769 time= 0.09384\n",
            "Epoch: 0010 Iter: 0001 Edge: 0000 train_loss= 4.29879 val_roc= 0.73978 val_auprc= 0.71166 val_apk= 0.62055 time= 0.09421\n",
            "Epoch: 0010 Iter: 0151 Edge: 0003 train_loss= 4.39420 val_roc= 0.77237 val_auprc= 0.74402 val_apk= 0.73996 time= 0.09158\n",
            "Epoch: 0010 Iter: 0301 Edge: 0000 train_loss= 3.94114 val_roc= 0.74420 val_auprc= 0.71631 val_apk= 0.65818 time= 0.09566\n",
            "Epoch: 0010 Iter: 0451 Edge: 0003 train_loss= 4.00416 val_roc= 0.76651 val_auprc= 0.73408 val_apk= 0.62259 time= 0.09169\n",
            "Epoch: 0010 Iter: 0601 Edge: 0000 train_loss= 5.48629 val_roc= 0.74471 val_auprc= 0.71620 val_apk= 0.65477 time= 0.09521\n",
            "Epoch: 0010 Iter: 0751 Edge: 0003 train_loss= 2.54897 val_roc= 0.77090 val_auprc= 0.74283 val_apk= 0.73791 time= 0.08851\n",
            "Epoch: 0010 Iter: 0901 Edge: 0000 train_loss= 4.12046 val_roc= 0.74212 val_auprc= 0.71107 val_apk= 0.58501 time= 0.12230\n",
            "Epoch: 0010 Iter: 1051 Edge: 0003 train_loss= 4.30667 val_roc= 0.77718 val_auprc= 0.75488 val_apk= 0.71503 time= 0.09254\n",
            "Epoch: 0010 Iter: 1201 Edge: 0000 train_loss= 4.06291 val_roc= 0.74361 val_auprc= 0.71966 val_apk= 0.73591 time= 0.09085\n",
            "Epoch: 0010 Iter: 1351 Edge: 0003 train_loss= 3.02300 val_roc= 0.77058 val_auprc= 0.74093 val_apk= 0.70259 time= 0.09256\n",
            "Epoch: 0010 Iter: 1501 Edge: 0000 train_loss= 5.70823 val_roc= 0.74317 val_auprc= 0.71669 val_apk= 0.65929 time= 0.09004\n",
            "Epoch: 0010 Iter: 1651 Edge: 0003 train_loss= 3.11304 val_roc= 0.77247 val_auprc= 0.74494 val_apk= 0.79552 time= 0.09307\n",
            "Epoch: 0010 Iter: 1801 Edge: 0000 train_loss= 4.66823 val_roc= 0.74305 val_auprc= 0.71263 val_apk= 0.64199 time= 0.09000\n",
            "Epoch: 0010 Iter: 1951 Edge: 0003 train_loss= 3.73294 val_roc= 0.77106 val_auprc= 0.74234 val_apk= 0.63130 time= 0.09982\n",
            "Epoch: 0010 Iter: 2101 Edge: 0000 train_loss= 3.81205 val_roc= 0.74396 val_auprc= 0.71245 val_apk= 0.65843 time= 0.08953\n",
            "Epoch: 0010 Iter: 2251 Edge: 0003 train_loss= 2.56190 val_roc= 0.78298 val_auprc= 0.75910 val_apk= 0.70813 time= 0.09883\n",
            "Epoch: 0010 Iter: 2401 Edge: 0000 train_loss= 4.14763 val_roc= 0.74270 val_auprc= 0.72597 val_apk= 0.79213 time= 0.08701\n",
            "Optimization finished!\n",
            "Edge type= [00, 00, 00]\n",
            "Edge type: 0000 Test AUROC score 0.73254\n",
            "Edge type: 0000 Test AUPRC score 0.69228\n",
            "Edge type: 0000 Test AP@k score 0.67015\n",
            "\n",
            "Edge type= [00, 00, 01]\n",
            "Edge type: 0001 Test AUROC score 0.80468\n",
            "Edge type: 0001 Test AUPRC score 0.76467\n",
            "Edge type: 0001 Test AP@k score 0.75261\n",
            "\n",
            "Edge type= [00, 01, 00]\n",
            "Edge type: 0002 Test AUROC score 0.76046\n",
            "Edge type: 0002 Test AUPRC score 0.72953\n",
            "Edge type: 0002 Test AP@k score 0.64539\n",
            "\n",
            "Edge type= [01, 00, 00]\n",
            "Edge type: 0003 Test AUROC score 0.75778\n",
            "Edge type: 0003 Test AUPRC score 0.73516\n",
            "Edge type: 0003 Test AP@k score 0.78513\n",
            "\n",
            "Edge type= [01, 01, 00]\n",
            "Edge type: 0004 Test AUROC score 0.68675\n",
            "Edge type: 0004 Test AUPRC score 0.65632\n",
            "Edge type: 0004 Test AP@k score 0.58905\n",
            "\n",
            "Edge type= [01, 01, 01]\n",
            "Edge type: 0005 Test AUROC score 0.74721\n",
            "Edge type: 0005 Test AUPRC score 0.69806\n",
            "Edge type: 0005 Test AP@k score 0.62642\n",
            "\n",
            "Edge type= [01, 01, 02]\n",
            "Edge type: 0006 Test AUROC score 0.77260\n",
            "Edge type: 0006 Test AUPRC score 0.70199\n",
            "Edge type: 0006 Test AP@k score 0.52251\n",
            "\n",
            "Edge type= [01, 01, 03]\n",
            "Edge type: 0007 Test AUROC score 0.70023\n",
            "Edge type: 0007 Test AUPRC score 0.64962\n",
            "Edge type: 0007 Test AP@k score 0.28267\n",
            "\n",
            "Edge type= [01, 01, 04]\n",
            "Edge type: 0008 Test AUROC score 0.74660\n",
            "Edge type: 0008 Test AUPRC score 0.68421\n",
            "Edge type: 0008 Test AP@k score 0.48301\n",
            "\n",
            "Edge type= [01, 01, 05]\n",
            "Edge type: 0009 Test AUROC score 0.81704\n",
            "Edge type: 0009 Test AUPRC score 0.80089\n",
            "Edge type: 0009 Test AP@k score 0.79638\n",
            "\n"
          ]
        }
      ]
    }
  ]
}