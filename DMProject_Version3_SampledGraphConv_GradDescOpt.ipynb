{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMProject-Version3-SampledGraphConv-GradDescOpt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "t4zEC-3gLROH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "zPDFxiwWyTPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b52a3e8-0ac7-4c76-9c72-88fc4885f7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rijm4ze6LPl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d60983-9816-4233-a3ac-21a6b6020ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "import time\n",
        "import os\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from sklearn import metrics\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "G3YKFnJBLUdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sparse_mat(a2b, a2idx, b2idx):\n",
        "    n = len(a2idx)\n",
        "    m = len(b2idx)\n",
        "    assoc = np.zeros((n, m))\n",
        "    for a, b_assoc in a2b.iteritems():\n",
        "        if a not in a2idx:\n",
        "            continue\n",
        "        for b in b_assoc:\n",
        "            if b not in b2idx:\n",
        "                continue\n",
        "            assoc[a2idx[a], b2idx[b]] = 1.\n",
        "    assoc = sp.coo_matrix(assoc)\n",
        "    return assoc\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "Zxu8LLY8LxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "    This function computes the mean average precision at k between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])"
      ],
      "metadata": {
        "id": "_lxl2O1cL1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def zeros(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def ones(input_dim, output_dim, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "metadata": {
        "id": "Af-xGZdwLaQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "class MultiLayer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    # Properties    \n",
        "        name: String, defines the variable scope of the layer.\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, support=None, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.rank = 100\n",
        "\n",
        "        if support is None:\n",
        "            self.support = placeholders['support']\n",
        "        else:\n",
        "            self.support = support\n",
        "\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]])\n",
        "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])\n",
        "\n",
        "\n",
        "            # THIS PART IS TAKEN FROM SampledGraphConv in FastGCN    \n",
        "            sup = self.support[self.edge_type][k]\n",
        "            \n",
        "            norm_x = tf.nn.l2_normalize(x)\n",
        "            #norm_sup = tf.nn.l2_normalize(tf.sparse.to_dense(sup))\n",
        "\n",
        "            # should use norm_sup, but doesnt work for some idiotic tf reason\n",
        "            norm_mix = tf.sparse_tensor_dense_matmul(sup, x)\n",
        "            norm_mix = norm_mix*tf.transpose(tf.reduce_sum(norm_mix))\n",
        "            sampledIndex = tf.multinomial(tf.log(norm_mix), self.rank)\n",
        "\n",
        "            out = norm_mix \n",
        "            \n",
        "            outputs.append(self.act(out))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DEDICOMDecoder(MultiLayer):\n",
        "    \"\"\"DEDICOM Tensor Factorization Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            self.vars['global_interaction'] = weight_variable_glorot(\n",
        "                input_dim, input_dim, name='global_interaction')\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='local_variation_%d' % k)\n",
        "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
        "            product1 = tf.matmul(inputs_row, relation)\n",
        "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
        "            product3 = tf.matmul(product2, relation)\n",
        "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DistMultDecoder(MultiLayer):\n",
        "    \"\"\"DistMult Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DistMultDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='relation_%d' % k)\n",
        "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['relation_%d' % k])\n",
        "            intermediate_product = tf.matmul(inputs_row, relation)\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BilinearDecoder(MultiLayer):\n",
        "    \"\"\"Bilinear Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(BilinearDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, input_dim, name='relation_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AIyMpZb5LfMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "class EdgeMinibatchIterator(object):\n",
        "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
        "    random pairs of co-occuring edges.\n",
        "    assoc -- numpy array with target edges\n",
        "    placeholders -- tensorflow placeholders object\n",
        "    batch_size -- size of the minibatches\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats\n",
        "        self.feat = feat\n",
        "        self.edge_types = edge_types\n",
        "        self.batch_size = batch_size\n",
        "        self.val_test_size = val_test_size\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
        "        self.batch_num = [0]*self.num_edge_types\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r\n",
        "                self.idx2edge_type[r] = i, j, k\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        # Function to build test and val sets with val_test_size positive links\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                print(\"Minibatch edge type:\", \"(%d, %d, %d)\" % (i, j, k))\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "                print(\"Train edges=\", \"%04d\" % len(self.train_edges[i,j][k]))\n",
        "                print(\"Val edges=\", \"%04d\" % len(self.val_edges[i,j][k]))\n",
        "                print(\"Test edges=\", \"%04d\" % len(self.test_edges[i,j][k]))\n",
        "\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])\n",
        "        num_test = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "        num_val = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if len(test_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing test edges=\", \"%04d/%04d\" % (len(test_edges_false), len(test_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if len(val_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing val edges=\", \"%04d/%04d\" % (len(val_edges_false), len(val_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        # Re-build adj matrices\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def end(self):\n",
        "        finished = len(self.freebatch_edge_types) == 0\n",
        "        return finished\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "        # construct feed dictionary\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
        "        while True:\n",
        "            if self.iter % 4 == 0:\n",
        "                # gene-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "            elif self.iter % 4 == 1:\n",
        "                # gene-drug relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
        "            elif self.iter % 4 == 2:\n",
        "                # drug-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
        "            else:\n",
        "                # random side effect relation\n",
        "                if len(self.freebatch_edge_types) > 0:\n",
        "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
        "                else:\n",
        "                    self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "                    self.iter = 0\n",
        "\n",
        "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
        "                break\n",
        "            else:\n",
        "                if self.iter % 4 in [0, 1, 2]:\n",
        "                    self.batch_num[self.current_edge_type_idx] = 0\n",
        "                else:\n",
        "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
        "\n",
        "        self.iter += 1\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "    def num_training_batches(self, edge_type, type_idx):\n",
        "        return len(self.train_edges[edge_type][type_idx]) // self.batch_size + 1\n",
        "\n",
        "    def val_feed_dict(self, edge_type, type_idx, placeholders, size=None):\n",
        "        edge_list = self.val_edges[edge_type][type_idx]\n",
        "        if size is None:\n",
        "            return self.batch_feed_dict(edge_list, edge_type, placeholders)\n",
        "        else:\n",
        "            ind = np.random.permutation(len(edge_list))\n",
        "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
        "            return self.batch_feed_dict(val_edges, edge_type, placeholders)\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\" Re-shuffle the training set.\n",
        "            Also reset the batch number.\n",
        "        \"\"\"\n",
        "        for edge_type in self.edge_types:\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                self.train_edges[edge_type][k] = np.random.permutation(self.train_edges[edge_type][k])\n",
        "                self.batch_num[self.edge_type2idx[edge_type[0], edge_type[1], k]] = 0\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.freebatch_edge_types = list(range(self.num_edge_types))\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 0, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 1, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[1, 0, 0])\n",
        "        self.iter = 0"
      ],
      "metadata": {
        "id": "3FIO8W8tLjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ANMT3GmPLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "OibFC-vT2lcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1\n",
        "        self.decoders = decoders\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types}\n",
        "        self.input_dim = num_feat\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.support = placeholders['support']\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout, support=self.support,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "\n",
        "        for i, hid1 in self.hidden1.items():\n",
        "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=FLAGS.hidden1, output_dim=FLAGS.hidden2,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'distmult':\n",
        "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'bilinear':\n",
        "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'dedicom':\n",
        "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'distmult':\n",
        "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'bilinear':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'dedicom':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
        "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)"
      ],
      "metadata": {
        "id": "NvyZI6NFLnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decagon Optimizer**"
      ],
      "metadata": {
        "id": "bcuYtIKOLggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "        neg_samples_list = []\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                    true_classes=labels,\n",
        "                    num_true=1,\n",
        "                    num_sampled=self.batch_size,\n",
        "                    unique=False,\n",
        "                    range_max=len(self.degrees[i][k]),\n",
        "                    distortion=0.75,\n",
        "                    unigrams=self.degrees[i][k].tolist())\n",
        "                neg_samples_list.append(neg_samples)\n",
        "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
        "        self.outputs = tf.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "        return preds\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
        "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        #self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        \"\"\"Cross-entropy optimization.\"\"\"\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def gather_cols(params, indices, name=None):\n",
        "    \"\"\"Gather columns of a 2D tensor.\n",
        "    Args:\n",
        "        params: A 2D tensor.\n",
        "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
        "        name: A name for the operation (optional).\n",
        "    Returns:\n",
        "        A 2D Tensor. Has the same type as ``params``.\n",
        "    \"\"\"\n",
        "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "metadata": {
        "id": "hftqMu6_LrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "ruZ8gtknLkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU (hide GPU) due to memory constraints\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "# Train on GPU\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def prepare_data(placeholders, edge_types):\n",
        "    adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in edge_types.items()}\n",
        "\n",
        "    return adj_mats\n",
        "    \n",
        "    \n",
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=50)\n",
        "\n",
        "    return roc_sc, aupr_sc, apk_sc\n",
        "\n",
        "\n",
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'support': tf.sparse_placeholder(tf.float32),\n",
        "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.placeholder(tf.int32),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    }\n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    placeholders.update({'support': prepare_data(placeholders, edge_types)})\n",
        "    return placeholders\n",
        "\n",
        "val_test_size = 0.05\n",
        "n_genes = 500\n",
        "n_drugs = 400\n",
        "n_drugdrug_rel_types = 3\n",
        "gene_net = nx.planted_partition_graph(50, 10, 0.2, 0.05, seed=42)\n",
        "\n",
        "gene_adj = nx.adjacency_matrix(gene_net)\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "\n",
        "gene_drug_adj = sp.csr_matrix((10 * np.random.randn(n_genes, n_drugs) > 15).astype(int))\n",
        "drug_gene_adj = gene_drug_adj.transpose(copy=True)\n",
        "\n",
        "drug_drug_adj_list = []\n",
        "tmp = np.dot(drug_gene_adj, gene_drug_adj)\n",
        "for i in range(n_drugdrug_rel_types):\n",
        "    mat = np.zeros((n_drugs, n_drugs))\n",
        "    for d1, d2 in combinations(list(range(n_drugs)), 2):\n",
        "        if tmp[d1, d2] == i + 4:\n",
        "            mat[d1, d2] = mat[d2, d1] = 1.\n",
        "    drug_drug_adj_list.append(sp.csr_matrix(mat))\n",
        "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
        "\n",
        "\n",
        "# data representation\n",
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_drug_adj],\n",
        "    (1, 0): [drug_gene_adj],\n",
        "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: drug_degrees_list + drug_degrees_list,\n",
        "}\n",
        "\n",
        "# featureless (genes)\n",
        "gene_feat = sp.identity(n_genes)\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "# features (drugs)\n",
        "drug_feat = sp.identity(n_drugs)\n",
        "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
        "drug_feat = sparse_to_tuple(drug_feat.tocoo())\n",
        "\n",
        "# data representation\n",
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: drug_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: drug_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: drug_feat,\n",
        "}\n",
        "\n",
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'bilinear',\n",
        "    (0, 1): 'bilinear',\n",
        "    (1, 0): 'bilinear',\n",
        "    (1, 1): 'dedicom',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
        "flags.DEFINE_integer('batch_size', 100, 'minibatch size.')  # Changed from 512\n",
        "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
        "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
        "# substantial amount of time\n",
        "PRINT_PROGRESS_EVERY = 150\n",
        "\n",
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)\n",
        "\n",
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "    adj_mats=adj_mats_orig,\n",
        "    feat=feat,\n",
        "    edge_types=edge_types,\n",
        "    #batch_size=FLAGS.batch_size,\n",
        "    val_test_size=val_test_size\n",
        ")\n",
        "\n",
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "    placeholders=placeholders,\n",
        "    num_feat=num_feat,\n",
        "    nonzero_feat=nonzero_feat,\n",
        "    edge_types=edge_types,\n",
        "    decoders=edge_type2decoder\n",
        ")\n",
        "\n",
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "    opt = DecagonOptimizer(\n",
        "        embeddings=model.embeddings,\n",
        "        latent_inters=model.latent_inters,\n",
        "        latent_varies=model.latent_varies,\n",
        "        degrees=degrees,\n",
        "        edge_types=edge_types,\n",
        "        edge_type2dim=edge_type2dim,\n",
        "        placeholders=placeholders,\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        margin=FLAGS.max_margin\n",
        "    )\n",
        "\n",
        "print(\"Initialize session\")\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "feed_dict = {}\n",
        "\n",
        "print(\"Train model\")\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    minibatch.shuffle()\n",
        "    itr = 0\n",
        "    while not minibatch.end():\n",
        "        # Construct feed dictionary\n",
        "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "        feed_dict = minibatch.update_feed_dict(\n",
        "            feed_dict=feed_dict,\n",
        "            dropout=FLAGS.dropout,\n",
        "            placeholders=placeholders)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        # Training step: run single weight update\n",
        "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
        "        train_cost = outs[1]\n",
        "        batch_edge_type = outs[2]\n",
        "\n",
        "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
        "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false,\n",
        "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
        "\n",
        "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
        "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
        "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "print(\"Optimization finished!\")\n",
        "\n",
        "for et in range(num_edge_types):\n",
        "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
        "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tWB8QcL-Nd",
        "outputId": "2301d2b7-58a6-4fca-8ed5-c9da554f074e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge types: 10\n",
            "Defining placeholders\n",
            "Create minibatch iterator\n",
            "Minibatch edge type: (0, 0, 0)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 0, 1)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 1, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 0, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 1, 0)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 1)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 2)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Minibatch edge type: (1, 1, 3)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 4)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 5)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Create model\n",
            "WARNING:tensorflow:From <ipython-input-6-77b142306a03>:111: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-77b142306a03>:118: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From <ipython-input-6-77b142306a03>:138: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Create optimizer\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_grad.py:202: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize session\n",
            "Train model\n",
            "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 12.55105 val_roc= 0.47701 val_auprc= 0.49024 val_apk= 0.26092 time= 1.39363\n",
            "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 11.17457 val_roc= 0.51392 val_auprc= 0.51628 val_apk= 0.43393 time= 0.10836\n",
            "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 10.39825 val_roc= 0.49166 val_auprc= 0.48567 val_apk= 0.17976 time= 0.08794\n",
            "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 10.35593 val_roc= 0.51875 val_auprc= 0.52597 val_apk= 0.45158 time= 0.11316\n",
            "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 11.23996 val_roc= 0.47972 val_auprc= 0.48047 val_apk= 0.20852 time= 0.09777\n",
            "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 11.08638 val_roc= 0.51597 val_auprc= 0.52014 val_apk= 0.40082 time= 0.12333\n",
            "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 9.93519 val_roc= 0.48900 val_auprc= 0.48444 val_apk= 0.15282 time= 0.08893\n",
            "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 9.48715 val_roc= 0.51395 val_auprc= 0.52035 val_apk= 0.46493 time= 0.08695\n",
            "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 10.33053 val_roc= 0.48513 val_auprc= 0.47988 val_apk= 0.11671 time= 0.10043\n",
            "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 8.35327 val_roc= 0.52114 val_auprc= 0.52241 val_apk= 0.37485 time= 0.08779\n",
            "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 10.27200 val_roc= 0.48761 val_auprc= 0.48342 val_apk= 0.13817 time= 0.08737\n",
            "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 9.68915 val_roc= 0.52035 val_auprc= 0.51959 val_apk= 0.38769 time= 0.09980\n",
            "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 9.29648 val_roc= 0.47609 val_auprc= 0.47726 val_apk= 0.18664 time= 0.08665\n",
            "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 9.73820 val_roc= 0.52613 val_auprc= 0.52061 val_apk= 0.32912 time= 0.08883\n",
            "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 10.22471 val_roc= 0.48612 val_auprc= 0.48087 val_apk= 0.15348 time= 0.09936\n",
            "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 10.11110 val_roc= 0.52519 val_auprc= 0.51715 val_apk= 0.29507 time= 0.08776\n",
            "Epoch: 0001 Iter: 2401 Edge: 0000 train_loss= 9.55767 val_roc= 0.48591 val_auprc= 0.48130 val_apk= 0.15707 time= 0.08860\n",
            "Epoch: 0002 Iter: 0001 Edge: 0000 train_loss= 10.21219 val_roc= 0.48573 val_auprc= 0.47880 val_apk= 0.15053 time= 0.09144\n",
            "Epoch: 0002 Iter: 0151 Edge: 0003 train_loss= 9.85839 val_roc= 0.52242 val_auprc= 0.51567 val_apk= 0.28323 time= 0.08957\n",
            "Epoch: 0002 Iter: 0301 Edge: 0000 train_loss= 9.35892 val_roc= 0.49198 val_auprc= 0.48187 val_apk= 0.13382 time= 0.09251\n",
            "Epoch: 0002 Iter: 0451 Edge: 0003 train_loss= 9.91306 val_roc= 0.52163 val_auprc= 0.51751 val_apk= 0.31840 time= 0.08976\n",
            "Epoch: 0002 Iter: 0601 Edge: 0000 train_loss= 9.30227 val_roc= 0.48043 val_auprc= 0.47697 val_apk= 0.17636 time= 0.09209\n",
            "Epoch: 0002 Iter: 0751 Edge: 0003 train_loss= 9.04390 val_roc= 0.52653 val_auprc= 0.52012 val_apk= 0.30721 time= 0.09598\n",
            "Epoch: 0002 Iter: 0901 Edge: 0000 train_loss= 9.88396 val_roc= 0.48229 val_auprc= 0.48104 val_apk= 0.19248 time= 0.11007\n",
            "Epoch: 0002 Iter: 1051 Edge: 0003 train_loss= 9.67958 val_roc= 0.52682 val_auprc= 0.51506 val_apk= 0.25956 time= 0.09061\n",
            "Epoch: 0002 Iter: 1201 Edge: 0000 train_loss= 9.54188 val_roc= 0.48360 val_auprc= 0.48432 val_apk= 0.26447 time= 0.08868\n",
            "Epoch: 0002 Iter: 1351 Edge: 0003 train_loss= 9.02369 val_roc= 0.53172 val_auprc= 0.51362 val_apk= 0.15100 time= 0.09371\n",
            "Epoch: 0002 Iter: 1501 Edge: 0000 train_loss= 10.33975 val_roc= 0.48852 val_auprc= 0.48602 val_apk= 0.20046 time= 0.08748\n",
            "Epoch: 0002 Iter: 1651 Edge: 0003 train_loss= 9.31883 val_roc= 0.52507 val_auprc= 0.50930 val_apk= 0.21394 time= 0.09585\n",
            "Epoch: 0002 Iter: 1801 Edge: 0000 train_loss= 9.38002 val_roc= 0.49854 val_auprc= 0.50140 val_apk= 0.28323 time= 0.09413\n",
            "Epoch: 0002 Iter: 1951 Edge: 0003 train_loss= 8.87228 val_roc= 0.53497 val_auprc= 0.52128 val_apk= 0.20726 time= 0.09557\n",
            "Epoch: 0002 Iter: 2101 Edge: 0000 train_loss= 10.42700 val_roc= 0.49885 val_auprc= 0.49561 val_apk= 0.25351 time= 0.08819\n",
            "Epoch: 0002 Iter: 2251 Edge: 0003 train_loss= 9.56895 val_roc= 0.53896 val_auprc= 0.52370 val_apk= 0.21615 time= 0.09483\n",
            "Epoch: 0002 Iter: 2401 Edge: 0000 train_loss= 9.86514 val_roc= 0.49865 val_auprc= 0.50193 val_apk= 0.26059 time= 0.09087\n",
            "Epoch: 0003 Iter: 0001 Edge: 0000 train_loss= 9.81560 val_roc= 0.49931 val_auprc= 0.49781 val_apk= 0.21451 time= 0.09026\n",
            "Epoch: 0003 Iter: 0151 Edge: 0003 train_loss= 10.22671 val_roc= 0.53875 val_auprc= 0.52448 val_apk= 0.22715 time= 0.09391\n",
            "Epoch: 0003 Iter: 0301 Edge: 0000 train_loss= 10.47768 val_roc= 0.50335 val_auprc= 0.49527 val_apk= 0.25787 time= 0.09061\n",
            "Epoch: 0003 Iter: 0451 Edge: 0003 train_loss= 10.56059 val_roc= 0.53039 val_auprc= 0.52224 val_apk= 0.22707 time= 0.10572\n",
            "Epoch: 0003 Iter: 0601 Edge: 0000 train_loss= 10.94146 val_roc= 0.49289 val_auprc= 0.49604 val_apk= 0.25495 time= 0.08965\n",
            "Epoch: 0003 Iter: 0751 Edge: 0003 train_loss= 9.40405 val_roc= 0.52446 val_auprc= 0.52308 val_apk= 0.28967 time= 0.08728\n",
            "Epoch: 0003 Iter: 0901 Edge: 0000 train_loss= 10.36325 val_roc= 0.50053 val_auprc= 0.50278 val_apk= 0.31107 time= 0.08705\n",
            "Epoch: 0003 Iter: 1051 Edge: 0003 train_loss= 10.19828 val_roc= 0.52509 val_auprc= 0.52590 val_apk= 0.35745 time= 0.09062\n",
            "Epoch: 0003 Iter: 1201 Edge: 0000 train_loss= 9.80188 val_roc= 0.50087 val_auprc= 0.50319 val_apk= 0.32403 time= 0.08851\n",
            "Epoch: 0003 Iter: 1351 Edge: 0003 train_loss= 9.63780 val_roc= 0.52527 val_auprc= 0.52665 val_apk= 0.36013 time= 0.10567\n",
            "Epoch: 0003 Iter: 1501 Edge: 0000 train_loss= 9.19945 val_roc= 0.50286 val_auprc= 0.50719 val_apk= 0.32008 time= 0.08811\n",
            "Epoch: 0003 Iter: 1651 Edge: 0003 train_loss= 9.33023 val_roc= 0.52843 val_auprc= 0.52632 val_apk= 0.33196 time= 0.09033\n",
            "Epoch: 0003 Iter: 1801 Edge: 0000 train_loss= 9.61486 val_roc= 0.50507 val_auprc= 0.50683 val_apk= 0.36229 time= 0.09505\n",
            "Epoch: 0003 Iter: 1951 Edge: 0003 train_loss= 10.06337 val_roc= 0.52348 val_auprc= 0.52356 val_apk= 0.33737 time= 0.09455\n",
            "Epoch: 0003 Iter: 2101 Edge: 0000 train_loss= 9.97999 val_roc= 0.51101 val_auprc= 0.51224 val_apk= 0.30705 time= 0.09004\n",
            "Epoch: 0003 Iter: 2251 Edge: 0003 train_loss= 9.34664 val_roc= 0.52768 val_auprc= 0.52783 val_apk= 0.37443 time= 0.09008\n",
            "Epoch: 0003 Iter: 2401 Edge: 0000 train_loss= 8.98551 val_roc= 0.51400 val_auprc= 0.50882 val_apk= 0.28426 time= 0.10205\n",
            "Epoch: 0004 Iter: 0001 Edge: 0000 train_loss= 9.58894 val_roc= 0.51338 val_auprc= 0.50707 val_apk= 0.26453 time= 0.08979\n",
            "Epoch: 0004 Iter: 0151 Edge: 0003 train_loss= 9.80926 val_roc= 0.53040 val_auprc= 0.52818 val_apk= 0.29958 time= 0.08810\n",
            "Epoch: 0004 Iter: 0301 Edge: 0000 train_loss= 8.37067 val_roc= 0.52531 val_auprc= 0.51546 val_apk= 0.25417 time= 0.08790\n",
            "Epoch: 0004 Iter: 0451 Edge: 0003 train_loss= 8.98213 val_roc= 0.53131 val_auprc= 0.52871 val_apk= 0.32820 time= 0.09775\n",
            "Epoch: 0004 Iter: 0601 Edge: 0000 train_loss= 9.25470 val_roc= 0.51302 val_auprc= 0.51323 val_apk= 0.37601 time= 0.09133\n",
            "Epoch: 0004 Iter: 0751 Edge: 0003 train_loss= 9.37843 val_roc= 0.54213 val_auprc= 0.54054 val_apk= 0.38940 time= 0.09672\n",
            "Epoch: 0004 Iter: 0901 Edge: 0000 train_loss= 8.91698 val_roc= 0.50743 val_auprc= 0.50612 val_apk= 0.37168 time= 0.09785\n",
            "Epoch: 0004 Iter: 1051 Edge: 0003 train_loss= 9.22299 val_roc= 0.54064 val_auprc= 0.53267 val_apk= 0.29891 time= 0.09289\n",
            "Epoch: 0004 Iter: 1201 Edge: 0000 train_loss= 8.57600 val_roc= 0.53028 val_auprc= 0.51707 val_apk= 0.26050 time= 0.09067\n",
            "Epoch: 0004 Iter: 1351 Edge: 0003 train_loss= 9.71546 val_roc= 0.54107 val_auprc= 0.53851 val_apk= 0.32511 time= 0.09493\n",
            "Epoch: 0004 Iter: 1501 Edge: 0000 train_loss= 10.05575 val_roc= 0.51762 val_auprc= 0.51683 val_apk= 0.37897 time= 0.08858\n",
            "Epoch: 0004 Iter: 1651 Edge: 0003 train_loss= 9.69968 val_roc= 0.54656 val_auprc= 0.54260 val_apk= 0.32923 time= 0.09314\n",
            "Epoch: 0004 Iter: 1801 Edge: 0000 train_loss= 9.46574 val_roc= 0.53731 val_auprc= 0.52254 val_apk= 0.27210 time= 0.08794\n",
            "Epoch: 0004 Iter: 1951 Edge: 0003 train_loss= 8.71298 val_roc= 0.54828 val_auprc= 0.54237 val_apk= 0.35335 time= 0.09204\n",
            "Epoch: 0004 Iter: 2101 Edge: 0000 train_loss= 9.30872 val_roc= 0.53798 val_auprc= 0.52797 val_apk= 0.38190 time= 0.10439\n",
            "Epoch: 0004 Iter: 2251 Edge: 0003 train_loss= 10.71749 val_roc= 0.55186 val_auprc= 0.54164 val_apk= 0.29319 time= 0.11635\n",
            "Epoch: 0004 Iter: 2401 Edge: 0000 train_loss= 9.43560 val_roc= 0.56579 val_auprc= 0.54437 val_apk= 0.31358 time= 0.08829\n",
            "Epoch: 0005 Iter: 0001 Edge: 0000 train_loss= 8.50982 val_roc= 0.56044 val_auprc= 0.54484 val_apk= 0.32897 time= 0.09854\n",
            "Epoch: 0005 Iter: 0151 Edge: 0003 train_loss= 8.56870 val_roc= 0.55523 val_auprc= 0.54008 val_apk= 0.33749 time= 0.12054\n",
            "Epoch: 0005 Iter: 0301 Edge: 0000 train_loss= 8.98033 val_roc= 0.56796 val_auprc= 0.54668 val_apk= 0.33031 time= 0.08681\n",
            "Epoch: 0005 Iter: 0451 Edge: 0003 train_loss= 8.74473 val_roc= 0.55505 val_auprc= 0.54154 val_apk= 0.32341 time= 0.08958\n",
            "Epoch: 0005 Iter: 0601 Edge: 0000 train_loss= 8.10993 val_roc= 0.57346 val_auprc= 0.55199 val_apk= 0.29234 time= 0.08866\n",
            "Epoch: 0005 Iter: 0751 Edge: 0003 train_loss= 9.37028 val_roc= 0.55828 val_auprc= 0.54550 val_apk= 0.34161 time= 0.08768\n",
            "Epoch: 0005 Iter: 0901 Edge: 0000 train_loss= 8.88151 val_roc= 0.53028 val_auprc= 0.52690 val_apk= 0.41317 time= 0.08907\n",
            "Epoch: 0005 Iter: 1051 Edge: 0003 train_loss= 8.21405 val_roc= 0.56405 val_auprc= 0.55510 val_apk= 0.32418 time= 0.09167\n",
            "Epoch: 0005 Iter: 1201 Edge: 0000 train_loss= 8.36052 val_roc= 0.57887 val_auprc= 0.55670 val_apk= 0.39123 time= 0.09008\n",
            "Epoch: 0005 Iter: 1351 Edge: 0003 train_loss= 8.34918 val_roc= 0.56036 val_auprc= 0.54550 val_apk= 0.32829 time= 0.08993\n",
            "Epoch: 0005 Iter: 1501 Edge: 0000 train_loss= 9.13907 val_roc= 0.58672 val_auprc= 0.56378 val_apk= 0.35081 time= 0.09092\n",
            "Epoch: 0005 Iter: 1651 Edge: 0003 train_loss= 8.46272 val_roc= 0.56055 val_auprc= 0.54665 val_apk= 0.36320 time= 0.09960\n",
            "Epoch: 0005 Iter: 1801 Edge: 0000 train_loss= 8.15085 val_roc= 0.57470 val_auprc= 0.55577 val_apk= 0.36788 time= 0.09094\n",
            "Epoch: 0005 Iter: 1951 Edge: 0003 train_loss= 7.29661 val_roc= 0.56298 val_auprc= 0.54761 val_apk= 0.35699 time= 0.09872\n",
            "Epoch: 0005 Iter: 2101 Edge: 0000 train_loss= 8.46429 val_roc= 0.58566 val_auprc= 0.56909 val_apk= 0.45092 time= 0.09087\n",
            "Epoch: 0005 Iter: 2251 Edge: 0003 train_loss= 7.80668 val_roc= 0.56240 val_auprc= 0.54671 val_apk= 0.33615 time= 0.08922\n",
            "Epoch: 0005 Iter: 2401 Edge: 0000 train_loss= 9.92655 val_roc= 0.56741 val_auprc= 0.55626 val_apk= 0.40041 time= 0.09239\n",
            "Epoch: 0006 Iter: 0001 Edge: 0000 train_loss= 8.91352 val_roc= 0.58139 val_auprc= 0.56429 val_apk= 0.43690 time= 0.10899\n",
            "Epoch: 0006 Iter: 0151 Edge: 0003 train_loss= 8.19119 val_roc= 0.56817 val_auprc= 0.55118 val_apk= 0.38383 time= 0.09491\n",
            "Epoch: 0006 Iter: 0301 Edge: 0000 train_loss= 8.77984 val_roc= 0.58071 val_auprc= 0.56368 val_apk= 0.41125 time= 0.11544\n",
            "Epoch: 0006 Iter: 0451 Edge: 0003 train_loss= 7.72301 val_roc= 0.56827 val_auprc= 0.55979 val_apk= 0.40618 time= 0.10152\n",
            "Epoch: 0006 Iter: 0601 Edge: 0000 train_loss= 8.11803 val_roc= 0.58187 val_auprc= 0.56667 val_apk= 0.46647 time= 0.09357\n",
            "Epoch: 0006 Iter: 0751 Edge: 0003 train_loss= 8.43869 val_roc= 0.56781 val_auprc= 0.55862 val_apk= 0.40726 time= 0.09525\n",
            "Epoch: 0006 Iter: 0901 Edge: 0000 train_loss= 8.08625 val_roc= 0.58212 val_auprc= 0.56885 val_apk= 0.42141 time= 0.08944\n",
            "Epoch: 0006 Iter: 1051 Edge: 0003 train_loss= 8.43232 val_roc= 0.57057 val_auprc= 0.56083 val_apk= 0.39079 time= 0.10162\n",
            "Epoch: 0006 Iter: 1201 Edge: 0000 train_loss= 7.91315 val_roc= 0.58389 val_auprc= 0.57034 val_apk= 0.41687 time= 0.10038\n",
            "Epoch: 0006 Iter: 1351 Edge: 0003 train_loss= 8.56787 val_roc= 0.57236 val_auprc= 0.56541 val_apk= 0.45630 time= 0.08999\n",
            "Epoch: 0006 Iter: 1501 Edge: 0000 train_loss= 8.61000 val_roc= 0.57321 val_auprc= 0.54966 val_apk= 0.40168 time= 0.08865\n",
            "Epoch: 0006 Iter: 1651 Edge: 0003 train_loss= 7.42693 val_roc= 0.57076 val_auprc= 0.56135 val_apk= 0.42177 time= 0.09071\n",
            "Epoch: 0006 Iter: 1801 Edge: 0000 train_loss= 7.94518 val_roc= 0.57787 val_auprc= 0.55371 val_apk= 0.36739 time= 0.09918\n",
            "Epoch: 0006 Iter: 1951 Edge: 0003 train_loss= 8.38163 val_roc= 0.57202 val_auprc= 0.56296 val_apk= 0.44206 time= 0.08902\n",
            "Epoch: 0006 Iter: 2101 Edge: 0000 train_loss= 8.28310 val_roc= 0.57033 val_auprc= 0.54892 val_apk= 0.38372 time= 0.08872\n",
            "Epoch: 0006 Iter: 2251 Edge: 0003 train_loss= 8.46011 val_roc= 0.57899 val_auprc= 0.56799 val_apk= 0.41762 time= 0.09076\n",
            "Epoch: 0006 Iter: 2401 Edge: 0000 train_loss= 8.69618 val_roc= 0.59384 val_auprc= 0.57836 val_apk= 0.50522 time= 0.08685\n",
            "Epoch: 0007 Iter: 0001 Edge: 0000 train_loss= 7.99519 val_roc= 0.60253 val_auprc= 0.58410 val_apk= 0.50470 time= 0.09171\n",
            "Epoch: 0007 Iter: 0151 Edge: 0003 train_loss= 7.84742 val_roc= 0.57898 val_auprc= 0.57221 val_apk= 0.50035 time= 0.09151\n",
            "Epoch: 0007 Iter: 0301 Edge: 0000 train_loss= 8.45864 val_roc= 0.60322 val_auprc= 0.58625 val_apk= 0.50355 time= 0.08923\n",
            "Epoch: 0007 Iter: 0451 Edge: 0003 train_loss= 9.08651 val_roc= 0.57998 val_auprc= 0.57182 val_apk= 0.44844 time= 0.08755\n",
            "Epoch: 0007 Iter: 0601 Edge: 0000 train_loss= 7.65696 val_roc= 0.60047 val_auprc= 0.58253 val_apk= 0.47177 time= 0.08938\n",
            "Epoch: 0007 Iter: 0751 Edge: 0003 train_loss= 8.39929 val_roc= 0.57936 val_auprc= 0.57453 val_apk= 0.50343 time= 0.10058\n",
            "Epoch: 0007 Iter: 0901 Edge: 0000 train_loss= 7.71174 val_roc= 0.59274 val_auprc= 0.57547 val_apk= 0.45692 time= 0.08619\n",
            "Epoch: 0007 Iter: 1051 Edge: 0003 train_loss= 8.16903 val_roc= 0.58993 val_auprc= 0.57694 val_apk= 0.47472 time= 0.09928\n",
            "Epoch: 0007 Iter: 1201 Edge: 0000 train_loss= 9.03797 val_roc= 0.60209 val_auprc= 0.58286 val_apk= 0.50974 time= 0.08684\n",
            "Epoch: 0007 Iter: 1351 Edge: 0003 train_loss= 7.17166 val_roc= 0.57798 val_auprc= 0.57195 val_apk= 0.47707 time= 0.09634\n",
            "Epoch: 0007 Iter: 1501 Edge: 0000 train_loss= 8.66974 val_roc= 0.59688 val_auprc= 0.58513 val_apk= 0.47366 time= 0.08915\n",
            "Epoch: 0007 Iter: 1651 Edge: 0003 train_loss= 7.18931 val_roc= 0.58782 val_auprc= 0.57911 val_apk= 0.50693 time= 0.09015\n",
            "Epoch: 0007 Iter: 1801 Edge: 0000 train_loss= 8.95552 val_roc= 0.59994 val_auprc= 0.58719 val_apk= 0.50285 time= 0.10508\n",
            "Epoch: 0007 Iter: 1951 Edge: 0003 train_loss= 7.60012 val_roc= 0.58700 val_auprc= 0.57243 val_apk= 0.47031 time= 0.08849\n",
            "Epoch: 0007 Iter: 2101 Edge: 0000 train_loss= 8.14166 val_roc= 0.60565 val_auprc= 0.58686 val_apk= 0.47758 time= 0.10731\n",
            "Epoch: 0007 Iter: 2251 Edge: 0003 train_loss= 8.60749 val_roc= 0.59251 val_auprc= 0.58234 val_apk= 0.49855 time= 0.08989\n",
            "Epoch: 0007 Iter: 2401 Edge: 0000 train_loss= 7.74863 val_roc= 0.60201 val_auprc= 0.58203 val_apk= 0.44582 time= 0.08865\n",
            "Epoch: 0008 Iter: 0001 Edge: 0000 train_loss= 6.28231 val_roc= 0.60399 val_auprc= 0.58802 val_apk= 0.51658 time= 0.10450\n",
            "Epoch: 0008 Iter: 0151 Edge: 0003 train_loss= 6.20980 val_roc= 0.59081 val_auprc= 0.58246 val_apk= 0.52352 time= 0.08816\n",
            "Epoch: 0008 Iter: 0301 Edge: 0000 train_loss= 8.15003 val_roc= 0.60497 val_auprc= 0.58391 val_apk= 0.43543 time= 0.08864\n",
            "Epoch: 0008 Iter: 0451 Edge: 0003 train_loss= 6.74139 val_roc= 0.59344 val_auprc= 0.58691 val_apk= 0.47108 time= 0.08797\n",
            "Epoch: 0008 Iter: 0601 Edge: 0000 train_loss= 8.64714 val_roc= 0.60610 val_auprc= 0.58602 val_apk= 0.48256 time= 0.08890\n",
            "Epoch: 0008 Iter: 0751 Edge: 0003 train_loss= 7.66378 val_roc= 0.59654 val_auprc= 0.58777 val_apk= 0.53367 time= 0.09262\n",
            "Epoch: 0008 Iter: 0901 Edge: 0000 train_loss= 7.61977 val_roc= 0.60679 val_auprc= 0.58915 val_apk= 0.51367 time= 0.08861\n",
            "Epoch: 0008 Iter: 1051 Edge: 0003 train_loss= 7.91268 val_roc= 0.59518 val_auprc= 0.58362 val_apk= 0.53779 time= 0.11699\n",
            "Epoch: 0008 Iter: 1201 Edge: 0000 train_loss= 7.69687 val_roc= 0.61182 val_auprc= 0.58873 val_apk= 0.41900 time= 0.09227\n",
            "Epoch: 0008 Iter: 1351 Edge: 0003 train_loss= 8.76774 val_roc= 0.59483 val_auprc= 0.58663 val_apk= 0.54089 time= 0.08805\n",
            "Epoch: 0008 Iter: 1501 Edge: 0000 train_loss= 7.33023 val_roc= 0.60794 val_auprc= 0.58955 val_apk= 0.47941 time= 0.10287\n",
            "Epoch: 0008 Iter: 1651 Edge: 0003 train_loss= 6.96639 val_roc= 0.59692 val_auprc= 0.58494 val_apk= 0.53276 time= 0.09085\n",
            "Epoch: 0008 Iter: 1801 Edge: 0000 train_loss= 7.69483 val_roc= 0.61408 val_auprc= 0.59325 val_apk= 0.46572 time= 0.08935\n",
            "Epoch: 0008 Iter: 1951 Edge: 0003 train_loss= 8.41389 val_roc= 0.60009 val_auprc= 0.58712 val_apk= 0.50443 time= 0.09320\n",
            "Epoch: 0008 Iter: 2101 Edge: 0000 train_loss= 6.93099 val_roc= 0.62202 val_auprc= 0.60155 val_apk= 0.45322 time= 0.08951\n",
            "Epoch: 0008 Iter: 2251 Edge: 0003 train_loss= 7.51310 val_roc= 0.59753 val_auprc= 0.58884 val_apk= 0.50986 time= 0.09145\n",
            "Epoch: 0008 Iter: 2401 Edge: 0000 train_loss= 8.47053 val_roc= 0.61548 val_auprc= 0.59796 val_apk= 0.52576 time= 0.09166\n",
            "Epoch: 0009 Iter: 0001 Edge: 0000 train_loss= 8.19785 val_roc= 0.61491 val_auprc= 0.59624 val_apk= 0.51046 time= 0.11278\n",
            "Epoch: 0009 Iter: 0151 Edge: 0003 train_loss= 7.86645 val_roc= 0.60022 val_auprc= 0.59246 val_apk= 0.56630 time= 0.08839\n",
            "Epoch: 0009 Iter: 0301 Edge: 0000 train_loss= 8.84682 val_roc= 0.61360 val_auprc= 0.59187 val_apk= 0.46272 time= 0.09414\n",
            "Epoch: 0009 Iter: 0451 Edge: 0003 train_loss= 8.19592 val_roc= 0.60241 val_auprc= 0.59458 val_apk= 0.53292 time= 0.08867\n",
            "Epoch: 0009 Iter: 0601 Edge: 0000 train_loss= 6.78784 val_roc= 0.62075 val_auprc= 0.59689 val_apk= 0.43658 time= 0.08789\n",
            "Epoch: 0009 Iter: 0751 Edge: 0003 train_loss= 7.55522 val_roc= 0.60231 val_auprc= 0.59592 val_apk= 0.58393 time= 0.09073\n",
            "Epoch: 0009 Iter: 0901 Edge: 0000 train_loss= 5.29058 val_roc= 0.61687 val_auprc= 0.59505 val_apk= 0.51248 time= 0.08980\n",
            "Epoch: 0009 Iter: 1051 Edge: 0003 train_loss= 7.37733 val_roc= 0.60649 val_auprc= 0.59625 val_apk= 0.52131 time= 0.09292\n",
            "Epoch: 0009 Iter: 1201 Edge: 0000 train_loss= 8.40356 val_roc= 0.60981 val_auprc= 0.59398 val_apk= 0.50658 time= 0.09903\n",
            "Epoch: 0009 Iter: 1351 Edge: 0003 train_loss= 8.44442 val_roc= 0.60352 val_auprc= 0.59583 val_apk= 0.53989 time= 0.08730\n",
            "Epoch: 0009 Iter: 1501 Edge: 0000 train_loss= 6.54301 val_roc= 0.62102 val_auprc= 0.59991 val_apk= 0.54123 time= 0.09089\n",
            "Epoch: 0009 Iter: 1651 Edge: 0003 train_loss= 8.09113 val_roc= 0.60264 val_auprc= 0.59207 val_apk= 0.55512 time= 0.09175\n",
            "Epoch: 0009 Iter: 1801 Edge: 0000 train_loss= 8.93786 val_roc= 0.59729 val_auprc= 0.57658 val_apk= 0.50509 time= 0.09024\n",
            "Epoch: 0009 Iter: 1951 Edge: 0003 train_loss= 7.87053 val_roc= 0.61033 val_auprc= 0.59865 val_apk= 0.56709 time= 0.10853\n",
            "Epoch: 0009 Iter: 2101 Edge: 0000 train_loss= 7.47178 val_roc= 0.59921 val_auprc= 0.57580 val_apk= 0.48091 time= 0.08897\n",
            "Epoch: 0009 Iter: 2251 Edge: 0003 train_loss= 9.27985 val_roc= 0.61166 val_auprc= 0.60443 val_apk= 0.64665 time= 0.09645\n",
            "Epoch: 0009 Iter: 2401 Edge: 0000 train_loss= 7.18107 val_roc= 0.60051 val_auprc= 0.57793 val_apk= 0.48137 time= 0.09579\n",
            "Epoch: 0010 Iter: 0001 Edge: 0000 train_loss= 6.83185 val_roc= 0.59760 val_auprc= 0.57385 val_apk= 0.44056 time= 0.08856\n",
            "Epoch: 0010 Iter: 0151 Edge: 0003 train_loss= 7.22753 val_roc= 0.61541 val_auprc= 0.60702 val_apk= 0.60509 time= 0.09472\n",
            "Epoch: 0010 Iter: 0301 Edge: 0000 train_loss= 7.95960 val_roc= 0.59861 val_auprc= 0.57695 val_apk= 0.48547 time= 0.08971\n",
            "Epoch: 0010 Iter: 0451 Edge: 0003 train_loss= 7.50593 val_roc= 0.60850 val_auprc= 0.60815 val_apk= 0.67087 time= 0.09502\n",
            "Epoch: 0010 Iter: 0601 Edge: 0000 train_loss= 8.90273 val_roc= 0.59059 val_auprc= 0.56785 val_apk= 0.45466 time= 0.08798\n",
            "Epoch: 0010 Iter: 0751 Edge: 0003 train_loss= 6.26480 val_roc= 0.61659 val_auprc= 0.60699 val_apk= 0.61902 time= 0.08996\n",
            "Epoch: 0010 Iter: 0901 Edge: 0000 train_loss= 7.45885 val_roc= 0.60262 val_auprc= 0.57980 val_apk= 0.46320 time= 0.10161\n",
            "Epoch: 0010 Iter: 1051 Edge: 0003 train_loss= 7.68166 val_roc= 0.61749 val_auprc= 0.61049 val_apk= 0.61718 time= 0.09475\n",
            "Epoch: 0010 Iter: 1201 Edge: 0000 train_loss= 7.74031 val_roc= 0.60804 val_auprc= 0.58453 val_apk= 0.53329 time= 0.08996\n",
            "Epoch: 0010 Iter: 1351 Edge: 0003 train_loss= 6.47550 val_roc= 0.61606 val_auprc= 0.60798 val_apk= 0.63352 time= 0.09239\n",
            "Epoch: 0010 Iter: 1501 Edge: 0000 train_loss= 7.26197 val_roc= 0.60723 val_auprc= 0.58220 val_apk= 0.46485 time= 0.08915\n",
            "Epoch: 0010 Iter: 1651 Edge: 0003 train_loss= 7.16421 val_roc= 0.61652 val_auprc= 0.61045 val_apk= 0.62658 time= 0.09672\n",
            "Epoch: 0010 Iter: 1801 Edge: 0000 train_loss= 7.26432 val_roc= 0.60074 val_auprc= 0.57889 val_apk= 0.49271 time= 0.09458\n",
            "Epoch: 0010 Iter: 1951 Edge: 0003 train_loss= 5.98656 val_roc= 0.61511 val_auprc= 0.61183 val_apk= 0.70612 time= 0.08961\n",
            "Epoch: 0010 Iter: 2101 Edge: 0000 train_loss= 5.83342 val_roc= 0.60250 val_auprc= 0.57553 val_apk= 0.41037 time= 0.09053\n",
            "Epoch: 0010 Iter: 2251 Edge: 0003 train_loss= 6.47778 val_roc= 0.62132 val_auprc= 0.61447 val_apk= 0.63216 time= 0.09326\n",
            "Epoch: 0010 Iter: 2401 Edge: 0000 train_loss= 7.82618 val_roc= 0.60256 val_auprc= 0.58071 val_apk= 0.53627 time= 0.09223\n",
            "Optimization finished!\n",
            "Edge type= [00, 00, 00]\n",
            "Edge type: 0000 Test AUROC score 0.62198\n",
            "Edge type: 0000 Test AUPRC score 0.59793\n",
            "Edge type: 0000 Test AP@k score 0.53510\n",
            "\n",
            "Edge type= [00, 00, 01]\n",
            "Edge type: 0001 Test AUROC score 0.57911\n",
            "Edge type: 0001 Test AUPRC score 0.56295\n",
            "Edge type: 0001 Test AP@k score 0.35535\n",
            "\n",
            "Edge type= [00, 01, 00]\n",
            "Edge type: 0002 Test AUROC score 0.59270\n",
            "Edge type: 0002 Test AUPRC score 0.58355\n",
            "Edge type: 0002 Test AP@k score 0.47857\n",
            "\n",
            "Edge type= [01, 00, 00]\n",
            "Edge type: 0003 Test AUROC score 0.61832\n",
            "Edge type: 0003 Test AUPRC score 0.60149\n",
            "Edge type: 0003 Test AP@k score 0.55221\n",
            "\n",
            "Edge type= [01, 01, 00]\n",
            "Edge type: 0004 Test AUROC score 0.51142\n",
            "Edge type: 0004 Test AUPRC score 0.50697\n",
            "Edge type: 0004 Test AP@k score 0.24813\n",
            "\n",
            "Edge type= [01, 01, 01]\n",
            "Edge type: 0005 Test AUROC score 0.47334\n",
            "Edge type: 0005 Test AUPRC score 0.48637\n",
            "Edge type: 0005 Test AP@k score 0.32005\n",
            "\n",
            "Edge type= [01, 01, 02]\n",
            "Edge type: 0006 Test AUROC score 0.50724\n",
            "Edge type: 0006 Test AUPRC score 0.50641\n",
            "Edge type: 0006 Test AP@k score 0.28085\n",
            "\n",
            "Edge type= [01, 01, 03]\n",
            "Edge type: 0007 Test AUROC score 0.54590\n",
            "Edge type: 0007 Test AUPRC score 0.52655\n",
            "Edge type: 0007 Test AP@k score 0.24938\n",
            "\n",
            "Edge type= [01, 01, 04]\n",
            "Edge type: 0008 Test AUROC score 0.55633\n",
            "Edge type: 0008 Test AUPRC score 0.54780\n",
            "Edge type: 0008 Test AP@k score 0.38809\n",
            "\n",
            "Edge type= [01, 01, 05]\n",
            "Edge type: 0009 Test AUROC score 0.54844\n",
            "Edge type: 0009 Test AUPRC score 0.52272\n",
            "Edge type: 0009 Test AP@k score 0.24149\n",
            "\n"
          ]
        }
      ]
    }
  ]
}