{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMProject-MainVersion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QWbgVFQzTv5P",
        "t4zEC-3gLROH",
        "G3YKFnJBLUdU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Modeling Polypharmacy Side Effects with Graph Convolutional Networks**"
      ],
      "metadata": {
        "id": "QWbgVFQzTv5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polypharmacy side effects emerge because of drug-drug interactions, in which activity of one drug may change, favorably or unfavorably, if taken with another drug. The knowledge of drug interactions is often limited because these complex relationships are rare, and are usually not observed in relatively small clinical testing. \n",
        "\n",
        "**Decagon** is an approach for modeling polypharmacy side effects. The approach\n",
        "constructs a multimodal graph of protein-protein interactions, drug-protein target interactions, and the polypharmacy side effects, which are represented as drug-drug interactions, where each side effect is an edge of a different type. Decagon is developed specifically to handle such multimodal graphs with\n",
        "a large number of edge types.\n",
        "\n",
        "Furthermore, Decagon models particularly well polypharmacy side effects that have a strong molecular basis, while on predominantly non-molecular side effects, it achieves good performance because of effective sharing of model parameters across edge types."
      ],
      "metadata": {
        "id": "stXSvDJMTTg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polypharmacy side effect is modeled as a multirelational link prediction problem on a multimodal graph encoding drug, protein, and side effect relationships. More precisely, these relationships are represented by a graph G = (V,R) with N nodes (e.g., proteins, drugs) and labeled edges (relations) (v_i, r, v_j ), where r is the edge type (relation type): \n",
        "(1) physical binding between two proteins, \n",
        "(2) a target relationship between a drug and a protein, or \n",
        "(3) a particular type of a side effect between two drugs. \n",
        "\n",
        "Original project can be found at: http://snap.stanford.edu/decagon/"
      ],
      "metadata": {
        "id": "QPRa_J-7UEUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "t4zEC-3gLROH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "zPDFxiwWyTPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ec79d5-f8dc-4e8f-cde6-a457295fb3ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rijm4ze6LPl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb0c81d-31d9-4eb7-bf49-af99c1370e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import os\n",
        "import networkx as nx\n",
        "import sys\n",
        "\n",
        "from sklearn import metrics\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "from scipy.stats import ks_2samp\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "G3YKFnJBLUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section most of the functions used in the project are implemented.**"
      ],
      "metadata": {
        "id": "507OhmSfDn6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns sparse matrix.\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape\n",
        "    \n",
        "#Computes the average precision at k. This function computes the average precision at k between two lists of items.\n",
        "\n",
        "def apk(actual, predicted, k=10):\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "#Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items.\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])\n",
        "\n",
        "#Creating a weight variable with Glorot & Bengio (AISTATS 2010) initialization. Used in Graph Convolution Calculation.\n",
        "\n",
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "#Returns a zero tensor with dimensions (in, out).\n",
        "\n",
        "def zeros(input_dim, output_dim, name=None):\n",
        "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "#Returns a one tensor with dimension (in, out).\n",
        "\n",
        "def ones(input_dim, output_dim, name=None):\n",
        "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n"
      ],
      "metadata": {
        "id": "_lxl2O1cL1Os"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flags are used during the training phase, in order to send the parameters to the Neural Network.\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "#Global unique layer ID dictionary for layer name assignment.\n",
        "\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "#Helper function, assigns unique layer IDs.\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "# Wrapper for tf.matmul (sparse vs dense) tensor multiplication.\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "#Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements).\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)"
      ],
      "metadata": {
        "id": "Af-xGZdwLaQb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graph Layers**"
      ],
      "metadata": {
        "id": "W4w22qO_Eqtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is that Decagon learns how to transform and propagate information, captured by node feature vectors, across the graph. Every nodeâ€™s network neighborhood defines a different neural network information propagation architecture but these architectures then share functions/parameters that define how information is shared and propagated. For a given node Decagon performs transformation/aggregation operations on feature vectors of its neighbors. This way Decagon only takes into account the first-order neighborhood of a node and applies the same transformation across all locations in the graph. Successive application of these operations then effectively convolves information across the K-th order neighborhood (i.e., embedding of a\n",
        "node depends on all the nodes that are at most K steps away), where K is\n",
        "the number of successive operations of convolutional layers in the neural\n",
        "network model.\n",
        "\n",
        "\n",
        "In each layer, Decagon propagates latent node feature information\n",
        "across edges of the graph, while taking into account the type (relation) of\n",
        "an edge."
      ],
      "metadata": {
        "id": "nj15htjwYyQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Base layer class for Multi Graphs, it's expanded later. Defines basic API for all layer objects.\n",
        "\n",
        "class MultiLayer(object):\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "#Used when the output has to be computed with getting certain input.\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "#Graph convolution layer for sparse inputs.\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, support=None, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.rank = 100\n",
        "\n",
        "        if support is None:\n",
        "            self.support = placeholders['support']\n",
        "        else:\n",
        "            self.support = support\n",
        "\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k)\n",
        "\n",
        "#In this function, the SampledGraphConv in FastGCN is used, and changed the original GCN.\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "          \n",
        "          #Dropping certain values, where 1-self.dropout is the probability of how many will stay, and according to that, the mask is defined. \n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]])\n",
        "\n",
        "            #In this part, the convolution is done, feature vector for the node is multiplied by weights of the local neighbour of the node.\n",
        "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])\n",
        "\n",
        "            #Support is defined (it gives directions where a priority is given comparing to other nodes) - this is done based on adj matrix and edge type.\n",
        "            sup = self.support[self.edge_type][k]\n",
        "            \n",
        "            #Normalization of input\n",
        "            norm_x = tf.nn.l2_normalize(x)\n",
        "            #norm_sup = tf.nn.l2_normalize(tf.sparse.to_dense(sup))\n",
        "\n",
        "            #In the original FastGCN they use norm_sup, but in my case, this did not work.\n",
        "\n",
        "            #Here we multiply support with convoluted input, basically filtering out some of the neighbouring nodes.\n",
        "            norm_mix = tf.sparse_tensor_dense_matmul(sup, x)\n",
        "            #We are trying to get diagonal matrix, and we do this in order to get nodes from the neighbourhood of the support.\n",
        "            #After this, the norm_mix matrix is a diagonal matrix, and after sampling for multinomial distribution, these nodeas are now contained on diagonal. \n",
        "            norm_mix = norm_mix*tf.transpose(tf.reduce_sum(norm_mix))\n",
        "            #Getting indexes of those samples (nodes), because we know that on the diagonal matrix, all the nodes are the ones from support.\n",
        "            sampledIndex = tf.multinomial(tf.log(norm_mix), self.rank)\n",
        "            #Since GCN is full graph convolution, sampled graph convolution (by sampling the neighbourhood) works faster (FastGCN paper gives some interesting results for this),\n",
        "            #as it works with tensors that are smaller in size.\n",
        "\n",
        "            out = norm_mix \n",
        "            #Output is the exit from activation function (activation layer).\n",
        "            outputs.append(self.act(out))\n",
        "\n",
        "#Summing of all tensors from the list (for every of these relationships, we got the tensor that represents the output of activation function).\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "#Basic graph convolution layer for undirected graph without edge labels.\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        x = inputs\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AIyMpZb5LfMi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder**"
      ],
      "metadata": {
        "id": "vZ5p0rDYRKbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of decoder is to reconstruct labeled edges in G by relying on learned node embeddings and by treating each label (edge type) differently. In particular, decoder scores a (vi, r, vj )-triple through a function g whose goal is to assign a score g(vi, r, vj ) representing how likely it is that drugs vi and vj are interacting through a relation/side effect type r.\n",
        "\n",
        "\n",
        "Two cases has to be distinguished:\n",
        "\n",
        "(1) When v_i and v_j are drug nodes, the decoder g in assumes a global model of drug-drug interactions (i.e., R) whose variation and importance across polypharmacy side effects are described by side-effectspecific diagonal factors;\n",
        "\n",
        "(2) When v_i and v_j are not both drug nodes, the decoder g in employs a bilinear form to decode edges from node embeddings."
      ],
      "metadata": {
        "id": "tjfok1ryY3gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For different type of relationships, a different Decoder is used.\n",
        "\n",
        "#DEDICOM Tensor Factorization Decoder model layer for link prediction. It is specifically created for Decagon. By encoding it gets matrices.\n",
        "#Creates \"relaxing\" of tensors, meaning that it does not removes the values, but decreases their weight. In that way, it can edit priorities of relationships\n",
        "#local surrounding, and if there is a drug that is connected to many proteins - relationships with that drug have higher weight.\n",
        "\n",
        "class DEDICOMDecoder(MultiLayer):\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            self.vars['global_interaction'] = weight_variable_glorot(\n",
        "                input_dim, input_dim, name='global_interaction')\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='local_variation_%d' % k)\n",
        "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
        "            product1 = tf.matmul(inputs_row, relation)\n",
        "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
        "            product3 = tf.matmul(product2, relation)\n",
        "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "#DistMult Decoder model layer for link prediction. The same as previous one, but without decreasing of the weights.\n",
        "\n",
        "class DistMultDecoder(MultiLayer):\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(DistMultDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                tmp = weight_variable_glorot(\n",
        "                    input_dim, 1, name='relation_%d' % k)\n",
        "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            relation = tf.diag(self.vars['relation_%d' % k])\n",
        "            intermediate_product = tf.matmul(inputs_row, relation)\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "#Bilinear Decoder model layer for link prediction.\n",
        "\n",
        "class BilinearDecoder(MultiLayer):\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(BilinearDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, input_dim, name='relation_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
        "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n",
        "\n",
        "#Inner Product Decoder for link prediction.\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "wBACDtfQRNXN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Edge Minibatch Iterator**"
      ],
      "metadata": {
        "id": "xjyn3NS1RRNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This minibatch iterator iterates over batches of sampled edges or random pairs of co-occuring edges."
      ],
      "metadata": {
        "id": "lR0W_szdDWml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "#In order to do the training, Dataset has to be divided in Batches, where every of the batches is being sent to separate (tensor) units (going through edges of graph model).\n",
        "\n",
        "class EdgeMinibatchIterator(object):\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats\n",
        "        self.feat = feat\n",
        "        self.edge_types = edge_types\n",
        "        self.batch_size = batch_size\n",
        "        self.val_test_size = val_test_size\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
        "        self.batch_num = [0]*self.num_edge_types\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "\n",
        "#Going throught edges of one batch and creating of dynamically allocated arrays, where all members are 0. \n",
        "#Later, this values are filled with different values (metrics, accuracy,...)\n",
        "\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r\n",
        "                self.idx2edge_type[r] = i, j, k\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        #Function to build test and val sets with val_test_size positive links (is there a effect between two drugs).\n",
        "\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                print(\"Minibatch edge type:\", \"(%d, %d, %d)\" % (i, j, k))\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "                print(\"Train edges=\", \"%04d\" % len(self.train_edges[i,j][k]))\n",
        "                print(\"Val edges=\", \"%04d\" % len(self.val_edges[i,j][k]))\n",
        "                print(\"Test edges=\", \"%04d\" % len(self.test_edges[i,j][k]))\n",
        "\n",
        "#Converting adj matrix to sparse matrix, and then to tuple. Tuple contains coordinates, and values at those coordinates (similar to sparse tensor). \n",
        "\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "#Mask of test edges, if it is a test edge, value in the tensor is 1, otherwise it is 0.\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])\n",
        "        num_test = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "        num_val = max(50, int(np.floor(edges_all.shape[0] * self.val_test_size)))\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if len(test_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing test edges=\", \"%04d/%04d\" % (len(test_edges_false), len(test_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if len(val_edges_false) % 1000 == 0:\n",
        "                print(\"Constructing val edges=\", \"%04d/%04d\" % (len(val_edges_false), len(val_edges)))\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        #Re-build adj matrices.\n",
        "\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def end(self):\n",
        "        finished = len(self.freebatch_edge_types) == 0\n",
        "        return finished\n",
        "\n",
        "#Update of placeholders.\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "\n",
        "        # construct feed dictionary\n",
        "\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "#Update of values for a certain batch.\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "#Select a random edge type and a batch of edges of the same type (finding the next batch).\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        while True:\n",
        "            if self.iter % 4 == 0:\n",
        "                # gene-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "            elif self.iter % 4 == 1:\n",
        "                # gene-drug relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
        "            elif self.iter % 4 == 2:\n",
        "                # drug-gene relation\n",
        "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
        "            else:\n",
        "                # random side effect relation\n",
        "                if len(self.freebatch_edge_types) > 0:\n",
        "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
        "                else:\n",
        "                    self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
        "                    self.iter = 0\n",
        "\n",
        "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
        "                break\n",
        "            else:\n",
        "                if self.iter % 4 in [0, 1, 2]:\n",
        "                    self.batch_num[self.current_edge_type_idx] = 0\n",
        "                else:\n",
        "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
        "\n",
        "        self.iter += 1\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "#Number of batches.\n",
        "\n",
        "    def num_training_batches(self, edge_type, type_idx):\n",
        "        return len(self.train_edges[edge_type][type_idx]) // self.batch_size + 1\n",
        "\n",
        "#Update of validation part of the Batch.\n",
        "\n",
        "    def val_feed_dict(self, edge_type, type_idx, placeholders, size=None):\n",
        "        edge_list = self.val_edges[edge_type][type_idx]\n",
        "        if size is None:\n",
        "            return self.batch_feed_dict(edge_list, edge_type, placeholders)\n",
        "        else:\n",
        "            ind = np.random.permutation(len(edge_list))\n",
        "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
        "            return self.batch_feed_dict(val_edges, edge_type, placeholders)\n",
        "\n",
        "#Used for permutation of edges, in order to not have overfitting.\n",
        "\n",
        "    def shuffle(self):\n",
        "        for edge_type in self.edge_types:\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                self.train_edges[edge_type][k] = np.random.permutation(self.train_edges[edge_type][k])\n",
        "                self.batch_num[self.edge_type2idx[edge_type[0], edge_type[1], k]] = 0\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.freebatch_edge_types = list(range(self.num_edge_types))\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 0, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[0, 1, 0])\n",
        "        self.freebatch_edge_types.remove(self.edge_type2idx[1, 0, 0])\n",
        "        self.iter = 0"
      ],
      "metadata": {
        "id": "3FIO8W8tLjZd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ANMT3GmPLchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model is an abstract class, that contains basic constructor, and functions (build, fit, predict).\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "OibFC-vT2lcJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types\n",
        "        self.num_edge_types = sum(self.edge_types.values())\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1\n",
        "        self.decoders = decoders\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types}\n",
        "        self.input_dim = num_feat\n",
        "        self.nonzero_feat = nonzero_feat\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.support = placeholders['support']\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "#hidden1 is defined as dictionary (defined as layers for all possible relationships). This is the first, encoding part, where sparse layer is used, and therefore,\n",
        "#the GraphConvolutionSparseMulti (function for sparse convolution) is used. The linear function f(x)= x is used as the activation fuction.\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=FLAGS.hidden1,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout, support=self.support,\n",
        "                logging=self.logging)(self.inputs[j]))\n",
        "        for i, hid1 in self.hidden1.items():\n",
        "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "\n",
        "#Dense layer is used for embedding, the first part of the decoder part.\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=FLAGS.hidden1, output_dim=FLAGS.hidden2,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "#Depending on the edge type, a specific decoder is used.\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'distmult':\n",
        "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'bilinear':\n",
        "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            elif decoder == 'dedicom':\n",
        "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
        "                    input_dim=FLAGS.hidden2, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "#The goal of decoder is to reconstruct labeled edges in G by relying on learned node embeddings and by treating each label (edge type) differently.\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'distmult':\n",
        "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'bilinear':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
        "                    loc = tf.eye(FLAGS.hidden2, FLAGS.hidden2)\n",
        "                elif decoder == 'dedicom':\n",
        "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
        "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)"
      ],
      "metadata": {
        "id": "NvyZI6NFLnRV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decagon Optimizer**"
      ],
      "metadata": {
        "id": "bcuYtIKOLggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        #Latent_inters and latent_varies represent \"hiddent\" edges between the nodes, for example if two drugs are connected through a protein.\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "        neg_samples_list = []\n",
        "        for i, j in self.edge_types:\n",
        "          #Going through all edges and creating candidates whose links are true/false. 'True' means that there is a valid connection (bio-chemically) between two drugs,\n",
        "          #False means that there is no valid connection, but they can be connected through the same protein.\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                    true_classes=labels,\n",
        "                    num_true=1,\n",
        "                    num_sampled=self.batch_size,\n",
        "                    unique=False,\n",
        "                    range_max=len(self.degrees[i][k]),\n",
        "                    distortion=0.75,\n",
        "                    unigrams=self.degrees[i][k].tolist())\n",
        "                neg_samples_list.append(neg_samples)\n",
        "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
        "        self.outputs = tf.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "        \n",
        "        #Prediction for a specific batch (decoding of embedding)\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "        return preds\n",
        "\n",
        "        #Prediction of outputs for all batches\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
        "\n",
        "  #Optimizar and loss function changes.\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
        "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        #self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        #Maximum-margin optimization using the hinge loss.\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _mse_loss(self, aff, neg_aff):\n",
        "        #Mean squared error\n",
        "        diff = tf.nn.relu((tf.subtract(neg_aff, tf.expand_dims(aff, 0))), name='diff')\n",
        "        loss = tf.reduce_sum(tf.math.square(diff))\n",
        "        return loss       \n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        #Cross-entropy optimization.\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "#Gather columns of a 2D tensor, used for training in order to get the subset that's responsible for current batch.\n",
        "def gather_cols(params, indices, name=None):\n",
        "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "metadata": {
        "id": "hftqMu6_LrUB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "ruZ8gtknLkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU (hide GPU) due to memory constraints.\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "#Creating placeholder for support.\n",
        "\n",
        "def prepare_data(placeholders, edge_types):\n",
        "    adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in edge_types.items()}\n",
        "\n",
        "    return adj_mats\n",
        "    \n",
        "#Creating Accuracy scores from Decagon Optimizer.\n",
        "\n",
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "    #Predict on test set of edges, since the relationships between drugs can be positive and negative, the procedure has to be separated in two different parts. \n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "   \n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=50)\n",
        "\n",
        "    return roc_sc, aupr_sc, apk_sc\n",
        "\n",
        "#Contruction and update of all placeholders.\n",
        "\n",
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'support': tf.sparse_placeholder(tf.float32),\n",
        "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.placeholder(tf.int32),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    }\n",
        "    \n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    placeholders.update({'support': prepare_data(placeholders, edge_types)})\n",
        "    return placeholders\n",
        "\n",
        "# Creating the dataset from the paper (they refer it to a dummy dataset, but they say it's \n",
        "# a realistic representation).\n",
        "\n",
        "val_test_size = 0.05\n",
        "n_genes = 500\n",
        "n_drugs = 400\n",
        "n_drugdrug_rel_types = 3\n",
        "gene_net = nx.planted_partition_graph(50, 10, 0.2, 0.05, seed=42)\n",
        "\n",
        "gene_adj = nx.adjacency_matrix(gene_net)\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "\n",
        "gene_drug_adj = sp.csr_matrix((10 * np.random.randn(n_genes, n_drugs) > 15).astype(int))\n",
        "drug_gene_adj = gene_drug_adj.transpose(copy=True)\n",
        "\n",
        "drug_drug_adj_list = []\n",
        "tmp = np.dot(drug_gene_adj, gene_drug_adj)\n",
        "for i in range(n_drugdrug_rel_types):\n",
        "    mat = np.zeros((n_drugs, n_drugs))\n",
        "    for d1, d2 in combinations(list(range(n_drugs)), 2):\n",
        "        if tmp[d1, d2] == i + 4:\n",
        "            mat[d1, d2] = mat[d2, d1] = 1.\n",
        "    drug_drug_adj_list.append(sp.csr_matrix(mat))\n",
        "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
        "\n",
        "\n",
        "#Data representation\n",
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_drug_adj],\n",
        "    (1, 0): [drug_gene_adj],\n",
        "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: drug_degrees_list + drug_degrees_list,\n",
        "}\n",
        "\n",
        "#Featureless (genes)\n",
        "gene_feat = sp.identity(n_genes)\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "#Features (drugs)\n",
        "drug_feat = sp.identity(n_drugs)\n",
        "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
        "drug_feat = sparse_to_tuple(drug_feat.tocoo())\n",
        "\n",
        "#Data representation\n",
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: drug_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: drug_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: drug_feat,\n",
        "}\n",
        "\n",
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'bilinear',\n",
        "    (0, 1): 'bilinear',\n",
        "    (1, 0): 'bilinear',\n",
        "    (1, 1): 'dedicom',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 2, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
        "flags.DEFINE_integer('batch_size', 100, 'minibatch size.')  # Changed from 512\n",
        "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
        "\n",
        "# Important -- Do not evaluate/print validation performance every iteration as it can take substantial amount of time!\n",
        "PRINT_PROGRESS_EVERY = 150\n",
        "\n",
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)\n",
        "\n",
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "    adj_mats=adj_mats_orig,\n",
        "    feat=feat,\n",
        "    edge_types=edge_types,\n",
        "    batch_size=FLAGS.batch_size,\n",
        "    val_test_size=val_test_size\n",
        ")\n",
        "\n",
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "    placeholders=placeholders,\n",
        "    num_feat=num_feat,\n",
        "    nonzero_feat=nonzero_feat,\n",
        "    edge_types=edge_types,\n",
        "    decoders=edge_type2decoder\n",
        ")\n",
        "\n",
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "    opt = DecagonOptimizer(\n",
        "        embeddings=model.embeddings,\n",
        "        latent_inters=model.latent_inters,\n",
        "        latent_varies=model.latent_varies,\n",
        "        degrees=degrees,\n",
        "        edge_types=edge_types,\n",
        "        edge_type2dim=edge_type2dim,\n",
        "        placeholders=placeholders,\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        margin=FLAGS.max_margin\n",
        "    )\n",
        "\n",
        "print(\"Initialize session\")\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "feed_dict = {}\n",
        "\n",
        "print(\"Train model\")\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    minibatch.shuffle()\n",
        "    itr = 0\n",
        "    while not minibatch.end():\n",
        "        # Construct feed dictionary\n",
        "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "        feed_dict = minibatch.update_feed_dict(\n",
        "            feed_dict=feed_dict,\n",
        "            dropout=FLAGS.dropout,\n",
        "            placeholders=placeholders)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        # Training step: run single weight update\n",
        "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
        "        train_cost = outs[1]\n",
        "        batch_edge_type = outs[2]\n",
        "\n",
        "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
        "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false,\n",
        "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
        "\n",
        "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
        "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
        "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "print(\"Optimization finished!\")\n",
        "\n",
        "for et in range(num_edge_types):\n",
        "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
        "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tWB8QcL-Nd",
        "outputId": "d41a0568-a7d8-468d-9823-ecb1dcbb7479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge types: 10\n",
            "Defining placeholders\n",
            "Create minibatch iterator\n",
            "Minibatch edge type: (0, 0, 0)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 0, 1)\n",
            "Constructing test edges= 0000/0663\n",
            "Constructing val edges= 0000/0663\n",
            "Train edges= 11952\n",
            "Val edges= 0663\n",
            "Test edges= 0663\n",
            "Minibatch edge type: (0, 1, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 0, 0)\n",
            "Constructing test edges= 0000/0664\n",
            "Constructing val edges= 0000/0664\n",
            "Train edges= 11958\n",
            "Val edges= 0664\n",
            "Test edges= 0664\n",
            "Minibatch edge type: (1, 1, 0)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 1)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 2)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Minibatch edge type: (1, 1, 3)\n",
            "Constructing test edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Constructing val edges= 0000/0868\n",
            "Train edges= 15642\n",
            "Val edges= 0868\n",
            "Test edges= 0868\n",
            "Minibatch edge type: (1, 1, 4)\n",
            "Constructing test edges= 0000/0378\n",
            "Constructing val edges= 0000/0378\n",
            "Train edges= 6810\n",
            "Val edges= 0378\n",
            "Test edges= 0378\n",
            "Minibatch edge type: (1, 1, 5)\n",
            "Constructing test edges= 0000/0136\n",
            "Constructing val edges= 0000/0136\n",
            "Train edges= 2466\n",
            "Val edges= 0136\n",
            "Test edges= 0136\n",
            "Create model\n",
            "WARNING:tensorflow:From <ipython-input-5-2297ce1c9f77>:81: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-2297ce1c9f77>:92: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From <ipython-input-5-2297ce1c9f77>:113: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Create optimizer\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_grad.py:202: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize session\n",
            "Train model\n",
            "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 10.40034 val_roc= 0.49862 val_auprc= 0.49663 val_apk= 0.20263 time= 2.49267\n",
            "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 10.49539 val_roc= 0.47153 val_auprc= 0.47818 val_apk= 0.24230 time= 0.08874\n",
            "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 10.18957 val_roc= 0.49953 val_auprc= 0.48788 val_apk= 0.18642 time= 0.09074\n",
            "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 9.60027 val_roc= 0.47024 val_auprc= 0.47361 val_apk= 0.17834 time= 0.09631\n",
            "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 10.16814 val_roc= 0.49440 val_auprc= 0.48911 val_apk= 0.20636 time= 0.08721\n",
            "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 10.15314 val_roc= 0.46816 val_auprc= 0.47099 val_apk= 0.19757 time= 0.08959\n",
            "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 10.21339 val_roc= 0.49399 val_auprc= 0.48645 val_apk= 0.18608 time= 0.09092\n",
            "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 9.73838 val_roc= 0.46743 val_auprc= 0.46847 val_apk= 0.14683 time= 0.08886\n",
            "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 10.24778 val_roc= 0.50133 val_auprc= 0.49398 val_apk= 0.21147 time= 0.16127\n",
            "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 10.88912 val_roc= 0.46734 val_auprc= 0.46979 val_apk= 0.16652 time= 0.10634\n",
            "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 10.16710 val_roc= 0.50360 val_auprc= 0.49216 val_apk= 0.19768 time= 0.08762\n",
            "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 9.39196 val_roc= 0.47105 val_auprc= 0.47425 val_apk= 0.19539 time= 0.08774\n",
            "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 9.90672 val_roc= 0.49721 val_auprc= 0.48942 val_apk= 0.18820 time= 0.10389\n",
            "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 9.86963 val_roc= 0.47262 val_auprc= 0.47358 val_apk= 0.18321 time= 0.09749\n",
            "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 9.80095 val_roc= 0.50003 val_auprc= 0.49544 val_apk= 0.22214 time= 0.08876\n",
            "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 9.02686 val_roc= 0.47136 val_auprc= 0.47455 val_apk= 0.21797 time= 0.08681\n",
            "Epoch: 0001 Iter: 2401 Edge: 0000 train_loss= 9.71447 val_roc= 0.50003 val_auprc= 0.49314 val_apk= 0.20142 time= 0.09209\n",
            "Epoch: 0002 Iter: 0001 Edge: 0000 train_loss= 10.25182 val_roc= 0.50290 val_auprc= 0.49660 val_apk= 0.24375 time= 0.08792\n",
            "Epoch: 0002 Iter: 0151 Edge: 0003 train_loss= 10.06742 val_roc= 0.47302 val_auprc= 0.47908 val_apk= 0.21755 time= 0.09282\n",
            "Epoch: 0002 Iter: 0301 Edge: 0000 train_loss= 8.87403 val_roc= 0.50155 val_auprc= 0.49535 val_apk= 0.20617 time= 0.09481\n",
            "Epoch: 0002 Iter: 0451 Edge: 0003 train_loss= 9.93401 val_roc= 0.47815 val_auprc= 0.48024 val_apk= 0.20298 time= 0.09061\n",
            "Epoch: 0002 Iter: 0601 Edge: 0000 train_loss= 9.41107 val_roc= 0.50109 val_auprc= 0.50923 val_apk= 0.33200 time= 0.08960\n",
            "Epoch: 0002 Iter: 0751 Edge: 0003 train_loss= 9.82195 val_roc= 0.47614 val_auprc= 0.48620 val_apk= 0.26093 time= 0.09063\n",
            "Epoch: 0002 Iter: 0901 Edge: 0000 train_loss= 10.14738 val_roc= 0.49921 val_auprc= 0.50838 val_apk= 0.38324 time= 0.09353\n",
            "Epoch: 0002 Iter: 1051 Edge: 0003 train_loss= 10.15122 val_roc= 0.47626 val_auprc= 0.48618 val_apk= 0.27090 time= 0.18853\n",
            "Epoch: 0002 Iter: 1201 Edge: 0000 train_loss= 9.38626 val_roc= 0.50118 val_auprc= 0.51303 val_apk= 0.36328 time= 0.10446\n",
            "Epoch: 0002 Iter: 1351 Edge: 0003 train_loss= 9.04510 val_roc= 0.47690 val_auprc= 0.48551 val_apk= 0.24732 time= 0.09063\n",
            "Epoch: 0002 Iter: 1501 Edge: 0000 train_loss= 9.95691 val_roc= 0.50220 val_auprc= 0.51449 val_apk= 0.40949 time= 0.09581\n",
            "Epoch: 0002 Iter: 1651 Edge: 0003 train_loss= 10.31600 val_roc= 0.47758 val_auprc= 0.48718 val_apk= 0.27168 time= 0.09172\n",
            "Epoch: 0002 Iter: 1801 Edge: 0000 train_loss= 10.14582 val_roc= 0.50047 val_auprc= 0.51333 val_apk= 0.42768 time= 0.09078\n",
            "Epoch: 0002 Iter: 1951 Edge: 0003 train_loss= 9.86764 val_roc= 0.47544 val_auprc= 0.48935 val_apk= 0.31101 time= 0.09018\n",
            "Epoch: 0002 Iter: 2101 Edge: 0000 train_loss= 9.25575 val_roc= 0.50081 val_auprc= 0.51202 val_apk= 0.35171 time= 0.09050\n",
            "Epoch: 0002 Iter: 2251 Edge: 0003 train_loss= 9.49328 val_roc= 0.47834 val_auprc= 0.48973 val_apk= 0.31208 time= 0.09436\n",
            "Epoch: 0002 Iter: 2401 Edge: 0000 train_loss= 10.02145 val_roc= 0.50055 val_auprc= 0.51290 val_apk= 0.43154 time= 0.09639\n",
            "Optimization finished!\n",
            "Edge type= [00, 00, 00]\n",
            "Edge type: 0000 Test AUROC score 0.51874\n",
            "Edge type: 0000 Test AUPRC score 0.53407\n",
            "Edge type: 0000 Test AP@k score 0.48916\n",
            "\n",
            "Edge type= [00, 00, 01]\n",
            "Edge type: 0001 Test AUROC score 0.51330\n",
            "Edge type: 0001 Test AUPRC score 0.51010\n",
            "Edge type: 0001 Test AP@k score 0.29771\n",
            "\n",
            "Edge type= [00, 01, 00]\n",
            "Edge type: 0002 Test AUROC score 0.50509\n",
            "Edge type: 0002 Test AUPRC score 0.50841\n",
            "Edge type: 0002 Test AP@k score 0.31322\n",
            "\n",
            "Edge type= [01, 00, 00]\n",
            "Edge type: 0003 Test AUROC score 0.50945\n",
            "Edge type: 0003 Test AUPRC score 0.50100\n",
            "Edge type: 0003 Test AP@k score 0.22517\n",
            "\n",
            "Edge type= [01, 01, 00]\n",
            "Edge type: 0004 Test AUROC score 0.47913\n",
            "Edge type: 0004 Test AUPRC score 0.48868\n",
            "Edge type: 0004 Test AP@k score 0.27582\n",
            "\n",
            "Edge type= [01, 01, 01]\n",
            "Edge type: 0005 Test AUROC score 0.49662\n",
            "Edge type: 0005 Test AUPRC score 0.48490\n",
            "Edge type: 0005 Test AP@k score 0.17637\n",
            "\n",
            "Edge type= [01, 01, 02]\n",
            "Edge type: 0006 Test AUROC score 0.45907\n",
            "Edge type: 0006 Test AUPRC score 0.45616\n",
            "Edge type: 0006 Test AP@k score 0.13200\n",
            "\n",
            "Edge type= [01, 01, 03]\n",
            "Edge type: 0007 Test AUROC score 0.52008\n",
            "Edge type: 0007 Test AUPRC score 0.49793\n",
            "Edge type: 0007 Test AP@k score 0.15987\n",
            "\n",
            "Edge type= [01, 01, 04]\n",
            "Edge type: 0008 Test AUROC score 0.44713\n",
            "Edge type: 0008 Test AUPRC score 0.44926\n",
            "Edge type: 0008 Test AP@k score 0.09515\n",
            "\n",
            "Edge type= [01, 01, 05]\n",
            "Edge type: 0009 Test AUROC score 0.57548\n",
            "Edge type: 0009 Test AUPRC score 0.54577\n",
            "Edge type: 0009 Test AP@k score 0.30810\n",
            "\n"
          ]
        }
      ]
    }
  ]
}